challenges_texts
manual mapping of trial balance to financial statements
time-consuming reporting process
"Tedious, time-consuming, manual spreadsheet-based processes"
"Collaboration, data aggregation, and analysis were resource-draining"
Lack of workflows in Budget Accelerator leading to confusion
Manual processes leading to long reporting cycles
Human error in manual reporting
Lack of in-depth reporting options
Difficulty in identifying potential problems or risks
Need for efficient identification of critical information
"Manual, time-consuming planning processes"
High rate of errors due to manual data transfer
Inability to track real-time planning processes
Complex planning across multiple units and markets
Manual and time-consuming reporting processes
Dependence on a single employee for ERP knowledge
Inability to create custom reports easily with MRI
Limited reporting
Lack of insights
Insufficient development resources
Power BI has no out-of-the-box planning capability.
"Spreadsheet planning is unwieldy, error-prone, time-consuming."
Security patches and server issues
Data restores and refreshes causing lost data
Delayed or inaccurate forecasts impacting downstream processes
Different versions of data model cubes creating multiple sources of truth
Managing €20B in procurement accurately and efficiently
Collecting and validating data manually
Handling errant inserted rows or columns in Excel files
Manual report creation
Data collection across multiple sites and groups
Power BI's limitation of not supporting commentary
Disconnected systems and reliance on emailing data
Manual data validation and processing
Risk of manual keying errors
Risk to patient privacy with spreadsheet security limitations
Potential fines from government and insurance agencies
Manual data collection from 50+ spreadsheets
Inconsistent performance of complex macros and formulas
High manual workload for sales forecasting
Incomplete records for trade spending
Disconnected sales forecasting process
Carrying more inventory than needed
Too much overtime for IT team
Low employee morale
Traditional Excel spreadsheets were slow and hard to manage
Persistent data quality challenges
Excessive time to maintain Excel workbooks
"Lack of complete, standardized solution"
Power BI’s inability to support write-back and data aggregation out-of-the-box
Costly and time-consuming custom coding
Steep costs and learning curves with Legacy Platforms
Learning new ERP and reporting processes
Initial difficulty with unfamilar software
Oracle’s native reporting tools could not meet finance team needs
Transitioning to new systems is hard on the team and tends to cause a lag or drop in reporting
Copying information from Excel to Access database manually
Managing increased workload due to volunteer growth
Managing volunteer and transport details inefficiently
Difficulty in accessing database from different locations
Supply chain disruptions
Need for accurate real-time data
Manual reporting process consumed valuable finance team time
Risk of human error in manual reporting
Time-consuming process for project managers
Complying with diverse regulatory standards across 30 international markets
"Manually driven, disparate processes for data aggregation and reporting"
Volume of work and accuracy requirements making it difficult to meet deadlines
Occasional need for extensions and hiring additional resources
Delivery performance was degrading as the company grew
ERP tool had shortcomings in planning and scheduling
Engineers had difficulty staying on track with task priorities
Bolt-on approach for analytics capabilities was inadequate
Legacy solution didn’t allow users to join data objects across components
Need for a scalable tool to handle complex conglomerations of disparate data
Requirement for timely and relevant intelligence for surveillance
Struggling to combine data for analysis
Time-consuming manual data export to Excel
High internal demand for custom dashboards
Overwhelming backlog of customer requests for improvements
Weeks to fix critical issues and high costs for updates
Legacy reporting tool was ill-equipped to meet heavy data usage
Lacked flexibility for customers' business demands
Previous reporting tool created interoperability issues
Cumbersome and time-consuming report creation process
Development delays due to incompatibility of the previous tool
Providing customers with the ability to report on archived data from any database
Making data reporting accessible and intuitive for diverse user groups
Crystal Reports could not achieve complex formats and instilled many reporting silos
Manually running and distributing reports was time-consuming
Significant development times with previous BI solution
Lack of customization in the existing solution
Inability to meet customer demands for data interaction
Increased IT overhead due to custom ad hoc requests
Difficulty in maintaining and scaling the in-house solution
"Excel-based, home-grown reporting capability was inflexible and painful to maintain"
detracted from their overall solution
Familiarizing users with new data-gathering methods
Adding new data to a data warehouse effectively
Aligning ever-changing data for real-time analytics
Flood of data without tools to process or analyze efficiently
Consolidating data meaningfully
Security and compliance concerns
Administrative delays and slow data extraction
Lack of overall view of project progress
Manual data extraction and quality checks
Uncertain state of working capital
Limited customization options with native Qlik Sense
Lack of configuration options initially available with QlikView
Difficulty in achieving pixel-perfect design with Qlik Sense alone
Complexity of traditional patient medical records
Limited native capabilities of Qlik Sense
Maintaining and upgrading custom-built software
Need for powerful data visualisation capabilities
Manual process of creating customized presentations
Limitations with Qlik Sense in some fields
Rapidly growing database
Linking processes across the value chain
Data feeding from across Europe
Limited inpatient beds
Operational challenges impacting performance
Bottleneck due to small BI team spending more time developing apps than analyzing data
providing training and support to a large cohort of self-service business users
Loss of native capability with Qlik Sense migration
Qlik Sense not providing all needed functionality and customization
User dissatisfaction with initial versions of Qlik Sense
End users were more comfortable with Excel and Powerpoint
Limitations of native Qlik Sense in providing customizable colors for P&L reports
managing outpatient clinic bookings
tracking staff vaccination records
Poor performance of free analytics products with big data
Lack of visual quality with Qlik's native functionality
Not all users had the level of understanding necessary to work with and analyse data effectively
Lack of visual objects in-bundle with Qlik Sense
Preference for table formats
Limitations of Qlik's native capabilities
Achieving specific business requirements
Lengthy data processing times
Limited functionality of Qlik Sense for financial reporting
Building trust
Getting started
Elevating Data Literacy
Loss of capabilities during QlikView to Qlik Sense migration
Difficulties in transitioning customers from QlikView to Qlik Sense
Combining different data sources
Developing custom solutions to meet business requirements
Compatibility maintenance with new browser or Qlik releases
Missing key functionalities in standard Qlik Sense
Maintaining essential analytics functionalities during migration
Transition from QlikView to Qlik Sense while maintaining functionality
Managing data input complexity with linked Excel sheets
Growing complexity
Manual processes consuming staff time
Difficulties with previous ERP systems
Initial resistance to change due to 'if it ain’t broke don’t fix it' mindset
Complexity of data investigation across different modules without Spreadsheet Server
Running multiple versions of Oracle E-Business Suite
Prohibiting end-users from accessing required information
Existing website could not meet modern eCommerce best practices
Product information taxonomy and content not structured for Magento
Parsing text-heavy data into SKU-level attributes
Developing second data model for Frameworks
juggled disparate manual processes for creating and maintaining product information
multiple data silos across the business
ecommerce and catalog teams often generated conflicting product information
connecting networks and systems
Processing large external file uploads
Building custom programs for legacy data conversion
Manual month-end FI upload process
Unsuccessful attempts at in-house direct-upload solution
Resistance from users on the legacy system to migrate to SAP
managing product information across multiple custom systems
support for thousands of end users globally
need for consistent product data in multiple languages and countries
Analysts needing access to Spark data using preferred BI tools
Need for SQL-query approach familiarity
evaluating present and future financial needs
identifying gaps within SAP functionality
building tools that cost time and money and lack flexibility
duplicate entries creating error messages
consuming valuable time in payroll upload
Manual entry of customer and document reference details
1000 line-item limit in SAP
Manually loading and updating data in SAP
Complex routines and reliance on outside consultants
Needed a tool to streamline data management
Lack of SAP-experienced resources
Required a tool that didn't need SAP expertise
Need for accelerated data-processing
Requirement for cross-platform connectivity
Information not found
Need to access critical lab data efficiently and reliably
Ensuring trusted data for informed recommendations
Slow and complex reporting processes
Manual recompilation of data
Steep learning curve with native tools
New SOX compliance processes
comply with heightened regulatory and privacy standards
"access to consistent, accurate and reliable data"
proper governance
monitor who and how data is being used
strong handle on what data is where
"replace existing web services ODBC driver with a faster, more cost-effective solution"
"Handling multi-dimensional big data (volume, velocity, and variety)"
Difficulty in connecting to evolving data sources
Enable faster and more integrated development cycles
Improve end-user customer experiences
Enhance product quality
Minimize the need for data-source-specific infrastructure and expertise
Need for data connectors to connect ODBC-based client applications
Manual effort in SAP systems
Manual invoice activities
Migrations related to SAP upgrades
volume and complexity of SAP data updates
reassignment of classes and attributes for configurable product inventory
"manual process of preparing, formatting, checking, and transforming spreadsheets"
Outdated SAP ECC instance
Need for proper stock transfer procedures
Reworking Production Supply Areas and resource definitions
Data migration complexity
Complex SAP landscape making it difficult to retrieve data for daily operational activities
Need for extensive reporting capabilities on Oracle ERP data
Generic nature of canned reports in Oracle R12
High labor and time required to build customized views in-house
Lack of real-time data capability in Oracle BI Apps
tremendous volume of reports
duplication issues
obsolete report content
identifying a solution to streamline reporting and reduce customized reports
Loading large amounts of financial data into SAP
Cumbersome in-house solutions
Lack of validations and line item limitations
Drive greater adoption with data scientists across large enterprises
Empower users to connect to their databases and solve the challenge of data access to any data source
Accelerated category leadership with acquisition of Rsam
"Providing integrated, automated view of risk"
Rapid growth of new data sources and types
Data fragmentation in thousands of transaction tables
Information not reflected in business terms
Rework of reports required during Oracle upgrade
No Discoverer help file available
Developing custom views in Discoverer was expensive and time-consuming
Manual financial operations using Excel
Lack of operational reporting
Pandemic-related supply chain disruptions
Soaring fuel prices impacting inventory management
Finance department heavily dependent on IT for custom queries
Overwhelming workload for IT team
Complex global regulations surrounding food quality and handling
Minimal real-time reporting
Manual processes
Data not properly mined
Managing multi-currency consolidation
Rapid company growth and acquisitions
Aligning various data streams
Multiple reporting tools in use
Dependence on IT for report creation
Slow and structured reporting process
Lack of Excel friendly reporting
Time-consuming reporting process
Need for reliable data
Lots of unused data
Incomprehensible information from disconnected sources
Need for a central data repository
Reliance on manual processes during month-end and year-end close
Dependency on IT for ad hoc reports causing delays
Issues with data accuracy from ERP to Excel manual transfer
Transformation to adapt to changing economic conditions
Data spread out across different databases
No ETL expertise
"Disintegrated, separate, stand-alone housing system disconnected from finance system"
Tight timeframes for implementation and reporting
Backward-looking month-end reports
Limited visibility into operations
Inconsistent data fields and report information
Manual data entry resulting in inaccuracies
Reliance on IT team for information or ad hoc reports
Outdated applications
Manual tax data management processes
Pressure to meet challenging deadlines for month-end close
"Need for accurate, real-time data"
Answering ad-hoc questions effectively
Improving user perception and attitude toward SAP
Data overwhelm due to large amount of information
Handling increased processing and volume during the pandemic
Automate SAP data
Reengineering master data
Migrating to SAP S/4HANA
Relied heavily on multiple spreadsheets made by legacy employees
Time-consuming to fix errors
Needed to reduce human interaction
Providing unique data layouts and greater interactivity
Building generic reports for diverse client needs
Lack of data analytics expertise
Vague requirements for analytics solution
Needed a way to unlock vast amounts of cybersecurity data
customizability and control issues with other BI solutions
too much room for mistakes with spreadsheets
Need for quick implementation with minimal development effort
Integration of data from different applications
Significant investments in equipment and systems for packaging and data collection
Manual process requiring many man hours and using stale data
Rejection of Crystal Reports due to incompatibility with web-based architecture
Rejection of Business Objects due to time and costs
Lack of impressive visual elements in Microsoft’s SQL Reporting Services
Adapting to the various moving parts of broader ongoing data transformation program
Onboarding new products during implementation
Maintaining project on track with large data ecosystem transformation
Complex and cumbersome initial reporting tool
Users not accustomed to creating reports
Data elements difficult to identify
Financial data siloed across multiple systems
Impact of COVID-19 on sales and hiring freezes
Syncing data from acquired entities
manual processes taking a lot of people many hours
limited resources within the tax team
managing multiple processes simultaneously
Compliance with California regulations
Need to mine and handle data from various sources
Produce intuitive reports without requiring training for non-technical end users
Need for real-time dashboards for ticket sellers
Desire to provide personalized and customizable reports
high maintenance and development resources for homegrown analytics
difficulty in building custom reports and fixing issues
limited preset reports and lack of data aggregation/visualizations
diluted development cycles due to analytics module
High volume of custom report requests
Need for real-time data
Maintaining and updating a homegrown solution
Skills gap for building and maintaining analytics
Previous data-reporting tool was cumbersome
Needed to suit a range of users with varying technical expertise
Reduced errors and proofing cycles
Time-consuming manual planning process
Costly delays due to inefficient data management
Dated business software used for 40 years
No staff members with skills to build new reports
Supply chain issues due to international trade restrictions and COVID-19
Siloed data within business units
ERP solution lacked depth in reporting
Slow and inefficient process with Microsoft Excel linked to SAP BPC
Difficult version control and file management
Manual building of database views in SQL
Difficulty in drilling down detailed data in large Excel files
Loading and refreshing formulas was slow
Manual and time-consuming processes
Data spread across multiple sources
Small tax department handling complex tasks
Risk of human error
Connecting multiple General Ledgers
Managing numerous spreadsheet files
Risk of lifting and shifting existing core processes into another system
Disjointed approach to note keeping in financial data
ERP system was not suitable for their business model
manual and error-prone reporting process
ERP data tables in Dutch with complex abbreviations
high consultant fees
Retrieving data from Microsoft Dynamics NAV was difficult without the right report
Standard reports did not meet output requirements
Had to wait at least a day for system partner to create or fine-tune reports
"Manual, spreadsheet-driven process"
Limited communication between business units
Lack of visibility into the reporting process
No hierarchical reporting with existing data warehouse
Reports lacked visual appeal
Difficulty in communicating requirements from 'NAV' to 'cube' language
Unpredictable update times with previous analytics software
High costs associated with the previous BI tool
Running many manual reporting processes
Struggling to create and adapt reports
Lack of flexibility within their ERP
Manual feeding of data from six different ERPs
Difficulty managing legacy data
Complex and convoluted data uploading process
Time-consuming manual report building
Resistance to change during migration
Out of the box reports don't provide needed flexibility
Finance professionals end up dumping data into Excel
Clients get stuck setting up reports
Never published consolidated financial statements before
Managing large numbers of contracts in different currencies
Facilitating comparative analysis of budget and actual figures
Identifying and managing intercompany transactions
Managing and monitoring lease contracts over time
Training several tens of overseas users
Laborious and manual reporting process
Difficulty in maintaining auditability and consistency
Adapting to rapid regulatory changes
Manual effort
Input errors
Lack of transparency
First-time consolidation in 2016
Standardization and automation in the IDL environment
reliance on outside consultants for report writing
cost and time inefficiency
difficulty in modifying and creating reports
Significant resources consumed during closing process
Manual effort to create and cross-check reports
Limited bandwidth for analysis
Late changes required extensive validation
FRS reports weren't flexible or intuitive enough
Essbase Spreadsheet Add-In drill-through feature was inconsistent
Migration from Microsoft Dynamics NAV to Odoo with legacy data in NAV and QIS
Increasing amounts of data to report on
Complex and fragmented existing IT infrastructure
Difficulty in accessing data efficiently
Finding a balanced solution for BI and reporting
Manually intensive and time-consuming reporting with native tools
Delayed data availability up to 45 days
Manual cleanup of data from AS/400 system
Time-consuming report generation
Lack of SQL coding knowledge
Reliance on homemade Excel spreadsheets
Time-consuming budget consolidation
Generating critical errors in spreadsheets
Difficulty in data extraction and combination for reports
Combining data from more than eight Microsoft Dynamics NAV databases
End users had difficulty navigating NAV systems to find necessary data
Manual effort in exporting and stitching data together in Excel each month
Using two separate ERP systems
Complex and technical reporting with SSRS
Errors from manual data handling
Time-consuming manual report generation
Time-consuming and frustrating process using Internet search engines and regulatory websites
Difficulty determining if documents were bilingual or only in English
Major influx of information to publicly disclose
Difficulty in searching for specific information
Complex data transfer between sources
Manual efforts to keep numbers accurate
Lack of audit trail
Consistency problems due to amendments to accounting policies
Last-minute changes from subsidiaries
facilitating input from multiple parties
manual input and copying of data
tight deadlines
Reliance on manual processes threatening timely and reliable report delivery
High risk of error and time-consuming manual data entry
Manual reporting processes not viable
Urgent need for a solution to generate XBRL files
Duplication of effort in annual report production in two languages
Manual report preparation was laborious and error-prone
Pressure of converting reports to EDGAR-compliant format
XBRL reporting requirements in two jurisdictions
Manual reporting was slow and error-prone
Input errors and version confusion
Corrupted files and formatting inconsistencies
Limited collaboration with only one person working on a document at a time
Tedious search for IFRS standards and examples in multiple locations
"Relying on EDGAR, SEDAR and internet search engines was exhausting"
Pinpointing specific information was tedious and time-consuming
Meeting compliance obligations
Improving efficiency
Managing complex and varied award types
Inaccuracy of existing Excel spreadsheet
Current software not GAAP compliant
Reporting required raw data dump and massive recalculation
Manual reporting was risky and not scalable
Risk of accounting errors with spreadsheets
Handling complex calculations for stock options
Managing combined equity compensation plans post-acquisition
Reluctance to move to cloud solutions due to data loss concerns
"Reliance on old, inflexible Management Reporter tool"
Use of manual processes and multiple spreadsheets for budgeting
Manual manipulation of cost center data
Time-consuming budget variance report production
Navigating to a remote desktop to design reports in older versions
Learning curve due to skipping several versions in the update
FRx being phased out
Ensuring data accuracy during migration to NetSuite
Time-consuming reporting process
Inability to template reports due to changing formulas
Increased number of hours and personnel required for reporting
Time-consuming manual reporting processes
Lengthy Hyperion uploads
Five-day month-end close process
Drastic change in reporting needs due to COVID-19
Different forecasting processes for each business unit
Antiquated and difficult-to-maintain tools
Labor-intensive manual processes
Error detection late in the process
Pulling data from multiple disparate systems
Labor-intensive data collection and report development processes
Limited forecasting and 'what-if' modeling capabilities
Reduce reporting deadlines
Manual processes and lack of automation
Resistance to change from staff
"Excel-based consolidation, budgeting, and reporting models"
Labor-intensive data collection and report development processes
Growth through acquisition and multiple systems and chart of accounts
Increased internal and regulatory reporting requirements
"Painful Excel-based consolidation, budgeting, and reporting models"
Labor-intensive and very cumbersome data collection and report development process
Time-consuming and manual process
Increased risk of errors
"Multiple disparate systems required to support consolidation, budgeting, and reporting processes"
Labor-intensive data collection and report development process
Internal management and regulatory reporting data integrity risks
Managing multiple spreadsheets
Reviewing formulas and links manually
Concerns over errors and lack of internal controls
"Excel-based consolidation, budgeting, forecasting, and strategic planning"
Labor-intensive data collection and report development process
Reporting errors on internal management and regulatory reports
Not enough time for data analysis
Labor-intensive manual processes
Complicated budgeting and forecasting system
Extensive departmentalization
Potential for errors with manual steps and spreadsheets
Balky and labor-intensive Excel-based dashboards
Manual processes prone to errors
Limited functionality for detailed analysis
Obtaining buy-in and adoption of the new system
Dependency on skilled resources for SQL coding
Costly and time-consuming data migration
Maintaining and updating the old data warehouse
ERP system change led to BI solutions not working as planned
Previous ERP solution couldn't handle financial planning and reporting
Developing a Power BI tool was time-consuming and expensive
Using Excel compromised data and analytics quality
Default reporting was insufficient
High cost of custom report design
Long delays in obtaining tailored reports
Lengthy and cumbersome manual processes in creating reports
Difficult for individual users to create standard reports
Time-consuming reporting to various authorities and insurance providers
Risk of losing financial authorization due to lack of satisfying reporting
Risk of contract termination due to lack of internal control
Dealing with large volumes of data from various systems
Ensuring fast and secure access to management information
Only able to close its books once a year
Did not have a system to calculate contribution for each customer
Manual update of budget templates
Risk of data loss or errors
Fluctuations in demand based on various factors
Complexity due to COVID-19
Translating data into a row-column format for BI tools
Digital transformation across organization
Varying needs by department
Knowledge barriers of specialty software
Technical limitations of traditional business analytics platforms
Lengthy and error-prone process of collating screenshots from various industry platforms
Legacy system with PHP scripting and Excel files
Expense and limitations of Tableau
Complex challenges in product development
Building for a diverse customer set
Static nature of spreadsheets for exploratory and small-to-mid sized projects
Difficulty in customizing and developing with traditional BI solutions
creating and communicating custom statistical analyses
Backlog of work orders
Unresolved customer complaints escalating
Delays from an incumbent software vendor
Legacy software was only available on a single office computer
Inability to customize existing solutions without high cost and long delivery times
Difficulty in tracking and monitoring TBMs
Converting insights into actionable business decisions
Creating highly customized dashboards for clients
Working with disparate large datasets
Customizing dashboards became repetitive and time-consuming
Google’s Looker Studio lagged behind in accommodating sentiment data
Navigating environmental and safety regulations
"Dealing with changeable external factors like weather, seasonality, and hardware limitations"
Managing hard-coded curtailments in turbine control software
Ensuring an optimal end-user experience
Implementation of major changes with care
Uncertainty around the physical impact of wake steering on wind turbine equipment
Need for adaptability and portability
Requirement for more than basic visualizations
Scaling complex scientific processes
Ensuring experiment specifications are met as novel experiment types are developed
Regular updates of machine learning models with new data
Ensuring seamless access without re-authentication
Developing complex data applications beyond lower-code tools
Visualizing unusual datasets like vibration condition monitoring
Customization limitations in existing tools
Radical market changes in commodity trading
Competitor behavior
Communicating results of machine learning and optimization algorithms
Visualizing complex shipping and logistics solutions
time-insensitive and costly research
lack of full-stack web development team
building a full-service data vault
managing multiple data apps for a global platform
Traditional business intelligence solutions were insufficient
Needed a robust future-proof data and analytics architecture
Needed to get to market quickly
No single authority providing a comprehensive list of rare disease definitions
Changing data formats from sources like Orphanet
"Lacking a central, trusted source of rare disease definitions"
No consensus on the number of rare diseases
Time-consuming modifications to accommodate new data formats
connecting to varied data sources
getting data into the right shape
categorizing data into key customer segments
connecting to varied data sources
shaping data and categorizing it into customer segments
ensuring recommendations remain relevant in the future
High cost and difficulty of maintaining third-party solution
Inability to handle in-house updates and easily implement new features
Manual research for detecting organized fraud rings
High costs and maintenance difficulties with third-party solutions
"Complex, time-consuming claim processes"
Manual research for detecting organized fraud rings
"Manually reading and categorizing 30,000 work order problem descriptions"
Extracting detailed information about fluorescent tube light changes
Ensuring the analysis was accurate and objective
Manual reading and subjective categorization of work orders
Lack of detailed tabular records for specific changes
Aggregating data at the client level could dilute savings benefits
Need for a reliable and flexible platform
Requirement for reproducible workflows and standardization
Early detection of infrastructure variations
Need for a flexible software environment
Requirement for reliability and reproducibility
Standardization of operations
Calculate necessary properties via command-line
Ensure compatibility with KNIME nodes
Setup of properties calculation to cover various problems
Define proper input and output format from ORACLE™ view
Ensuring ACD/Labs Percepta can calculate needed properties via command-line
Compatibility of results with standard KNIME nodes
Defining the structure’s format of the input and output tables
High attrition rate
Imbalanced attrition datasets
High attrition rate
Imbalanced attrition datasets
Maintaining a clean dataset
Compensating for low-quality data
Low-quality data
Manually cleaning data-streams
High initial cost of commercial business models
Need for rapid dissemination of data analytics competence
high license fees for infrequent users
Expansive and volatile global stock market
Thousands of stocks with constantly changing values
Keeping track of thousands of stocks
Lack of reliable and current data
"Repetitive, manual tasks in spreadsheets"
"Processes not well-documented, not easily scaled or centralized"
Handling peak workloads with over 600 individual processes
"Repetitive, manual tasks in spreadsheets"
Tasks prone to human error
Excel processes not well documented and not scalable
Handling peak workloads
Collecting information from multiple sources
Slow process pace
Need for technical comments on compiled figures
Collecting information from multiple sources
Slow pace of the process
Complex and difficult quality control in project-by-project approach
Complex and hard-to-quality-control workflows
Time-consuming for business analysts to adapt workflows
Inefficiency of trial and error approach to optimizing merchandise levels
Optimization of merchandise levels through trial and error
Error-prone manual processes using Excel
Lack of version control
Limited ability to process large datasets
Manual and error-prone Excel-based model
Lack of version control
Limited ability to process large datasets
Time-consuming manual calculations
"Inefficient, cumbersome, and manual two-week process"
Monthly basis analysis using CSV extracts
"Inefficient, cumbersome, manual two-week process"
Originated with CSV extracts
Excel crashing mid computation
Resource-intensive IBM Congos implementation
Delays and difficulties with previous solution
Excel crashing during large data computations
Manual process being extremely error-prone
Resource-intensive implementation with IBM Cognos
Desired speed and interactivity not met with IBM Cognos
Complex models and large projects typically requiring significant development resources
Complex models
Getting functionality into production on a complex platform
Handling different data types and sources
Ensuring completeness of OCR for each image
Automating the review of documents with multiple formats
Handling different data types and sources
Ensuring completeness of OCR for each image
Lack of data-driven decision-making
Significant manpower allocated to tasks that could be automated
Different market maturity levels across countries
Predicting product sales for promotions
Lack of data-driven operational decisions
Time and manpower allocated to manual tasks
Accurately predicting product demand and trends in different geographies
Identifying optimal store locations
Different approaches to monitoring overstock
Higher risk of stock outages for fast-moving products
Spending too much time on individual analysis
Treating symptoms rather than finding root causes
Different approaches to monitoring overstock at over 80 locations
Higher risk of stock outages for fast-moving products
Higher inventory costs for slow-moving products
Teams treating symptoms rather than causes of overstock
History is not enough to predict the future
Rigid forecasting processes
No two products are alike
(Un)-availability of accurate transactional data
History is not enough to predict the future
Rigid forecasting processes
No two products are alike
Unavailability of accurate transactional data
Harmonizing data from different channels
Need for uniform metrics across channels
Need for uniform data harmonization across channels
Reducing miscommunication between requirements and implementation
Time-consuming and manual data integration from multiple systems
Complex data from multiple sources
Manual and time-consuming data integration
Need for expertise in multiple data types
Complex data from multiple sources
Database optimized for short-term reporting
Data team becoming a bottleneck
"Mixed dimensions, facts, and grain in ETL"
Complex and inefficient reporting processes
Inadequate OLAP cube setup
Database issues impacting business functions
Database architecture geared towards financial reporting was unsustainable
Data team became a bottleneck
Complex business logic embedded in the ETL
Inefficient reporting processes
Issues with Tableau server scheduling and data loading
Detecting unknown patterns in money laundering
Ensuring data literacy among non-tech auditors
Detecting unknown patterns in money laundering
Making machine learning accessible to non-tech auditors
Long wait times to get required data
Complexity in data processes and structures
Long wait time for data retrieval
High number of false positives in monitoring
Complex and time-consuming procedures
Manual and incomplete competitive analyses
Complex and time-consuming procedures
Manual and repetitive tasks
Large variations in input files
Large variations in input files
Manual process of gathering and summarizing expenses
Errors from manual copying and pasting
Manual gathering of Excel files from each employee
Combining multiple Excel files into a global file
Variety of backgrounds within the team
Different levels of coding experience
team members had different levels of coding experience and BI tool expertise
Labor-intensive manual data collection and reporting
Delay in reporting affecting operational decisions
Labor-intensive manual process of collecting and transforming data
Lag in reporting affecting operational decisions
Off the shelf solutions not meeting custom requirements
Interpreting the prepared data from Phase 1
Checking the plausibility of new prices
Weighing up different pricing measures
Need for pricing expertise to interpret data
Need for systematic price adjustment
Difficulty in tracking employee work hours across multiple locations
Incorrectly invoiced prices and complex pricing structure
Inconsistencies in product master data with large specifications
Difficulty in tracking employees' working hours across locations
Incorrectly invoiced prices and complex pricing structures
Inconsistencies in product master data
Integrating and merging various data sources
"Ensuring accurate mapping of customer ratings to specific ocean carriers, ships, and cabins"
Integrating and merging various data sources
Lack of a unified reporting dashboard
No automated information flow
Non-interconnected data sources
"Manual, labor-intensive, and slow reporting process"
No unified reporting dashboard accessible to all stakeholders
No automated information flow leading to delays in report creation
"Interconnected data sources were lacking, making reporting labor-intensive and slow"
Combining diverse data entities
Abstract chemical representation
Heterogeneity of data formats
data-intensive nature
combining data from multiple databases
heterogeneity of data formats
abstract chemical representation
Detecting problems early in experiments
Timely and reliable decision-making
Detecting problems early on in experiments
Efficiency in processing large datasets
Transferring ready-made models from development to production
Long implementation times and errors in the previous system
Transferring ready-made models from development into production
Slow model implementation time of one to two months
Dependency on software developers for model transfers
Automatically create personalized email campaigns
Collect data from various databases
Clean and categorize data
Secure data infrastructure
Automate and schedule workflows
"Automatically create personalized content based on user behavior, preferences, and habits"
Secure the infrastructure
Ingest data from data warehouse to HDFS Hadoop distribution
"Time-intensive, manual data blending"
Blending different data that have no natural link
Mapping lists usually prepared manually involving experts
"Time-intensive, manual data blending"
Blending different data that have no natural link
Handling millions of data points from multiple internal and external sources
Handling errors such as missing values and invalid entries
Ensuring robustness of models
Poor inventory management
Handling errors such as missing values and invalid entries in data
Uncontrollable variables in the industry like clinician preference and supplier changes
Redundant and expensive EOL testing
High energy costs for physical testing
Huge carbon footprint impact
High cost of physical EOL testing
Time-consuming manual measurements
High energy consumption for stress tests
"Ingesting, cleaning, and combining data from multiple sources"
Painstaking manual process for data transformation
"Struggled to provide timely, dynamic marketing reports"
Bottlenecks due to data demand being placed on data science team
Siloed nature of reports
Inadequate data sharing tools
storing and processing vast amounts of data
gathering and analyzing data quickly
ensuring secure handling of patient-level data
dealing with high traffic on dashboards
Fragmented data silos
Manual data reporting
Lack of data governance
Outdated BI tools with plain tables
Excel sheets unable to handle colossal amount of data
Need for a company-wide data-driven mindset
Necessity to submit findings in a standardized and centralized way
Needed to save evaluation time
Establish a baseline of external image
speed to market
slow information dissemination (two weeks)
need for secure data access
reports from the police system were tabular and not well structured
manual effort to create maps
increase in information requirements
"Relied on a patchwork of tools including SQL, Chronos, BBA, and Excel"
Tools lacked visual analytics aspects
High skill and time required to work with existing tools
issues related to homogeneity and reliability of results from multiple data visualization tools
complexity related to technology and costs
Time-consuming data analysis
Issues with version control using Excel
Difficulties in drill-down analysis in Excel
Reliance on standard software for analysis
Quantifying employee development
Consolidating data from multiple disparate sources
Analyzing written feedback
Existing network of disparate spreadsheets
Lack of fit for purpose BI tools
Multiple disparate approaches to collaborative analytics and data sharing
No real architecture in place
siloed and fractured data
manual data pulling from legacy systems
disparity and complexity of data
difficulty in updating and sharing SQL queries
Time-consuming manual reporting process
Difficulty in consolidating data from multiple sources
Need for more efficient data visualization tools
Understanding customer movement during renovations
Spotting issues with promotion data
Information not found
Managing large data sets
Creating ad-hoc reports
Growing complexity with increasing company size
Shift to remote work during COVID-19 pandemic
Complicated to see the big picture for key business decisions
Struggled to make connections among siloed data
Needed to lessen the need for on-premise deployment maintenance
Managing multiple data sources
Inefficient manual data analysis in Excel
"Complex data integration from Oracle, Hadoop, Teradata"
Ranking vulnerability for patient cases and managing care delivery priorities
Accelerating data transformation due to COVID-19
Adapting to new technology uses and new kinds of collaboration
Producing different versions of the same analytics for KPIs
Lack of statistical discipline and comparative data
Data stored in different systems and formats
Physicians' skepticism towards healthcare data
Different performance reports saved in different places
Slow adoption due to data limitations
More than 20 disparate data sources
36 different applications tracking various data
"messy, disparate data"
"time-consuming collection, combination, cleaning, and categorization of data"
manual data preparation and analysis
streamlined delivery method
"Overwhelming task of ingesting, cleaning, and combining data from many sources"
Painstaking manual process to pull and transform information
"Struggled to provide timely, dynamic marketing reports"
economic recovery
fragmented and siloed legacy systems
On-premises server struggled with internal demand
Physical servers crashed intermittently
"Lost 2,000 hours of productivity per month"
Complexity of cloud migration for internal users
Initial lack of a dedicated technical team
Need for quick iteration based on client demand and user feedback
Manual data entry and cross-referencing from various spreadsheets
"Difficulty in making real-time, impactful business decisions"
Managing data silos
Accuracy of data
Manual consolidation
User adoption
Handling different analytics demands globally
Handling large-scale data during the pandemic
Uncertainty around vaccine quantities and procurement times
Data knowledge centralized in a single team
Difficulty fulfilling data requests
Disconnect between company’s culture and existing BI strategy
Cultural resistance to data access
Complexity in unifying data from different sources
Need to transform to face industry disruptors
Moving past simply viewing data to engaging with it
Building a data culture
Establishing and nurturing a data culture
Managing a large volume of data (around 50 terabytes)
Varying levels of prior technical exposure among users
Modernizing legacy retail analytics platform
Reliance on specific individuals’ skills for Excel macros
Lack of complex and interactive features in Excel
Information not found
Traditional statistics lessons were inadequate for the 21st century
"need to provide agents with data on product availability, performance metrics, and expected business results"
Lack of trust in data
Users' comfort level with data
Securing executive support
"Fragmented, siloed global payroll data"
"Need for timely, accurate payroll reporting"
Simplifying logic without disrupting the previous approach
Ensuring the confidence of users in the new system
Challenges with data flow from cloud to on-premise systems
Managing data manually
Establishing a formal data literacy program
Uniting the people involved in WFP operations around a single community of shared data
Shortage of medication and supply
Lack of hospital resources
Fragmented systems and databases across the ecosystem
"Explosion in data from connected technologies, mobile devices, and cloud software"
Need for specialist skills to write custom code and SQL
"Maintaining usability with security, data quality, integrity, governance, and confidentiality"
Finding the right talent with evolving roles and skills
Manually collecting data from different sources
Complying with different regional privacy regulations
Initial manual process with heavy coding emphasis
Requirement of expert knowledge for embedding dashboards
Manual effort to model data and create dashboards
Driving adoption of new technology
Business teams had to request IT for reports and dashboards
Delays in report and dashboard delivery due to complex queries and coding
Storage space limits due to rapid adoption
Dissatisfaction during the initial migration period
Complex queries from previous forms required time-consuming study
Analyzed multi-big data with tools developed in-house
Needed a solution to reduce reporting time
Required faster market response
inefficiency of extracting insight from vast amounts of data
"constraints around filtering, extracting, and presentation of data"
Long time to download log data and create visualizations with Excel
High costs of developing and improving data collection software
Data held in silos
Different reporting formats
Manual reporting processes
Manual juggling of data
Ensuring accuracy in payor data
Getting people to understand and use the data
Managing the complexity of growing data
Static nature of previous data collection methods
Lack of real-time data
Need to track new trends emerging post-study
Adopting a new way of doing things
Ensuring a functioning data culture
Low interest and literacy in data utilization for internal operations
"Lack of knowledge in specific work fields (e.g., construction equipment)"
Managing reporting and analysis with spreadsheets became unsustainable
Needed a robust business intelligence tool
Slow and manual analytics reporting cycle
Requirement to quickly process large volumes of data
Difficulty in making the final result of analyses appreciated
Risk of results getting lost behind complex narratives
Complex existing tools with limited visualization
Manual data extraction prone to human error
Lack of a complete picture of data
Bringing together different datasets
Managing data quality from multiple sources
Customers wanted more data points
Need to develop features and capabilities from the ground up if building custom
Information not easily accessible
Virtually no integration among systems
Long-standing issues of access to information
Original data platform strategic shift
Tight migration timeline
COVID-19 pandemic impacting training and migration processes
Mix of legacy and new systems and applications
Strict regulations and oversight requirements
Multiple Tableau instances created silos
Risk of inconsistencies
"Need for efficient and effective way to analyze, visualize, and understand data"
Eliminating manual processes
Managing data from over 100 different systems
Initial resistance to change
Complexity of data residing in different systems and formats
Automating decisions previously made one-on-one
Ensuring all employees could use data effectively
Availability and accessibility of data
No single source of truth for information
Reports were not easy for non-technical users to understand
Manual reporting with spreadsheets prone to error
Time-consuming report preparation (one to two weeks)
Complex business operations requiring constant report recreation
Managing diverse analytics tools
Starting to operate independently post-incorporation into NSHD
communication issues within the team
long list of report requests for IT team
need to simplify data analysis and reporting for non-IT users
Ineligibility for aid for undocumented and mixed-status families
No rental assistance until December 2020
Limited distribution of funds by July 2021
Understanding the impact of the pandemic on customers
Adapting from in-person to remote work
Need for control over data
Shortening development timelines
Outsourced and heavily manual data analytics systems
Large amounts of data insights were overlooked
Overwhelming client and staff with unnecessary alarms
Manual checking of client records led to inefficiencies and human error
Data stored in local spreadsheets and disparate silos
Error-prone manual integration
Poorly contextualized data
Low data usage (only 4%)
Long wait times for information requests
Time-consuming data management
Slow data refresh times
product that customers may not want to use
managing sheer volume of data
data quality and data drift
responsible use of data to avoid bias
Business realities not easily presented up front
Difficult to predict sales and earnings
Issues with user operations and inefficiency
Aggregation/analysis work performed in each user department
Data analysis using spreadsheets was hard to comprehend for non-experienced employees
Employees relied on IT for reporting
Data siloed across legacy systems
Difficulty in getting a single view of customers and their journey
Global competition and complex supply chains
Increasing demand and server capacity issues
Handling massive amounts of data exceeding millions of units per month
Time-consuming data aggregation work performed dozens of times a month
Limitations on data volume handling
Need for dynamic visualizations
Different monthly sales reporting tools per channel
Difficulty comparing data from various formats
Initial reports still difficult to read and use
Different staff perspectives on data use
Manual labor and errors in Excel
Lack of prior experience with Tableau
Manual processing of incoming data
Handling KPIs for 14 markets
Team's reliance on existing spreadsheets
Difficulties organizing and processing data from surveys
Overflowing spreadsheets and multiple data sources
Identifying specific problem areas in stadium seats
"Data insights locked in separate, fragmented systems"
Multiple versions of the truth due to different BI solutions
Ineffective way to share data
Limited overview of business performance
Planning for ICU and hospital bed capacity
Significant revenue loss due to shutdown
Accelerating digital transformation
Time-consuming analysis
Difficulty in drilling down into details
Delayed insights
Planning for the unplanned
Ensuring appropriate faculty ratio for classes and hybrid instruction
information silos across departments
lack of centralized information
Adapting to COVID-19 market changes
Shifting from door-to-door sales to online model
2020’s challenging global landscape
Integrating multiple data sources
Managing analytics internally across platforms
Onboarding users to a new way of accessing data
Tracking and managing complex leasing performance during COVID-19
Rapid organizational growth leading to a mix of personnel with and without SQL skills
Labor-intensive visualization approach with Redash
Users not effectively utilizing previous BI products
Dependence on Excel for data analysis
Struggles with unrestricted self-service analysis
turning large volumes of data into actionable insights
lack of industry-wide context for individual businesses
diversity of data growing at a rapid rate
Legacy tool was overly complex and a burden to manage
Concerns about infrastructure upgrades breaking dashboards
Extra work involved in managing the infrastructure as data volumes grew
delay in access to information
manual report creation
Relying on self-service methods during the COVID-19 pandemic
Understanding customer orders across various tracking systems
data was not always fast or easy to access
spreadsheets used for internal analysis and reporting
long wait times for data to be prepared
Disparate data sources and legacy systems
Manual work by many employees
Data quality and reliability issues
Fragmented ecosystem with multiple production environments
Resistance to change from employees accustomed to list-based reports
Fragmented data landscapes
High external spend and invoice volumes
Difficulty in maintaining a comprehensive view of operations
Engaging students on new technologies in a mostly rural state
Students being first-generation college attendees
Hesitancy to invest time in learning new skills
Previous solution required too much coding to extend visualizations
Aggregating health data containing sensitive customer information
Analyzing large volumes of data
Manual connection to servers
Difficulty in viewing decentralized data at a glance
lack of intuitive and interactive dashboards
data visualizations performed on a case-by-case basis
Difficulty in tracking and analyzing data within spreadsheets
Requirement of expertise using spreadsheet formulas
Files failing to load
Difficulty for server capacity and performance to keep up with the rate of growth
lack of access to information like finance KPIs
time-consuming reactive reporting by the BI team
Reliance on spreadsheets
Time wasted on consolidating data from different sources
"Collating, analysing and visualising information from different data sources"
Siloed analytics and reporting leading to data discrepancies
Collecting and analyzing management data with Excel was labor-intensive
Data being out-of-date by the time of management meetings
Issuing instructions based on old data caused confusion
Rethinking data model and architecture for scaling
Identifying employees for Data Rockstar role
traditional data analysis methods required a lot of repeated operations and provided poor visualization
need to build a data analysis mindset among employees
Centralized data extraction under IT
Need for democratized data access
Siloed systems
Non-standardized ways of working
Reliance on IT for data exploitation
Data siloed in different systems
Manual extraction and consolidation of data was time-consuming and error-prone
"extracting data from a database, preparing the data, and creating a report used to take weeks"
complexity of the product portfolio necessitating live call center agent
Manual data analysis process
Relying on a small group of skilled programmers
Time-consuming data cleaning and preparation
Complex and inaccessible data analysis with programming tools and spreadsheet formulas
Need to analyze large historical datasets
BI solution built on open source software not supporting future needs
Central BI team inundated with everyday business questions
Wasting employees’ valuable time due to bottlenecks
MicroStrategy didn't meet needs for fast delivery and professional visuals
Reports compiled using spreadsheets took too long
Hesitancy about using a new platform
"Data was not standardized, making it difficult to build a consistent analysis and reporting system"
Data silos led to non-transparent decision-making
Long reporting times before using Tableau
Huge amount of data from different platforms
Expensive and time-consuming data integration
Dispersed data sources
ensuring business moves as a connected whole
need for a single source of truth & deeper insights
decentralized environment
Initial debate on whether to develop own analytics tool or use a solution like Power BI
Increased service demand during COVID-19
Maintaining employee safety
Adapting to home-based work platform
Understanding customer risk profiles
Ensuring data storage and governance
Large amount of false and exaggerated data in the real estate market
Difficulty in quickly gaining necessary insights into the real estate market
Issues with data consistency using basic spreadsheets
Inability to effectively analyze large amounts of phone data
departments functioned mostly in silos
rise in 'shadow IT' with various business groups using their own vendors
IT team had to learn and support many disparate systems
Tech-savvy competency among less tech-savvy workers
Resistance to large-scale operational change
Using numerous platforms and channels for analysis and reporting
Difficulty in measuring and forecasting performance accurately
Increasing pressure on Insights team
Deciding what data to include
Telling the data story effectively
Hard to administer previous BI tools
Inefficiencies with data cleansing using Microsoft Excel
Struggles in keeping customers engaged due to changing requirements
Dependence on IT department to extract and analyze data
Extensive SQL training requirements
Existing BI system highly dependent on IT
"Complicated and repetitive manual work from data query, extraction, cleansing and analysis to report development"
Each hotel had its own set of manual reports
High risk of errors during manual updating process
Excessive running costs and reporting delays
Manual effort to model data and create visualizations
Need for real-time data access
Static reports with no interactivity or visual appeal
Product limitations of legacy BI solution
Decentralized analytics with static spreadsheets containing millions of rows of data.
Need to reduce IT burden for easier upgrades.
Manual process of downloading and wrangling data into spreadsheets
Reformatting each report for readability
Proliferation of dashboards hiding insights
Integrating diverse data sources
Manual input and rebuilding reports in Excel
Spotting insights within spreadsheets of flight/traffic data
Handling tens of thousands of booking cancellation requests
Reducing working hours while maintaining essential operations
Creating own version of the truth using different data sources
Reporting on performance to different stakeholders
Understanding supply and demand changes
Monitoring settlement risk in real-time
Labour intensive and time-consuming liquidity verification
Fragmented datasets
No data lake or data warehouse
Excel reached its limit when analyzing data from extensive server logs
Outmoded analytics solution causing long report generation times
Manual creation of over 900 HR reports
Lack of analytical insights in reports
Information not found
Need for quick deployment
Difficulty in getting customer demographic or browsing data for the South East Asia market
Initial reliance on spreadsheets for data analysis
Time-consuming creation of charts and graphs
Need for new and enhanced requirements due to the pandemic
Unique data collection challenges in each of the 80 countries
Scattered nature of collected data resulting in low trust
Long time to process information requests
Need for more secure access to data
Making the most of data in medical research
Moving away from paper content and spreadsheets
Difficulty in presenting valuable data in an easy-to-understand way
Lack of time and resources to build an embedded solution in-house
Uncoordinated grassroots efforts
Manual tracking of donations
Mismatch between what hospitals need and what people can provide
Combining large amounts of non-uniform datasets
Siloed data across different functions
Missing key insights for team performance
Limited capabilities for analyzing and presenting data
Backlog of analytics requests
Challenges in delivering timely executive reporting
Reduced number of in-person visits due to COVID-19
Manual compilation of performance reports taking several weeks
Complex analysis process before using Tableau
Time-consuming traditional data analysis methods
High volume of calls restricting emergency services
Virus-related calls to general emergency number limited tracking efforts
Manual reporting being time consuming
Creating a new data culture
Limited direct contact with customers due to third-party distributors
Difficulty in realizing data-driven management
Need for tools that users want to use and can use intuitively
Disruption to normal supply chains
"Shut down of local markets, transport companies, and warehouses"
urgency of COVID-19 response
large demand for new data and reporting
Using simplistic reporting and analytics tools before Tableau
Identifying cost savings and increasing efficiency
considerable time spent collecting and preparing data
inaccurate data leading to diminished trust
Manual process of downloading and publishing workbooks
Cumbersome customization process for each customer
Risk of deploying incorrect or outdated content
"Ensuring safety and wellbeing of over 14,000 employees across 130 locations"
data in multiple places leading to inconsistencies
varying report creation processes
Disparate systems
Difficulty in getting a holistic view of data
Tracking accuracy of loan applications and financial transactions
Relying on a patchwork of tools that took a lot of skill and time
Lack of visual analytics aspects
Inability to see the bigger picture in Grant Tracker
Lack of a holistic view of funding areas and project success
issues related to homogeneity and reliability of results
technologically complex and costly system
Data scattered across separate reports
Difficulty in detecting important signals amidst data noise
Outdated reporting processes
Efficient planning and management of predictive maintenance
Collecting and integrating growing amounts of diverse data quickly
Former BI tools being difficult to use and time-consuming
Dependence on the internal IT team for schema management and report development
Information not found
difficult to quantify employee development
data stored across multiple disparate sources
analyzing single Excel files without consolidation
high dropout rate in feedback forms
Overwhelming data abundance from 'seas of spreadsheets'
Challenging legacy actions and staff behaviors
time-consuming data analysis with Excel and PowerPoint
issues with version control
difficulty in drill-down capabilities
reliance on standard software for analysis
recurring IT problems
"data locked away in data warehouses, spreadsheets and other systems"
lack of data insight needed to run a modern healthcare organisation
Manual process of preparing and querying Excel spreadsheets
Building custom UIs for each client
Individual departments operating in silos
Fragmented data ecosystem
Organic growth leading to lack of governance
Abundance of unorganized dashboards affecting discoverability
Multiple versions of the same data source being published
Fragmented data silos
Conflicts and confusion from using PowerPoints and Excel
Manual report analytics were time-consuming
Third party services were expensive
"Handling large volumes of data (e.g., 30,000 employees)"
Data consistency problems
Fragmentation due to disparate sources
Slow and time-consuming insights using Excel
Difficulty in providing extra levels of insight with traditional methods
Outdated tools impacting productivity
Manual analysis being labor-intensive and time-consuming
Difficulty analyzing data in Excel
"Disconnected procurement, risk, and finance systems"
Time-consuming analysis stuck in spreadsheets
Legacy systems requiring data extraction interfaces
Fragmented data sources and tools
Resistance from some employees used to centralized reporting
State performance funding appropriation changes
Need for a deeper understanding of data
Scalability and intuitive solution needs
Technical knowledge required for other BI solutions
Fragmented data sources
Not all employees had access to key business and operational insights
Dependency on employees with deeper technical knowledge
Static data used during meetings hampered speed of decision-making
Data Warehouse team took days to process information
Cumbersome process of sending excel sheets
Limited reach of information dissemination
Time-consuming data analysis
Difficulty in explaining complex data insights
Slow system
Outdated and inconsistent data
Existing data warehouse approach was outdated
Aging workforce in the sector
Unified management of user and sales data from different departments
Difficulty in forming comprehensive judgments from disparate reports
Impossible to do any analysis on Excel beyond half a day
Limited number of rows of data support in Excel
Lack of systems that allowed employees to self-serve their own data needs
Need for consistent reporting across the company
Combining disparate data sources during the 2018 merger
Fragmented data and personnel
Lower alignment across departments
"Harder to make fast, accurate business decisions"
Underlying data infrastructure unable to keep up with expansion
longest delivery time was six weeks
heavy focus on traditional channels made it hard to understand the effect of digital channels
delivery of key findings in up to 10 separate reports made it difficult for clients to see the whole picture
Clients unable to delve into data with static PDFs
Technological limitations causing a three-week lag in report turnaround
Original data systems struggling to cope with extra demand
Important system syncs taking longer
Fragmentation of data sources
Lack of standardized analytics/reporting tools
tracking beer wastage at every stage of production
knowing what products need to be manufactured to meet market demands
Fragmented data across multiple local silos
No central data analytics and business intelligence function within the department
data had no single standardization and varied across different businesses
Confusion about performance of Cigna’s Collaborative Care participants
"Difficulty in knowing if engagement, satisfaction, and experience levels were maintained or exceeded"
Low engagement and missed opportunities from static legacy reports
Existing tools could not handle the data volume
Tools were too complex for non-technical users
Limited access to workforce data before Tableau
Delayed reporting which led to outdated data
"Unclean, scattered, and unreliable data"
Lack of standardized system and end-to-end system integration
Vast amounts of data
Limitations of SQL requests and spreadsheets
Manual processes leading to errors
lack of a central data repository
reliance on summary reports lacking detail
previously slow speed to insight
Existing network of disparate spreadsheets
Lack of a proper BI function
No real data architecture
Existing tool insufficient for advanced business questions
Managing over 400 reports regularly was time-consuming and resource-heavy
Manual tracking of claims prone to human error
Claims stalling or slipping through the cracks
Slow performance due to live custom SQL queries
Inconsistent ad hoc reports from different Excel sheets
Potential misreporting and compliance issues
Time-consuming month-end finance reports
Identifying roadblocks in insurance application processes
Lack of evidence or deeper insights to speed up processes
Data fragmentation
Quality and accuracy issues with Excel
Manual report building process
Limited insights from data
lack of immediate expertise to deliver dashboards and functionality
previous limitations of basic in-house dashboards
Existing data warehouse approach was complex and slow
Reports were often incorrect and delayed
Complete reliance on IT department
Time and resource intensive tabular reports
Lack of internal capabilities to analyze data
Long development cycles for custom reports
Business users without SQL knowledge found it hard to access and visualize their data
Difficulty in making sense of large quantities of data to identify patterns and anomalies
common IT problems faced by customers
Information not found
difficulty in engaging presentations with PowerPoint and Excel
manual spreadsheet analysis was time-consuming and outdated
restrictive licensing model of previous tools
disparate workforce coordination
Lack of a common view across 170 childcare centres
Business users unable to derive insights through self-service analytics
Reliance on a small team of commercial analysts working with huge Excel files
Creating hyper-local content for diverse audiences
Disjointed analytics practices before adopting Tableau
Managing large amounts of incoming data quickly
Reliance on Excel spreadsheets leading to multiple versions of data
Conflicting definitions among e-commerce teams leading to internal arguments
Need to integrate data from Google Analytics and ongoing marketing campaigns
Finding visualization tools that could present information in a captivating format and cater to specialist data users while remaining accessible
Data literacy
Initial skepticism about data analytics
Lack of dedicated analytics tools or data platforms
Data inconsistency
Slow analysis and difficult benchmarking
High demand on central analytics team
Risk is the biggest challenge for a financial lending institution
Disparate forms of data leading to no unified vision across the business
High number of data silos
Reliance on Excel spreadsheets
Disconnect between BI team and business teams
Manual tracking of business targets
Duplicated data entry into multiple platforms
Time-consuming report submission process
Creating a single source of truth from over 200 data sources
Customizing different levels of dashboard access
Slow and inefficient monthly reporting
Handling large and constantly changing data
Duplication of effort and inconsistency in Excel reporting
Lack of real-time reporting and publishing capabilities
Limited insights with Excel
Difficulty tracking service effectiveness with extensive data
Capacity constraints in infusion bays
Placing inventory before demand
Predicting seasonal styles and features
Handling overstock or mismatched product preferences
Difficulty in tracking a game launched on a global scale with basic queries and spreadsheets
Explosive growth in data and analytics requests
Information not found
Significant percentage of analysts’ time went on creating and maintaining business reports
cost and clunkiness of previous solution (Microsoft Power BI and spreadsheet analysis)
manual data processing and slow time to insight
Standard reports did not allow to slice and dice data
Creating reports on traditional platforms like Excel was difficult and time-consuming
Homemade tools were time consuming
Fractured data environment
Inefficient data extraction and analysis with Excel
Lack of transparency between the department and external stakeholders
Highly-regulated environment requiring data access and compliance balance
Legacy tools creating inefficiencies and data inaccuracies
Strained relationship between IT and Business due to increased controls
"Manual, laborious data extraction and reporting processes"
Time-consuming data analysis with spreadsheets
"Manual data consolidation, normalization, and formatting"
Differently structured data sources from various advertising publishers
Lack of data from print-based voucher books
Managing large influx of data post app-launch
"Rapidly changing business models, channels, and streaming services"
Handling data from multiple disparate sources
Siloed and disparate data
Manual data pulling from legacy systems
Lack of a single source of truth
Different access to data leading to confusion
Lack of uniform reporting standards
High individual effort for data preparation
Long development time for new analyses due to IT department backlog
Data inconsistencies and duplication
Streamlining IT management and improving decision-making process
Challenges in implementation of Tableau
Restricted access to company data before 2015
Ongoing information lag with monthly or quarterly updates
Inability to analyze large volumes
Reporting inconsistencies
Long lead times
Creeping cost of ownership and scalability
Stability issues
"Historically, analysts spent 80% of their time on data preparation"
Need for a 360-degree view of the customer
slow and cumbersome process without the right tools
time taken to compile and analyze data led to key insights going unnoticed
Reaching capacity in Tableau environment
Long wait times for Tableau licenses
Fragmented reporting across various BI platforms
time-consuming manual reporting
human error in data processing
lack of exploratory analysis time
"Before Tableau, relied on an external partner for report creation"
Time-consuming data analysis using Excel
efficiency
timeliness
product line appropriateness
Time-consuming analysis with conventional tools
Conventional tools not delivering immediate insights
"Cumbersome, manual process with Google Analytics and Excel"
Outdated data by the time reports were generated
Time-consuming and difficult to interpret operational reports
Data siloes
Keeping up with varied and numerous demands from employees
Traditional reporting couldn't keep up with the demands of employees and departments
Business Analytics & Artificial Intelligence unit working at full capacity
Inability to rely on static Excel spreadsheets
Limited IT capacity
Reliance on development efforts to display data
Time-consuming process to develop new views
Manual data analysis in Excel was time-consuming
Inability to map lifetime customer journey across different channels and platforms
Aggregating data from multiple sources
Quality of data from traditional channels
Blending different types of data
Repeated analysis works
Limited access to reports
Manual data extraction from Excel files and multiple systems
Manually aggregating vast amounts of data
Lack of awareness of data quality and consistency
Difficulty in identifying and diagnosing data-related issues
Adapting to daily change requirements for reporting
Requesting and loading data copies for troubleshooting
Time and staff resources required to build reports manually
Limited exploration leading to missed opportunities
difficulties in quickly utilizing diverse data sources
"long, complex, and repetitive processes for data analysis"
fragmented data sources with no consistency in data analysis
Previous analytics tools only yielded static reports
Siloed data across departments
Inability to connect data with outside data
Limited report types (only 20 types)
Long development times for new business reports
Lack of trust in static reporting data
Reliance on Excel and manual SQL queries
Long development cycles and poor data quality
Lack of responsibility for data among employees
Serviced daily sales requests from over 500 salespeople
Created a reporting queue of up to 6 months
Information not found
High degree of variation and inaccuracies with manual data entry
"Different ways of gathering, interpreting, and using data by different stakeholders"
Difficulty in making quick and informed decisions due to data inconsistencies
Important data stored at a local level
Hindering day-to-day activity of teams
Important decisions made without pertinent information
Siloed information leading to potential costly errors
Overwhelming queue of tasks for the analytics team
Long reporting cycle with Excel reports
Managing multiple static reports
trying to implement dashboards over four months
Multiple replications of data
Undocumented calculated fields
"Slow, confusing, and frustrating data access"
Departments had to wait for results from IT
Static reports prepared manually
Long turnaround times for data reports
Long reporting queues
existing web application could not meet the demand for complex analyses
professional services team inundated with requests consuming significant time and resources
Need for quick setup
Need for a seamless experience for ad-hoc analysis
Standard reporting took about a month before Tableau
Data stored in siloed Excel spreadsheets
Time and labor-intensive analysis
Big coding efforts for enhancements
"Previously text-based, 25-page report compiled manually"
Need to streamline and improve HR reporting post Gannett and TEGNA split
"Inefficient, non-scalable analytics platform"
Long reporting cycles
Inconsistent data leading to distrust
Balancing service coverage and profitability
Identifying areas for infrastructure investment
Attributing various costs proportionally to each customer
Handling data from diverse sources
Complexity of technical and cost investigations
Time-consuming data consolidation
Manual reporting process was time-consuming
Bringing together data from multiple systems
Absence of streamlined data access
Managing large volumes of data across various customer infrastructures
Interfacing with a variety of databases
Need to answer questions on the spot during meetings or calls
Record-keeping done on paper
Errors in Excel sheets
Clinicians' initial skepticism about the value of BI tools
Creating data in Excel and making it into tables
Inaccurate data reporting
Emerging transmission patterns
Consolidating numerous reports from different formats
Creating a holistic picture of relief efforts
Analyzing border status and monitoring closures
Disseminating data via giant spreadsheets
Customizing data for particular regions or segments
competitive industry with tight margins
Getting students up to speed with data analytics
required analytics solution interfacing with various databases
presenting data interactively
answering questions promptly in meetings or calls
Initial sluggish meetings without productive outcomes
Disparate data spread across the university
Difficulty in accessing and interpreting data
Standard reports took about two months to create
Legacy warehouse couldn't scale to the volume and velocity of data
handling influx of users
Need for real-time data visualization
Requirement for professional-looking dashboards
Slow business processes and reporting delays
Bottlenecks in acquiring data
"Reports from ERP system were unreliable, outdated, and incomplete"
Inflexible data extraction from ERP system
Inconsistent and outdated reporting
Long duration for data extraction and reporting with previous tools
High manual effort in data manipulation and transformation
Need to spot problems before they become problems
Manual processing of data leading to errors
Difficulty in finding current sales figures
Divergent definitions of KPIs among departments
Manual effort for ad-hoc report requests
Fragmented and inefficient data collection and sharing
Fragile networks and unreliable connectivity
Tracking participants' progress
lack of clear and clean data
need for actionable analytics
educators reluctant to analyze data
negative association with data due to NCLB
aversion to data from past negative evaluations
Data scramble to find relevant information
Polishing up visualizations
Making visualizations memorable
Community health workers in Zambia catch only half of the malaria cases
Managing growing data volumes
Segregated data across different offices
Reliance on Excel spreadsheets
Employee disengagement with data
Reaching families unaware of pre-k programs
MicroStrategy’s internal processes are cumbersome
Complexity of reports in Excel
Excel not keeping up with growing data complexity
Scalability due to manual work with Excel and PowerPoint
Creating customer-specific reports
Time-consuming process with old systems
Limited functionality and difficult to understand charts
Data not easily accessible to everyone
Long wait times for data responses
Manually extracting data from numerous Excel reports
Distinguishing which Excel report was correct
Convincing senior management about the value of Tableau.
"Presenting data and reports in a scalable, secure, and intuitive manner."
Reporting done manually in Excel and PowerPoint
Time-consuming creation of Excel metrics every week
"Manual, time-consuming reporting processes"
Difficulty in acquiring data
Ensuring data quality within reports
Restricted IT resources
Need for insight into data when starting new or distressed companies
data was obscure
short time to provide feedback
Convincing vendors to invest in reconditioning vehicles
Learning without formal training
Spent hours getting indicators each week
Information security issues with using emails and text files for data sharing
reducing the number of variables for presentations
information overload in PowerPoint slides
Monitoring goals and indicators on a monthly basis
Ensuring data accuracy across multiple business areas
Getting students past the initial excitement to deeper learning
Adjusting teaching methodology over time to better address student learning process
Switching between multiple systems for data access
Time-consuming Excel-based analysis
Limits of Excel in terms of speed and data volume
Overwhelming volumes of data in spreadsheets
Inaccurate reporting due to manual data handling
Inflexible nature of spreadsheets
Large data sets make the story not obvious
Manual report generation taking up to two weeks
Visualizing large and multi-dimensional data sets with existing tools
Old business intelligence system
Data processing being tedious and prone to error
Lengthy backward development
Inefficient custom map creation
Identifying accident and disease rates at regional levels
Finding internships in a tight market
Learning SQL queries and database structure
Transition from traditional BI solutions to cloud-based system
Difficulty displaying large amounts of data
Extracting relevant information
static and slow reporting pace
manual reporting process
combining data from multiple sources
Showing potential value of production data to non-production performers
Clarifying the trend towards the development of traffic by point of address
Disparate data standards among customers
Reports taking months to generate
Manual and time-consuming report creation process
Working in silos previously
Bottleneck in reporting processes
Issues connecting previous analytics tool to critical data sources
Time-consuming data processes
IT team handling all reporting requests
Lack of self-service reporting capabilities
Understanding service patterns
Seeing details of every transaction
Limited visualization capability of Oracle database
Time-consuming manual sample tracking
Manual reporting processes were not scalable
Data outputs were analog
Slow reporting reaching decision makers
Ensuring both data-savvy and non-data-savvy employees can use the tool
Validating and blending data from multiple sources
Time and resource allocation
Presenting findings effectively to clients
Addressing compliance issues
Redesigning and changing the data structure
Connecting multiple data systems
Reducing the time to generate reports
Lack of tools to triangulate the true value of data
Static reports and ad-hoc querying made it difficult to see and understand data
Inefficient access to large datasets
Constant need to re-run reports with different selections
Balancing family and client commitments
Volume of data processing (500 million address records)
Mapping data onto geographies accurately
Existing tools were very technical and hard to use
Updating reports took considerable time and effort
Lack of flexibility in existing tools
Disengagement among staff and stakeholders
Collaboration difficulties due to different tools being used
Information not found
Data management and data warehousing complexity
Handling various clinical terminologies and master data issues
Dependence on Excel spreadsheets
Need for more meaningful data presentation
Consuming data from different sources
"Big data challenge, homogenizing and harmonizing data"
Using information that everyone can access and share
"Different data in each area, particularly in inventory"
Bottleneck in report creation
Need for real-time data sharing
delays in receiving detailed sales results
slow process due to volume of data
Previously required entire office to create data book
Used to share data in PDF and Excel formats
Reducing inventory error
Tracking inventory efficiently
Excel's limitations with large data sets
Manual time-consuming data processing in Excel
Visually unappealing charts in Excel and Power Map
Inability to interact with data in Power Map
Scattered data across different systems
Excessive time and resources required for data extraction and integration
Multiple versions of the truth leading to data conflicts
Outdated data by the time it was used for decision making
Slow and labor-intensive reporting processes
Different reports being produced by different teams
Numbers not matching and tying up
Very large demand
Huge information database
Managing security in a multi-tenant environment
Dealing with large data sets
Timely reporting issues leading to frustration among administrators
Teachers not receiving timely testing data
Lack of context in testing data
Original single sign-on solution reporting functionality was insufficient
Static and inflexible At-Risk Reports
Food banks not understanding the data from spreadsheets
Quick turnaround projects
Collecting data quickly
Need to deploy dashboards across multiple regions easily
managing data from various formats like TXT
ensuring everyone had the same version of reports
Dependency on individual processing in Excel
Need for a future-proof solution
Information not found
Other systems lacked compelling visualizations and return on investment
Changing the mindset of end users to self-service BI
Need to govern and organize the BI process
Human error in old inventory management processes
Expensive and inefficient Omnicell supply cabinets
Manual and error-prone inventory checks
Difficulty in creating and updating reports
Time-consuming special item creation process
Prior reliance on Microsoft Excel leading to inconsistent information
Multiple systems and spreadsheets causing data overload
Data mostly served through Excel pivot tables
Inefficiency in meeting business requirements with Excel
Prior data was not easy to access or user-friendly
Bringing in data from ERP system and integrating it
Establishing IT as true partners with the business
Ensuring successful adoption within divisions
Information not found
Increasingly time-intensive analysis for IT and engineering teams
Creating new reports or dashboards requiring significant developer or engineering resources
Long time to create customized dashboards
Difficulty in interacting with data using PHP
Slow data conversion with web technologies
Inability to blend multiple data sets
Traditional visual analytics tools required scripting skills and took weeks to create dashboards
"Students had to spend more time learning the tools, leaving less time for data work"
Processing continuous streams of huge amounts of data in a short time
Manually analyzing the data and creating reports
extremely vendor-dependent BI platform
slower and costlier analytics and reporting process
vulnerable and unstable reports in Microsoft Excel
dependency on vendor for report generation
Time-consuming data analysis with spreadsheets
Error-prone manual processes
Limited data perspective
No data visualization in old system
Analyzing data in silos
Different databases and analysis methods across departments
Manual report preparation
Varied work environments across company structures
Time-consuming report development by IT department
Different analytics tools and data infrastructures for each product
Time-consuming SQL scripting for report generation
Long waiting times for reports and analyses
Inadequate BI tools from data warehouse
Complexity and inflexibility of other evaluated solutions
Need to pull together data from many different sources
Identifying gaps at particular grade levels
Managing relationships with fans and corporate partners
Ensuring security while maintaining interactivity
Global mining industry downturn
Coal and iron ore price drops
Disparate and siloed data
Limited reporting capacity with previous BI tool
Data in different silos
IT department required to handle report generation
Ad hoc reports took too long to create
Departments often resorted to using Microsoft Excel
Traditional BI tools produce reports with 95 percent noise
"Multiple data sources (SQL Server, Google Analytics)"
Managers reporting from individual viewpoints
Disordered data needing unification
Time-consuming data collection from different systems
Static nature of PDFs leading to outdated information
Empowering a sales force in a scalable way
Manual and time-consuming process using Access and Excel
Communication of data across departments needed improvement
Inability to utilize information gathered from all departments
Needing a tool to effectively leverage data storytelling
Files collecting the data were scattered around and difficult to pull together
Data stored across multiple systems including ERP and spreadsheets
Manual and labor-intensive reporting process
Decisions based on old and untrusted data
Lack of a process-based way of thinking
Difficulty understanding specific situations in geographic regions
Long time to produce quarterly reports from Hadoop
Inability of end users to extract information from external datasets in Excel spreadsheets
End users needing access to updated data insights while outside the office
"Time-consuming process of extracting, analyzing data, and generating reports"
Handling and analyzing large datasets quickly on Hadoop
Analyzing extremely large datasets
Setting up data analyses with traditional tools
Querying cubes was not user-friendly
End users were uncomfortable generating and analyzing information
Analyzing large quantities of data with Excel
Writing custom code taking up to 3 weeks
Difficulty in sharing figures with branch managers
Time-consuming data processes
Complexity of other tools like QlikView
Delayed time-sensitive decisions
Lack of data culture
Large volume of data in Excel and SQL tools
Processing information took days
Understanding customer behavior during renovations
Identifying issues with charity promotion data
"Managing a large, archaic billing system"
Delays and problems with vendor’s reporting processes
Costly and inefficient report production
Creating manual reports for each customer
Long wait times for executive engagements
Losing weekends developing and adapting spreadsheets
Information not found
Time-consuming manual reporting process
Potential for human error in data consolidation
Surfacing noncompliance issues too late
Reducing turnaround time for data requests
Time-consuming monthly web analytics reports
Building and publishing inflexible reports
Time-consuming process for data analysis with Excel
Data in print publication being outdated once printed
Limited by paper format
Unsatisfactory experience with Cognos due to lack of interactivity and technical skill requirement
Requirement for timely report generation
Intensive data formatting
Static reporting
Labor-intensive data integration
Connect to Hult’s Salesforce data and learning management system on Amazon Redshift
Avoid steep development effort in creating a reporting system
Manual process and time-consuming data extraction from CSV files
Third-party analytics tools not providing data visualization
Difficulty in getting full perspective from spreadsheets
Limited BI resources
High competition and limited shelf space in premium supermarkets
Old process of compiling data and building dashboards took months
Information not found
Excel-based process was not efficient
Clients struggled with multi-spreadsheet reports
Ensuring data security was crucial
Needed to move from Excel-based reporting to a more advanced system
Setting up the foundational data warehouse
Adopting new social security systems in a post-war context
Initial performance problems with on-premise IT
Complex and error-prone spreadsheets
Previous tool lacked flexibility and ease of use
Initial report processing was labor-intensive
"Old system was long, complicated, and tedious"
Reaching the upper limit of vertical scalability
Need for a more user-friendly data visualization tool
Communicating value to clients
Dealing with siloed data in healthcare
Need to blend and combine data sources
Transitioning from gut feel to scientific coaching
"Time-consuming process of compiling, analyzing, and visualizing data"
Data prep took up to a day
Results were hard to read and inflexible
Decision makers struggled to understand reports
Delayed decision-making
None explicitly mentioned
Initial adaptation to a new way of doing things
Balancing speed and data security
Manual and non-actionable reports in Excel
Ensuring clients find value in the reports
Long round-trip times for analysis questions
Users not asking many relevant questions due to long response times
Getting people to adopt new tools over existing ones like Excel
Initial mistakes in implementation
Needed better data governance
Understanding trends and gaps within course offerings
Previous time-consuming process with Hadoop and Excel
New users needing help with questions
Learning process never ends
Understanding new market dynamics
Planning for deregulated pricing
Timely analysis of student sentiment
Convincing users to switch from Excel to Tableau
Manual tasks involved in updating Excel and Google spreadsheets
Need for a tool to support easier communication between offices
lack of a functioning data warehouse before Tableau
limited use of Cognos due to long implementation cycles
lack of a unified view of data
confusion around true numbers
historical reliance on static reports in Excel and .CSV files
Dealing with several data sources and formats
Manual data handling in Excel spreadsheets
Disparate systems of data collection and analysis
Multiple types of data analytics software and BI systems
Existing BI methodology required coding and wasn't scalable
Insights could only be sought by a few individuals
Backlog of requests delayed data access
Low productivity levels with the current solution
Handling large spreadsheets with millions of formulae
Discovering unexpected costs
Data stored in Excel with thousands of formulas and filters
Long time to access and interpret data
Lack of timely management information
Difficulty in scaling marks when coding manually
Centralizing data was slow and ineffective
Delivering effective KPIs
Accumulating a vast amount of undisclosed data
Spending 80% of time on data collection and aggregation
"Helping management make fast, informed decisions"
Time-consuming data exports to Microsoft Excel
Need for seamless connection to data sources
Data stored in individual Excel documents
Dependency on analysts for data access
Little to no data access for users
Mapping changes in source data due to ERP upgrade
non-integrated databases
Relying on various different platforms to analyze and report data
Multitude of different dashboards complicated and delayed decision-making
Handling billions of data points
Lack of timely insights
Difficulty presenting data to non-SQL users
Numerous huge spreadsheets on seemingly endless sheets of paper in tiny font.
Exporting and analysing spreadsheets was slow and tedious with little flexibility.
Data was often siloed and didn't 'speak' to other sets.
Double-handling specific sets of data hindered business efficiency.
Integrating multiple systems
Handling large volumes of transactional data
Advance work required in data analysis before Tableau
Time-consuming data processing and diagram creation
ERP back-end database is a relatively old technology
Different data structures at different mine sites
Avoiding stale data
Lengthy traditional BI processes
Need for a simple and easy to deploy solution
Handling regulatory side of home care and hospice industry
Handling large amounts of data and making sense of it
Adapting reports to specific needs of different departments
creating new reports took 2-3 weeks
Writing SQLs took a lot of time
Limited ability to monitor data frequently
Excel couldn't handle large amounts of data
Time-consuming data preparation for analysis
Manage enormous data with previous system
Limits in data visualization with previous system
Adverse culture towards data in education
Identifying key standards impacting student performance
Initial lack of awareness of Tableau on the Polish market
Finding answers from dispersed information sources
Excessive data to consume
Users relying too much on IT for reports
Difficulties in noticing changes over time with older tools
Manually updating and moving data files
disparate data systems
No visibility of data quality issues
Struggling with huge spreadsheets
Users finding existing tools too technical
initial skepticism
time spent stitching together data sets
Working on antiquated systems
Decision-makers not seeing the end result of policy changes
Slow government processes
working on limited funds
disparate data systems
data quality issues
Limitations of existing BI tools in providing a dashboard facility
Obtaining buy-in from top management
Difficulty in handling data with Excel
Lack of in-house knowledge for price realization report
Dependence on Microsoft Excel which was time-consuming
Limited development resources for building own analytics
Risk of burning development hours on experiments
Disparate data making real-time decisions difficult
Difficulty selling tickets for specific games like Father's Day games
Information not found
Storing student information in folders and notebooks
Lack of real-time data access in classrooms
Previously needing 10 to 12 hours to collect data sets
Using isolated spreadsheets before Tableau
Identifying causes of delays
Waiting for information before Tableau
High development time with Microsoft Excel
Lack of interactivity in previous tools
Retailers using old methods of fixed reports are poorly armed with information
Information not found
"social data being big, unwieldy, and unstructured"
laborious data analysis process before Tableau
Not using data effectively
Access to databases for engineers
Customers not understanding raw data
Difficulty explaining value proposition without visualization
Need for a fast and easy-to-use analytics tool
Requirement for a common point of reference for all employees
Data in many different formats from different sources
Managing large quantities of data
Time-consuming manual data retrieval and report creation
IT bottlenecks in creating reports
Synthesis of multiple data streams
Choosing the right analytical tool that harnesses a holistic view of data
Identifying root causes of bottlenecks
Transition to Tableau Server
Need to highlight and solve specific issues
Integration with various systems including Excel and SQL data warehouses
Handling over 50 databases collecting different types of data
"Making sense of over 6,000 XML files each season"
Need for price transparency
Quicker access to new levels and dimensions of data
Waiting for analytics team to fulfill data requests
Difficulty accessing and disseminating data prior to Tableau
Data was harder to get to and share
Reliance on consultants and outdated reporting systems
Difficulty in analyzing data using Crystal Reports
Time-consuming data extraction and report generation
Inconsistent report results depending on who generated them
Testing new insights quickly
Need for data visualization tools
Explosion in data and limitations of static reporting
Previously spent considerable time and effort on data analysis and report generation
Difficulty in showing added value to clients without Tableau
Long waiting times for report generation before using Tableau
Handling large amounts of data
Data in various formats
Size limitations of previous analytic systems
Tight deadlines due to late data arrival
Cumbersome and inefficient data analysis process
Time-consuming data extraction and graph generation
Long implementation processes with previous BI solutions
Labour-intensive processing
Long start times and non-real-time data refresh with previous solutions
Indirect and time-consuming approach of working through IT for data access
Lack of programming skills among most users
Manual work before implementation
Need to visualize geographical data
Finding a tool that provided a 360-degree solution
Handling massive data volumes
Aligning teams across the company
Optimizing content for mobile
Determining what types of video content to focus on
Managing increasing number of questions and datasets.
Unstructured data differing from province to province.
Complexity of geospatial analyses.
Information not found
Manual updates were very hard to update before
Time-consuming queries and dataset building with traditional tools
Need for multiple engineers to create dashboards
Data existing in silos
Time constraints for field teams to analyze data using traditional tools like Excel
Initial unavailability of technology in Chile
Dependency on highly IT-dependent tools
"Previously relied on a mixed bag of spreadsheets, data aggregation, and visualization tools"
Time-consuming data crunching and report creation
Difficulty in creating reports for those with limited IT experience
Need for clear visualization of data for brand partners
Existing old tools that can't provide quick reporting
Diverse financial and logistics software across countries
Inability to quickly use data silos within organizations
Difficulty in building applications with previous technologies
Initial complexity with large data sets
Need for quick and efficient data visualization
Initial difficulty in effectively communicating data insights
Manual analysis using Excel lists
Considerable effort due to high visitor and transaction volumes
Information not found
High volume of data
Systematizing information manually
delay in accessing sales database data
dependency on IT for generating reports and dashboards
Multiple formats of data requiring different programs
Time-consuming data analysis process
Similar products in the market
Need to turn data into insights quickly
Handling a large volume of data
Tight deadlines
Delays in receiving reports from IT
Consolidating data for government queries
Manual and error-prone Excel manipulations
tools not fit for purpose
need to blend unstructured information
Limited visualization capabilities before Tableau (mostly Excel)
Merging information from several systems
Unscheduled student arrivals causing unpreparedness
Difficulty in managing independent professional groups within universities
Lack of detailed student records and performance tracking
Time-consuming data collection and outdated information
Data-rich but analysis light field
Lengthy standard reporting time
Data-averse educators
Complexities of using existing industry-specific tools
Time spent on running reports
Mechanics of gathering data
Delays in getting information to decision-makers
Inefficiencies in physician schedules
Dependency on third-party consulting services
Adoption by rest of the organization
Behavior change in understanding and utilizing data
Showing HR's impact on the bottom line
Ensuring best candidates in recruiting process
Lack of programming skill sets within the team
Difficulty exploring data with other tools
Tracking millions of dollars in reimbursable projects using traditional means
Inability to consolidate and visualize data effectively
Maxed out capacity with Excel
Static data presentation
social data is dirty
need to identify relevant behavior in tweets
Lack of flexibility in manual sales reports
Inaccuracies and lack of standardization
Difficulty in having readily available sales performance information
lack of awareness among veterans
no definitive metrics for advertising recommendations
reports lacked interactivity
Identifying potential new suppliers
Gap in the Microsoft Business Intelligence stack around data representation
Lower than expected physician compliance with Electronic Medical Records
Existing processes negatively impacting compliance
Reliance on manual reporting systems
Time-consuming report preparation
Outdated information
Lack of reporting consistency between departments
Handling large quantities of data
Connecting varied data sources
Information not found
sales reps are not 'data' people
traditional methods of handling data were antiquated and cumbersome
Handling large amounts of data
Integrating multiple data sources
Managing directors couldn't see the data solutions
Data buried under a pile of rubble
Technology limitations in analyzing unstructured data
Information not found
Two-week queue of large data and ad-hoc analytics requests
Need for advanced features like error bars and color changes at thresholds
Extremely time-consuming data gathering
Manual process prone to errors
Report writer crashing during data extraction
Difficulty in managing promotions with real-time data
Handling huge amounts of data
Enabling non-experienced users to make data-driven decisions
Understanding individual student needs
Massive amounts of data to digest
Time-consuming data analysis process
Manual effort and potential for mistakes
Stale data for decision making
Difficulty in finding reliable international alumni interaction data
No insight into which recruiting fairs delivered best results
Difficult to answer questions and identify opportunities in complex environment
Data manipulation difficulties slowed access to updated information
No insight into which recruiting fairs delivered the best results
Complex environment with tens of thousands of students and thousands of employees
Data presentation in spreadsheet format was overwhelming
Slow data manipulation and access to updated information
State of turnaround
Culture transition to data-driven decision-making
Lack of BI-specific tools initially
Initial tools could not scale to meet demand
Monthly report updates were not timely enough for decision-making
Quality-checking and converting student data
Recreating reporting application interfaces
Ensuring access to current student data during conversion
Identifying and correcting data errors
Training a large number of new recruits and existing soldiers
Managing data from more than 50 interfaces and different formats
Ensuring security and compliance with DOD requirements
Analysts spent 80% of their time pushing data around in Excel
Static PowerPoints limiting insights
Handling a large volume of data (up to 4.3 petabytes)
Integrating diverse data formats from different operating facilities
Information not found
Previous BI solution delivered very little
Merging multiple information sources into one analysis
Lack of visibility into operations
Time-consuming manual report creation
Inconsistent contact center technology
Long report run times on SAP
Initial difficulty for new users to fully gain control and get the most out of Tableau
Root-causing nagging issues
Connecting different databases together
Initial reliance on database dumps and Excel spreadsheets
Difficulty in consuming and analyzing raw data
Initial demand exceeded capacity
Static and outdated reports
Costly data aggregation and report preparation
Different report formats required by each bank
Handling large amounts of data (half a billion hits a month)
Manual extraction and processing of data into CSV and Excel
Accessing and sharing data stored in disparate sources
Duplicated or split systems between offices
Lack of centralized data warehouse
Static Excel reports
Long decision-making process due to time differences
High standards for data accessibility
Consistently reproducing answers
Data stored across multiple sources
Tedious process of making graphs in Excel
Breaking away from industry-standard solutions
Convincing people of the usefulness of the data
Finding a solution that presents data clearly and impressively
Analyzing large datasets
Communicating complex data insights to directors
Limited budgets and resources
Inability to afford high-cost content
Manual process of data extraction and report creation
Dependence on the analytics team for data
Complexity of data sources due to acquisitions
Collecting and cleansing data
Difficulty in making sense of large data sets before Tableau
Spending excessive time cleaning data pre-Tableau
Different and fragmented silos of research data
Time-consuming data gathering and preparation
Creating visually appealing graphs and bar charts manually
Combining data from multiple sources
Transforming and compiling data into a single system
Introducing something new to clients who haven't seen such tools before
fragmented assortment of BI tools
silo-based tools gave a narrow glimpse of intelligence
reporting was principally an IT department affair
Needed to consolidate numerous pre-existing Excel reports
Initial skepticism about platform capabilities
no means of easily analyzing or making sense of all the data arriving in the organization
focused on transactions and support rather than analysis
Organizing and structuring data
Translating analytics into meaningful stories for users
Difficulty explaining prediction modeling to non-data people
Traditional reporting solutions were cumbersome and expensive
Time-consuming and resource-intensive data collection
Inconsistent sharing of patient survey results
Fractured approach with multiple technology platforms
Expensive and uncoordinated custom reports
Closed nature of previous QlikView tool requiring data analyst support
Delay in report preparation with QlikView
Manual data processing was time-intensive
Delays in data availability due to manual processes
Integration of dashboards for internal and external use
Managing security and authentication between Salesforce and Tableau
Long queue for IT to generate reports
Reports answering wrong questions requiring rework
Ad hoc and inconsistent homegrown solutions
Difficulty in blending data from disparate sources
Manual and time-consuming report creation processes
Initial investment and cost with traditional BI
Time to set up infrastructure
Lack of resources to maintain in-house server
Previously complex process of sharing data visualizations
Inaccuracy and inconsistency in Excel reporting
Accessing and presenting data from multiple systems
Time-intensive manual process with Excel
Clients' high expectations for visually appealing output
Need for faster project completion
competitive market
I
n
f
o
r
m
a
t
i
o
n
 
n
o
t
 
f
o
u
n
d
Requirement to get data from multiple sources
High effort and cost associated with traditional BI technologies
Need to set a budget a year in advance in a rapidly changing environment
Data being sent in from 14 or 15 different systems
Different electronic health record systems
No master dataset
Static and localized data visualization process
Cumbersome and slow separate Excel workbooks
Initial top-down analysis model limited to a few experts
Time-consuming querying and report generation processes
Laborious three-month process
Custom analysis and printing results
Complexity of creating multilingual scorecards
"Existing alternatives were too expensive, complex, or unmanageable"
Slow and difficult report generation process
Traditional BI projects did not meet customer needs
Existing tools did not make customers self-reliant
Information not found
Creating a lot of analytical content and adapting customer's business to this content
Convincing departments to adopt Tableau
Introducing sudden challenges during competition
Drowning in massive amounts of data
Managing data from multiple sources
Sales and marketing teams not used to working with data
Digging through raw data
Creating ad-hoc reports
Initial reliance on SQL programming and Excel for data analysis
Lengthy iterative process for creating charts and insights
Making stored data meaningful
Identifying waste
complexity of business and data architecture
integration of local technology into a global framework
Struggled with getting insight from existing data architectures
Time-consuming traditional reporting processes
Managing disparate and siloed data sources
Lack of cross-channel communication
Disorganized spreadsheets saved all over the network drive
Inconsistent report formats
Need to use Google Analytics API and extract the data manually
Pointing Tableau to a new data source
Information not found
Information not found
Need for near real-time data
Limited IT bandwidth
Convincing users to adopt self-service BI
Integration with existing datasets
disparate data sources and Excel spreadsheets
unmanageable spreadsheets for sales team
Lack of a data warehouse causing consistency issues
Difficulty in accessing and reporting data with previous SQL and Access system
developing visual systems in a timely manner
Increasing the use of Tableau within the company
Having different data warehouses and BI platforms in each geography
Getting people to think about new ways of working
Convincing users to adopt the new tool
Running live queries against their database not providing response times needed
Complex and large data sets inaccessible before
Traditional reporting tools proving inadequate
Oracle Reporting being clunky and unfriendly
Users not noticing the important data
Slow visualization with previous tools
Aggregated data losing important details
Repetitive work with Excel
Slow implementation due to committee decisions
Extracting data from outdated formats like nine-track tapes
Making data useful on the web was almost nonexistent
Limited time and resources for building visualizations
Inconsistent reporting methods
Tedious report preparation
High cost of ERP system and BI platforms
initial platform (Microsoft) did not meet analytics and reporting needs
None mentioned
Lost physicians after Hurricane Rita
Timeliness issues with contracted radiology group
reports were extremely time-consuming to create
reports were difficult to understand
Making data accessible to people who don’t work with it all the time
Producing a large strategic indicator report
Time-consuming traditional processes
Hours spent processing data and converting it into visual form
complexity and cost of available BI solutions
substantial consulting fees for custom solutions
Time-consuming survey analysis and reporting
Static and lengthy reports causing clients to lose interest
Massive database access without impacting other systems
need for sophisticated data visualization capabilities
"Developers spent too much time on creating filters, drill-downs, graphs"
Lengthy process for building new reporting capabilities
Analysts required technical knowledge for current capabilities
Need to integrate structured data analysis tool with preexisting unstructured data analysis technology
clients exporting data to a spreadsheet
struggling to display reformatted information graphically
Difficulty making credit hour productivity visible
Complexity of creating optimal course calendars
Time-consuming spreadsheet processes
None explicitly mentioned
Clients not comfortable with raw pivot table output
"Projects requiring weeks or months of data collection, cleaning, and analysis"
"Excel pivot tables were slow, inflexible, and difficult to learn"
Graphics generated from Excel didn’t effectively communicate insights
Transitioning from conventional planning to driver-based planning
Complexities in financial and operational data integration
Need for real-time data transfer and monitoring
High procedural costs
Lengthy approval processes
Need for flexibility in planning
Obsolete technological islands
Individualized calculations in different departments
Too many exchanges of Excel worksheets
Time-consuming manual data consolidation
High risk of error
Insufficient transparency
Consolidation of data being time-consuming and labor-intensive
Not all employees sending back Excel files in specified formats and templates
Linking and processing data from different feeder systems
High level of detail in the planning process and large number of departments involved
"Inflexible, disconnected, and unsecured financial processes"
Time-consuming and error-prone manual processing and integration
"Complex, intertwined plethora of Excel sheets"
Manual preparation of data and error-prone processes
Cumbersome and labor-intensive process of making changes in Excel
Spreadsheet-based budgeting and planning was too complex
Tedious process of comparing figures and producing reports
Costly and time-consuming data management and reconciliation
Difficulty in tracking staff activities and reconciling payroll
Manual processes for managing growing staff and expanded business hours
Limited visibility of the company’s needs
Peak demand periods causing processing delays
Documentation required modifications leading to longer processing times
Diversified operations spanning 30 countries with different approaches
Previous attempt to standardize failed due to lack of hybrid approach
Legacy system limitations and unstable internet connections
Raw material price volatility
Market and distribution consolidation
Emergence of disruptive business models
Acceleration of industry changes
Manually intensive and non-strategically aligned processes
Complex and time-consuming legacy reporting system
Time-consuming and costly old systems
Merger activities introducing different business requirements
Transitional period requiring transformation in business steering
Manual consolidation and forecasting in Excel were no longer working
Need for data homogeneity and management
Requirement for a flexible reporting system
Complex distribution decisions due to long and fixed technical times
Separate and non-consolidated planning using spreadsheets
Time-consuming and complicated entry and reporting options in SAP ERP
Need for a corroborative and plausible Sales and Demand Planning process
Siloed data
Hidden insights
Multiple versions of the truth
Labor-intensive processes
Inability to make the most of its data
Inadequate Excel budgeting model
High operational risks and potential misalignments
Simplistic reporting structure
Legacy software was too cumbersome and rigid
"Lack of ERP system, reliance on CSV-based solutions"
Lack of standardization in existing ERP system
Complexity of Excel-based budgets and forecasts
Inflexible existing system requiring manual data consolidation
High costs and time consumption for report development
Manual reconciliation and complex budgeting processes
Cumbersome and labor-intensive financial reporting process
Data stored in large Excel files
Time-consuming month-end report preparation
Time-consuming to create additional versions of financial plans
Manually administered spreadsheets were hard to manage
Needed a tool to meet the fluidity and high demands of modern markets
Overloaded legacy planning system
Long data loading times from SAP
Limitations of previous Excel solution
Digital evolution of planning and control processes
Need to evaluate data based on a reliable yet quick planning and analysis system
Uncertain market conditions caused by global disruption
"Rigid, inconsistent, manual processes before implementation"
"Manually intensive, spreadsheet-based forecasting"
Time-consuming forecasting processes
Limited view of planning processes
Time spent working and re-working information
Unstructured data collection through Excel spreadsheets
Focus on short-term forecasting (current or next month)
Complex organization chart in sales force
Managing new items and customers in the budgeting phase
Ensuring data quality and minimizing potential errors
Technological limitations of legacy solutions
Difficulty in forecasting phase with legacy platform
Inconsistencies and quality issues in data due to multiple manual data entry points
Implementing a full retail planning process
Disparate reporting processes
Lack of autonomy and flexibility
Fragmented data from various sources
Supporting a complex supply chain
Few software vendors providing advanced services
Lack of collaboration due to data-silos approach
Information redundancy
"Difficulty in driving planning, forecasting, and business modeling processes"
Manual spreadsheets causing planning errors and delays
Financial forecasting spreadsheets fraught with errors
Complexity in analyzing SKU traffic and delivery differences
Difficulty in maintaining large data sets in SAP ERP
inefficient Excel spreadsheets
time-consuming manual budgeting
lack of effective planning and impact assessment
Lengthy and inefficient consolidation process
Disparate ERP systems across business units
Limitations of Excel for financial analysis and reporting
High implementation costs and rigidity of alternative solutions like SAP BW
Over-reliance on time-consuming spreadsheet-based finance processes
New general ledger system lacked required reporting capabilities
Disparate financial processes across a complex organizational structure
Complex manual process in Excel for financial reporting
Confusion due to lack of standardized layouts and methods in Excel
Difficulty in producing forecasts and reports efficiently
Long production times
Fluctuating demand levels
Variations in crop yields
Balancing high availability with avoiding waste
Complex structure of the Delos Group
Need to manage intercompany transactions
Time-consuming preliminary data preparation
Disparate reporting and BI systems forming standalone insular solutions
SAP BW only usable by trained users
Manual interfaces leading to errors
Achieving consistent data storage
Heterogeneous and manual report generation
Lack of sufficient degree of automation
Non-standardized reporting system
need for a tool-kit approach
integration with existing BI deployment
High complexity from acquisitions in the retail sector
High manual effort required for maintenance and analysis
Reducing information silos
Moving beyond its comfort zone
Developing planning capabilities to keep pace with global growth
Rethinking financial information management across departments and jurisdictions
changes in the business environment in recent years
"Integrating planning, management control, and budget process into a single platform"
Speeding up the flow of reports
Building an efficient risk management framework
Managing many projects with individual Excel sheets
Excel being too manual and time-consuming
No single platform to generate different accounting segments
Existing planning solution approaching end-of-maintenance
Performance and usability issues
Need for sophisticated access control system for ESG reporting
Limited reporting per target group
Lack of automatic data imports
Manual data updates and combining multiple data sources
Difficulty in slicing and dicing information from multiple departments
Challenging and time-consuming ERP system data download
Information not found
Efficient evaluation of combined data from different systems
Need to reduce manual input for reporting
Lack of transparency and performance in planning processes
Need for higher level of detail and accuracy in order proposals
Entwicklungszyklen der Kollektionen verkürzen
Abbildung der Großhandelsverkaufs- und Sachkontenplanung
Abstimmung zwischen internationalen Niederlassungen
"Manual, slow, error-prone, and Excel-heavy supply chain planning"
Inefficient scenario planning with limited simulation capabilities
Disruptions in available quantities and changes in material quality affecting production
Monitoring key KPIs with data from multiple sources managed in spreadsheets
Time-consuming manual input for information requests
"Need for improvements in budgeting, planning, and forecasting processes"
Efficient monitoring and planning of product distribution
Unstructured data approach across multiple departments
Time-consuming manual input for information requests
"Inefficient budgeting, planning, and forecasting processes"
Managing complex consolidation with basic Excel spreadsheets
Ensuring data validity and avoiding mistakes
Existing ERP system's limitations
manual and cumbersome financial reporting and consolidation process
static and user-unfriendly POS information in PDF format
lack of direct link between accounting and POS systems
time-consuming data entry into spreadsheets
Translating strategic choices into operational terms
Assessing target achievements quantitatively and qualitatively
Organizational impact and coordination of people and activities
High complexity and asynchronous component activities of the business process
Excel formats varied across business companies making comparison difficult
Cumbersome manual work
Diverse requirements from six business companies
Collating data from multiple networks of spreadsheets
Costly and prone to manual error
Manual production of KPI report packs
translating strategic choices into operational terms
organizational impact and effective coordination
managing complex processes across many company areas
integration with existing IT systems
Existing reporting was not cohesive or reliable
Data consistency was poor
High pressure on IT department for ad-hoc reports
Adapting to rapidly changing market conditions
Reducing manual tasks in forecasting and reporting
High level of data collation and pre-processing required
"Need for flexible, multidimensional OLAP analyses"
Requirement for standard reporting facilities
Existing financial management processes were not keeping up with business demands
Time-consuming budget and forecast process
Reporting and analysis were not agile enough
Handling multiple separate and disparate reporting systems
Moving from a time-consuming budgeting cycle to flexible forecasting
Time-consuming data preparation
Performance issues in evaluations
Inflexible and outdated planning tools
Complex data entry and consolidation
Joint evaluations of technical and commercial data not possible
merging various source systems
continuing to develop the planning process
"Need for affordable, yet robust solution"
Ease-of-use for all users
Increased manual workload
System limits due to increasing data volume
Inconsistent planning landscape
Increasingly difficult Excel-based budget planning and forecasting
Growing file sizes and broken links
Manual updates for actual values from Accounting
Lack of approval system
Unsustainable Excel and Qlikview budgeting process
Non-standardized and laborious data pulling procedures
Different sources leading to inconsistent figures
Cleaning up master data during the migration process
Manual updating of Excel reports
Inefficiency due to lack of real-time data access
Complex PDF report distribution process
Controllo delle marginalità senza compromettere la competitività
Variabilità del costo delle materie prime
Adattamento della capacità produttiva al mutare delle condizioni di mercato
Existing ERP system limitations
limited data-entry capabilities in existing BI tools
lack of workflow customization in existing BI tools
Gaining a complete overview of personnel development
Undertaking accurate staff forecasting with existing tools
Receiving information about staff changes late
Monthly staffing forecast taking up to 15 working days
"Issues with MS Excel: long data collection times, lack of data transparency and quality, risk of inconsistent data in meetings"
"herramienta existente (Qlikview) no soportaba los procesos de Planificación, Monitorización y Análisis de forma integrada"
need for a high level of data collation and pre-processing
clear definition of the information required
procurement of a flexible technology platform
Manual and error-prone Excel planning
Time-consuming planning process (eight-week preparation)
Existing process was costly and prone to manual error
Manual production of reports was time-consuming
Needed to replace reliance on hundreds of spreadsheets
Required quick wins in the early adoption stages
very early stage of implementation
Existing IT systems were inadequate for fast growth
Heavy reliance on spreadsheets caused delays
"Complexities in fully allocated, weekly product profitability data"
Rethink budget planning amidst rapid growth with the construction of three gigafactories
Information not found
Over-reliance on Excel
Breaks in the company’s audit trails
Complexity in communication of management rules
Dependency on file authors
Lengthy delays in running standard reports or data queries
Overreliance on intuition and experience
Complexities in accounting methods
Maintaining a lean workforce
Avoiding technical solutions requiring heavy consulting
Disconnected sales and procurement planning
Geographically dispersed staff struggling to share information
Reports had limitations and could only be interrogated by finance
Static PDF reports hard to read on iPads
Finance was spending much time creating reports rather than analyzing them
Reports were not visually appealing
Information not found
"Predominantly manual, time-consuming procedures"
manual and archaic planning processes mainly conducted in Excel
disjointed approaches to planning
overcoming disparate spreadsheets
Manual and archaic planning processes conducted in Excel
Transforming budgeting and management control processes
More effectively managing Profit and Loss (P&L)
Improving technical performance
Disconnected and unintuitive legacy tools
Information reaching responsible persons too late
Inaccuracies in dealing with ultra-fresh produce
Managing store inventory restocking from warehouses
Transferring goods between stores
"time-consuming, manual tasks associated with financial consolidation process"
Understanding costs and profitability
Analyzing enrolment patterns and demographic shifts
Assembling and interpreting vast amounts of enrolment data
Time-consuming financial reporting with manual interventions
Delayed processes due to late postings and manual adjustments
"Need for a user-friendly, flexible solution to accommodate multi-currency, multi-country, and multi-level planning and reporting"
Outgrown budgeting and forecasting platform
Lack of transparency and trust in numbers
Platform lacking capabilities for financial visibility
Enterprise growth and reporting complexity
Currently relying on spreadsheet processes
Requirement for accurate management reporting across multiple financial and disparate data sources
excessive reliance on spreadsheet-based models
"link multiple databases for reporting, forecasting, and budgeting"
Information not found
Analyzing volumes of data from vessels daily
"Managing changing commodity prices, foreign exchange, and interest rate movements"
Creating tailored reports and ‘what-if’ scenarios at short notice
Transferring data between different databases efficiently
Working in siloes
Inability to provide consistent and accurate reports
Costly and inaccurate consolidation of spreadsheets
Time-intensive manual data gathering and processing
Information not found
Nutzung von zwei unterschiedlichen Software-Lösungen für Reporting und Planung
Fehleranfällige Medienbrüche
manual workload
maintenance of interfaces between different systems
currency discrepancies
evolving customer needs
massive upheaval in fashion and retail industry
Starting the project during the Corona crisis
I
n
f
o
r
m
a
t
i
o
n
 
n
o
t
 
f
o
u
n
d
Media disruptions
Unnecessary process and approval cycles
Managing immense volumes of data from multiple sources
High error rate with manual Excel processes
Flexibility to cope with specific company requirements
Outstanding integration into existing IT landscape including an integrated SAP interface
Deploying a reliable infrastructure without knowing the exact scale and demand
Building an IT infrastructure for a new service
Complexity of scaling infrastructure
Managing sophisticated models
Weekly crashes of ERP system
Slow services for clients in distant locations
Maintaining physical servers
Scalability issues
Complex migration process
Managing global ad operations with a small team
Scaling infrastructure to handle increasing demand
Negative perception of banks
Complex purchasing process
Need for clear performance metrics
Handling large video files quickly and reliably
Combining multiple camera feeds into a single video
Analyzing player movements and game statistics in real time
Performance requirements by Brazil’s Central Bank
Scaling with a small team
Experimentation to find the best suite of tools
Handling increased data volume
Hardware limitations on-premises
Scaling on-demand
Limited scalability for resources and processes
Physical servers preventing speed and innovation
Resource-sizing and governance constraints
Multiple versions of databases with different languages and technologies
High storage costs for duplicated data
Operational workloads in on-premise environment
Lack of automated backups in on-premise database
Storing and processing data volumes beyond on-premises capacity
Doubling rate of new data generation every year
Managing processing capacity manually
"Ensuring data ingestion, structuring, and analysis"
Manual security processes
Risk of human errors
Cloud Confusion with numerous suppliers and services on different clouds
Need for a single data lake system
Data silos due to each web platform handling its own analytics
Manual consolidation of data from numerous sources
Higher expenditure due to multicloud environment
Technical errors risking connection interruptions
Troubleshooting remotely without complete device status information
Rapid growth of online forum requiring high scalability
Need for lower latency compared to other offerings
Technical challenges in cloud technology setup
"Previous BI tool created untrustworthy, tangled data"
"Business users struggled, leading to inefficiency"
Need for a tool that matched the power of the entire data stack
Dealing with multiple data silos
Achieving unified client-first mindset with previous tech stack
Ensuring security and privacy in a highly regulated industry
Manual creation of reports and dashboards
Internal multistep workflows
Difficulty in delivering actionable information quickly
lacked technology and know-how for advanced analytics
time-consuming data analysis on on-premises infrastructure
difficulty coordinating between different businesses
need for scalable infrastructure
Siloed data
Multiple sources of truth
Manual data consolidation
Difficulty in understanding user interactions
Traffic congestion and infrastructure strain
High demand for service
Managing large volumes of data
Previous BI tool had slow ETL process (6-7 hours)
Inability to access real-time insights with previous tool
Previous tool couldn't incorporate custom metrics
Need for more scalability and flexibility
Improving e-commerce experience
Upgrading e-commerce architecture modeling
Handling large volumes of data
Optimizing integration and execution of AI algorithms
Ensuring data security and access permissions
Handling high call volumes efficiently and effectively
Quality assurance for mixed language (Cantonese-English) conversations
Lack of existing open source data or machine learning models for the problem
Data in multiple silos across the organization
Conflicting key metrics definitions
Manual compilation of data from multiple systems
Manual approach to analyzing images was costly and time-consuming
"Reviewing brain scans traditionally took 2,500 hours"
"Need to search, analyze, and sort real-estate documentation"
Managing infrastructure without needing more developers
Maintaining data integrity and reliability
Defining metrics consistently across different teams
Misunderstanding and dangerous interpretations of data
Data siloed in different systems across a number of teams
Getting the right items in front of the right customers at the right time
Limitations of on-premises data center
Traditional fixed ways of working in customer industries
Processing backlogs up to 50% of daily data
Bottlenecks in the network stack
Legacy data stack required IT and data team support for almost every internal data request.
Data silos and conflicting metrics.
Difficulty in scaling data pipelines and infrastructure.
Limited options for transmitting funds within the same bank
Banks' lack of resources to develop sophisticated technology infrastructure
Inconsistent data and conflicting results from queries
Managing a massive amount of queries and spreadsheets
Limited visibility into customer dissatisfaction before Looker
Initial deployment on a public cloud platform without a data center in Taiwan
Need to boost development efficiency
Managing spiky demand for partner email campaigns
Providing high-level security for client data
BI team overloaded with work
No semantic modeling layer
Data discrepancies
Latency in data extractions
High maintenance time with previous infrastructure
Manual deployment and maintenance of Hadoop clusters
Handling large amounts of data
Making complex calculations quickly
Integrating data from various client platforms
Scalability and cost issues in content delivery and storage
Complexity of video production sector
High loading times due to non-local server hosting
Manual categorization and labeling of photos
Build pipeline delays during peak hours
Unpredictable infrastructure requirements due to increasing customer numbers and new technology
Previous data warehouse refresh rate of 24 hours was insufficient for real-time customer service needs
Optimizing the K Filter at the end of beer brewing
Handling unpredictable variables in the filtration process
Previous system unable to handle two billion events daily
Need to reduce event response time to 50 milliseconds
Old VM-based infrastructure was complex to manage with a small DevOps team
Managing increased traffic with bigger clients
Lack of a structured data environment
Delays in obtaining data
Needed dashboards with automatic updates
Delivering a 24/7 service
Scalable infrastructure needs
Setting up and maintaining various pipelines manually
Bottlenecks from running reporting directly from production servers
Infrastructure challenges with the previous cloud provider
Managing large datasets
Real-time ground-level measurement
Performance issues with previous database
Cost and headaches with old solution
Competition from online retailers
Long provisioning times with on-premises data center
Migrating 15-year-old on-premises system
Absorbing all data from various sources
Need for real-time compliance monitoring
"Handling 40,000 events per second"
Custom solutions for different regions
need for real-time log ingestion and analysis
difficulty in accessing logs from different systems
Provisioning and deprovisioning additional machines in advance
Over-provisioning and monitoring infrastructure constantly
Maintaining up-to-date security systems
Processing massive client data files efficiently
Storing large data files securely
Need to quickly adapt to new social media services
Avoiding influencers with large numbers of fake followers
Need to maintain a global presence while ensuring high-quality service
Managing high volumes
Ineffective use of cloud-based technology with a traditional provider
Need for comprehensive support for technical details
Scattered information across the industry
Lack of internet coverage and limited access to computers in rural areas
Digital literacy among farmers
High costs and less intuitive developer tools with AWS
"Handling high volume of image processing, tool and seed orders, and multilingual queries"
minimize infrastructure management demands
manage multiple servers across diverse locations
Existing solution could not speed up production and delivery
Technical capability requirements for customers
high foreign exchange fees and banking costs
compliance with PCI DSS
need for low latency and high reliability
short development and release cycles
Data stored in offline spreadsheets with little coordination
Clients needing to host the PIM themselves or rely on local partners
Demanding standards for security and availability in financial services
Infrastructure capacity shortfalls
Expensive and slow process of adding additional capacity
Latency between cross-service communications
Inaccurate data collection and frequent data loss with previous public cloud
Labor-consuming manual configuration for data compliance
Managing massive datasets
Attributing shifts in customer behavior to specific investments
ensuring validity of contract signatures
verifying identity of signatories
compliance with regulations in different countries
Uncertain and volatile markets making business plans outdated
Data coming in large quantities making it harder to make sense of it all
Need for a more heavy-duty solution than Compute Engine for the final product
Requirement for high-speed data handling for ad placements
Keeping costs and maintenance low while scaling
Minimizing the maintenance and administration load on technology staff
Supporting growth while maintaining stability and speed
Handling large-scale document migrations
Ensuring security against various threats
Existing solution showing signs of strain
Expensive data handling
Scattered infrastructure
Scalability issues
Analyzing millions of interactions at speed
Scaling to match 10% monthly growth in traffic
Infrastructure issues with previous cloud provider
Cash flow strain during COVID-19
high bandwidth of video data leading to latency
storage of large video and IoT data
Identifying small patterns in large datasets
Rapidly processing and analyzing vast amounts of data
scaling cost-effectively and quickly
complex calculations for pension services
eliminating outdated networks slowing response times
Seamless real-time collaboration across global units
Overhauling legacy on-premises infrastructure
Higher cost of ownership and maintenance of on-premises infrastructure
Scaling up hardware as needed
Inability of on-premises solutions to adapt to data growth and complexity
Time-consuming data parsing and preprocessing
Long wait times for test results
Limited on-premises data center capacity
Scattered internal view of the business from siloed tools
"Broad, unclean production systems"
Growing competition from larger banks and FinTech companies
Managing on-premises data storage and compute power
Securing client data during cloud migration
Timely processing of Canadian Emergency Relief applications
Ad hoc and deeply manual analytics processes
Multiple data copies and heavy manipulation to produce reports
Legacy data environment required enhancements for real-time analysis and insights
Dependency on faxes to get mammogram information from healthcare providers
Developing data analytics and maintaining IT infrastructure
Analyzing large amounts of data rapidly
Existing infrastructure slowing growth
Regulatory restrictions on cloud computing
Dependence on third-party data centers
Legacy system compatibility
Need to scale rapidly during peak periods
Handling large volumes of API queries
Managing significant increase in enterprise customers
Handling large volume of data from IoT devices
Handling extreme scaling from ten to thousands of transactions a second
Complying with privacy and security-related issues
Infrastructure challenges
Security challenges
Maintenance challenges
Monitoring challenges
Determining correlation between video and behavioral data
Achieving massive data crunching for predictive modeling
Managing an extensive amount of data
Satisfying data demands of internal stakeholders and external users
Ensuring PII security and GDPR compliance
Addressing bottlenecks caused by manual reporting processes
Keeping track of individual concessions' performance
Reacting quickly to changes in consumer behavior
Managing complexity from acquisitions
Centralizing services started only in 2021
Harmonizing and consolidating infrastructure and applications across Europe
Delivery delays of email marketing system
Manual tagging and marking of products
Previous on-prem infrastructure limited innovation
Complex queries taking too long to process
Data not in a single location
On-premises infrastructure not sufficient for analytics
Complexity and scale of medical data
Handling diverse formats of data
Querying data fast enough to provide usable insights
Enhancing management and analysis processes
Decision-making process needed improvement and automation
Building a team to implement FinOps culture
Ensuring control over costs and efficient expenses
"Need for modern, secure, and consistent infrastructure"
Compliant with safety regulations of a banking environment
Maintaining ability to develop and control costs
Ensuring security and regulatory compliance
Determining whether the product is running on physical servers or virtual machines
Identifying the technology stacks involved and how they are deployed
Migrating from the BlackBerry data center to a new provider
Highly regulated industry
Attractive target for cybercriminals
Increased attack surface due to transformation
Implementing a cloud-native security platform
Limited tech team to manage the system initially
Handling tens of gigabytes of data daily
System performance slowed with rapid growth
Managing infrastructure in-house
Needed a more sophisticated recommendation tool
Wanted to reduce friction in the sales funnel
Maintain availability and performance at low cost
Automate operations
Process data in real-time
Stability issues with previous cloud provider
Failure of small components shutting down wind turbines
Initial cloud service provider could not handle industry-focused solution
Ensuring security and reliability as an independent company
Handling payments as an intermediary
Legacy database not coping with the volume of data
"High maintenance, upgrading, and licensing costs"
Handling huge volumes of traffic
Scaling infrastructure during Ramadan
On-premises infrastructure limitations
Maintaining servers instead of improving platform features
"Issues with search results, payment processing, and user connections"
Infrastructure falling behind due to rapid expansion
Limitations of legacy SQL server
Handling increased downloads and active users during COVID-19
Inability to scale on-premises infrastructure
Long data processing times (4 hours)
IT team spending too much time on infrastructure management
Difficulty scaling with increasing data volumes and complex queries
Optimizing data warehouse to improve query performance
Scalability issues on existing platform
Weak integration between components
Tracking customer journeys across touchpoints
Complete overhaul of customer data
Engaging with customers online effectively
High storage costs with on-premises setup
Managing separate systems for structured and unstructured data
Time-consuming machine learning model development on old platform
Lack of initial customer data
Need for comprehensive data collection and analysis
Handling massive order surges during the pandemic
Needed cloud platform to scale and process data in real time
Ensured robots didn’t exhibit any change to customers during migration
Finding a cloud provider that offers advanced AI technologies and cost-effective scaling
Performing sophisticated data analysis and building ML models quickly and economically
"Manual data aggregation from Mixpanel, Salesforce, and spreadsheets"
Time-consuming report creation
Inability to segment data effectively
Reliability and stability issues with previous cloud provider
Virtual machine shutdowns affecting availability
Scalability limitations of initial setup with App Engine
Need for handling increasing data loads and new types of data
Fragmentation into many departments with their own technology stacks
Different on-premises databases and systems such as SAP
Grid instability due to variable input of renewable energy
Historical data reliance leading to poor prediction models
High time spent by data scientists cleaning poor-quality data
Handling complex system of APIs and microservices
Meeting stringent regulatory requirements in healthcare
Manual sales and sellthrough reports prone to human error
High integrity data warehouse requirement
"Delivering speed, scalability, and flexibility"
Need for fast system development
Requirement for secure invoicing system and optimized communication
Multiple datasets from multiple channels
Lack of multi-channel analytics
Unreliable mobile networks
Calculating shortest route between various points in real-time
Transforming the organization’s digital culture
Ensuring high productivity and connectivity
Managing large volumes of data
Increasing infrastructure availability
Managing sheer volume of digital customer intent data
Integrating and harmonizing data from various sources
Internal development team spending disproportionate time on data requests
Inconsistent metric definitions and data accuracy
Delays and outdated data due to reliance on development team
Initial infrastructure was unable to meet business requirements
Difficulty finding people with experience across multiple cloud environments
IT legacy environment syndrome
Scattered and heterogeneous infrastructure
Large VMware footprint
Lack of a unified platform for ecommerce needs
High cost of building and managing in-house data science team and tech stack
Complexity in managing different systems
Difficulty in sharing medical images between sites
On-premises servers reaching their highest capacity
"Need for a reliable, scalable, and flexible environment"
Managing backend provisioning
Ensuring real-time data insight
Handling sensitive financial data
requirement for stable cloud services
"supporting up to 50,000 concurrent connections without disruption"
Handling large data volumes without risking instability or performance loss
Uncertainty in volume of transactions and data to be processed
Managing 500+ nodes
Operating across multiple clouds
Establishing consistency and ease of use with immature blockchain protocols
Need for a reliable and scalable platform
Quick calculation and stability
Thousands of drivers connected at a time
Difficulties scaling up under a third-party cloud solution
Time-out errors for complex data query operations
Struggling with the demands of processing additional information
fragmented IT landscape
different technology stacks across departments
need for a unified data platform
Grid instability due to variable input of renewable sources
Poor quality data requiring extensive cleaning
managing complex system of APIs and microservices
ensuring security and privacy in heavily regulated industries
"Maintaining data integrity, security, and privacy"
Manual and error-prone reporting processes
Ambitious goal seen as impossible by some
Need to quickly build an app with a small team
Developing IT infrastructure quickly after becoming independent
Collating data from multiple franchisees
Building secure invoicing and data storage
Multiple channels with multiple datasets
Lack of multi-channel analytics
Dealing with real-life scenarios where downtime means missed customers
Unreliable mobile networks complicating data processing
transforming the organization’s digital culture
ensuring stability and scalability for data management
Managing the sheer volume of digital customer intent data
Marrying digital and offline data for a complete view
Disproportionate amount of time servicing data requests
Persistent problems with in-house data analytics tool
Inconsistent metric definitions and data accuracy
Initial infrastructure not meeting business requirements
Difficulty in finding people with experience across multiple cloud environments
legacy IT environment
Scattered and heterogeneous infrastructure
No one-size-fits-all solution for migration
Lack of holistic analytics products for middle to enterprise markets
Managing and integrating multiple acquired technologies
Managing different PACS systems
Exchanging medical images between sites
Limited capacity of on-premises servers
Need for reliable and quick data processing
Handling sensitive financial data
Maintaining low latency during peak demand
need for stable cloud services to avoid downtime
"supporting up to 50,000 concurrent connections"
handling large data volumes
uncertainty in transaction volumes
Managing 500+ nodes
Ensuring seamless operation across on-premises and cloud environments
Need to complete training models within one day
System delays leading to inaccurate journey times
Need for a robust system with high availability
Issues with scaling up under third-party cloud solution
High failure rate of complex data queries
Long query return times in previous setup
Large monolithic code base
Need for efficient server provisioning
Handling billions of analytic events
Managing large datasets of 1.3 PB
Analyzing individuals' genome structures securely and efficiently
Duplicated or error-prone records
Slow and cumbersome data analysis process
Lack of technical resources
Unacceptable delay in hardware deployment
Initial doubts about cloud capabilities
Need for scalable infrastructure across a large archipelago
Requirement to comply with Indonesian regulations for data centers
Switching to remote operations during the pandemic
Reports only available as raw spreadsheet data
Manual processing of data leading to bottlenecks
Daily request for data taking almost an hour
Scaling quickly without incurring too much cost was addressed by using Google Cloud
Need for fast ad response time was addressed by using Google Cloud
Underreporting of potholes and blighted properties
Manual reporting inefficiencies
Seasonal fluctuations in pothole repairs
Initial technology was not mature and acted like a black box
Combining online user behavior data with offline sales data
Limitations with original cloud provider for handling large volumes of time series data
High time to load historical time series data
Need for an architecture that works with both large and small data sets
Scaling infrastructure to support rapid data analysis
Engaging students in the classroom
High volume of initial security alerts
Deciding how to handle different security findings
Managing the operational complexity of the system
Need for a robust infrastructure to process large volumes of data
Ensuring zero downtime
Adapting to new ways of working
Avoiding resource bottlenecks
Handling enormous data volumes
Forming accurate predictive models with the data
Massive waiting times for uploading large media files
Requirement for a seamless user experience
Infrequent pricing data and limited access to historical market data
Setting up costly and complex data operations
Compiling data from hundreds of sources.
Mix of legacy applications and systems.
Slow and manual deployment of ML models.
High GPU costs
Need for effective data processing
High cost of storing large volumes of data in relational databases
Need to keep costs low while retaining accessibility
Unpredictable market and traffic spikes
Upgrading nodes to the latest protocol quickly
Need for remote education due to COVID-19
Different tools not mutually connected
Zero maintenance downtime required
Difficulty in responding rapidly to load changes
Complex configurations with partitioned databases
High costs and lack of scalability of on-premises technologies
Significant processing power required for probabilistic record linkage
Outdated legacy on-premises architecture
Navigating COVID-19 lockdowns during migration
Frequent outages with previous cloud provider
Lack of support from previous cloud provider
Need to quickly understand and react to COVID-19 impacts on members
Tracking service disruptions and their impact on members
Lack of centralized system initially
Anxiety and stress from decentralized system
Supply and demand challenges
Legacy cloud infrastructure inefficiencies
"Scaling to support new users, applications, and security requirements"
Uptime issues due to outages with previous cloud provider
Bot infiltration of the signup process
Impact of large files incorporating detailed architectural plans
Outgrowing BI platform
Over-complicated setup
Inefficient processes taking over eight hours
Consumers not informed about efficient use of PV systems
Need for scalable data storage and processing
Developing an intelligent crop yield management and prediction app
Providing app functionality without internet connectivity
Databases were scattered across various offices and warehouses
Manual data retrieval and analysis process
"Moving from static, printed menus to digital menu boards"
Complex hierarchy with different product offerings
Documenting revenue for cash transactions
Meeting data ingestion and storage requirements
Need for quick ramp-up of new hires
Huge investment in development
Adoption of several tools
Scaling storage
Scaling computational performance
System reliability and availability
Escalating costs of VMs
Manual installation of Kubernetes clusters
Broken irrigation valve flooding zone
High maintenance burden with peer-to-peer storage
Need to innovate quickly in a competitive market
"Difficulty in discovering, accessing, and combining data across functions"
Time-consuming nightly data imports
Scalability challenges with old data infrastructure
High costs related to queries by different business units
Securing data during constant imports and exports
On-premises data center lacked robust disaster recovery capabilities
Obsolete hardware in the Hong Kong data center
Processing over 1 petabyte of satellite imagery without large infrastructure
Managing rapidly increasing data volumes
High CPU loads from genomic analysis
Unpredictable data traffic
Maintaining bank of high-spec servers was inefficient
Auditability of the platform
Meeting high regulatory requirements
Ensuring data security
Fluctuating production depending on weather conditions
Balancing energy supply and demand in real-time
Storing large amounts of excess electricity
Legacy email system lacked granular analytics
Employees didn't have time to use advanced technology
High email volume and inefficient meetings
Initial concerns about not controlling infrastructure
Need to handle traffic spikes smoothly
Overly siloed databases
Slow underlying technology infrastructure
Need to maximize data value
Providing personalized recommendations
Lack of traceability
Food waste due to defective products
Ensuring shipment integrity
Integrating AI and cloud capacity
improve security features to meet banking sector requirements
Scalability issues with hosted VPC
Ensuring zero downtime during migration
Integration of multiple data sources
Time-consuming manual processes for API management
Need for a powerful backend solution to run deep learning search tools
Supporting GDPR compliance
Need for scalability
Handling peak loads
Dealing with siloed analytics data from channel-focused tools
Restricting the team's ability to gain insights across all devices and channels
Frequent traffic surges
Ensuring scalability and data security compliance
Surge in user numbers
Significant increase in volume of data
Long data loading times (16 hours)
"Technical challenges with different tools for integration, visualization, and analysis"
Need for real-time analytics
Need for a governed data environment
Synthesizing large amounts of unstructured data
Developing a functional prototype within a short timeframe
Local hardware issues
Responsibility for backups
Performance degradation with hardware load
"Data cleaning, verification, and normalization"
"Disjointed data ingestion, processing, analysis, and visualization steps"
Terabyte-scale data ingestion
Handling large increase in data flow
SQL queries running too slowly for large data volumes
Surge in volume of data
Need for efficient data assessment
Hard to migrate on-premises custom applications
Could not find AI/ML solutions that integrated with existing environment
managing infrastructure
processing data efficiently
Had to manage much greater quantity of new data types
"Needed scalable, reliable solutions for data storage, usage, and transmission"
Scalability limits of internal data warehouse
Hardware and software maintenance for on-premises solution
Need for real-time data processing
High cost of manual investment processes
Lack of visibility with open source backend technologies
Integrating data from different platforms
High costs and management difficulties of on-premises solutions
Downtime and lack of sophisticated capabilities with initial public cloud solution
Customer qualification and acquisition
Connecting with high-potential customers in areas with significant demand potential
Shortage of physicians and specialists
High patient no-show rate
Deficiencies in customer support channels
Complexity of public transport in Norway
Need to move away from on-premises infrastructure
Avoid vendor locking
Flexible infrastructure utilization
"Serverless, fast, and cost-effective data processing"
Outgrew previous host and couldn't scale fast enough
Needed a powerful solution to gather more data
Complexity of on-premises infrastructure
Difficulty in meeting compliance standards
Scalability capabilities not available with previous cloud provider
Transforming in-house software to a cloud-based solution
Eliminating legacy IT environments
Improving data quality and analytics
Requiring expertise in data management
Data duplication and siloed solutions
Legacy data warehouse with fragmented tools
maintaining physical warehouses
updating inventory manually every hour
long implementation periods for legacy warehouse management software
Initial complexities in the migration
Unexpected variability in costs
Fractured and inefficient enterprise architecture
Different IT approaches in 17 countries
Reducing time to market
Achieving profitability
Managing a scalable infrastructure
Scaling to handle future growth
Managing thousands of charging stations in real time
Immature cybersecurity
Limitations of existing SOAR platform
Unpredictability of infrastructure demand
"Lack of flexibility, scalability, and agility in hosting model"
Long time to process queries before Vertex AI integration
Existing BI processes limited by exponential marketplace growth
Salespeople lacking access to necessary information
Managing one-off projects with different datasets or calculations
Accommodating multiple sellers with varying needs
Providing a compatible and integrable tool with existing technologies
Ensuring fast deployment and intuitive interface for sellers
Scaling due to significant number of users
Distance between the company and consumers due to reliance on distribution and sales outlets
Secondhand knowledge about consumer behavior and preferences
Integration of all datasets for big data analysis
Improving system stability
Handling increased traffic and content volumes
Avoiding downtime
Managing infrastructure without substantial internal resources
Increasing processing times
"Ensuring 99.99% uptime and handling 300,000 requests per minute"
Previous infrastructure could not support sophisticated AI and machine learning
Initial infrastructure had limited scalability and stability
Lack of monitoring and support for complex AI features
Inflexible legacy backend
Pain points with legacy architecture
Significant computing overhead or lengthy customer interaction with legacy ecosystem
Aligning systems and data outputs after acquisition
Complexity of integration with legacy systems
Deploying within mobile network operators' data centers
Scalability issues with on-premises hardware
High hardware costs and difficulty in servicing/upgrading
Needed to move from physical data centers to a modern cloud platform
Understanding mobile user behavior
Overprovisioning servers to handle traffic spikes
Turning safety data into actionable insights
Need for complete discretion in communication
Gaining buy-in from community stakeholders
Need to modernize IT infrastructure
Reduce OpEx spending
Improve security
Respond faster to client needs
Complexity of providing scalability and stability in a monolithic environment
Time required for development in a monolithic system
Limited processing power with in-house server and laptops.
Accuracy issues due to down-sampling data.
Needlessly complicated or too expensive existing market solutions
Responding quickly to user feedback and publishing new features
Managing delivery drivers efficiently
Minimizing food waste
Processing large volumes of data quickly
Breaking up monolithic services
Lack of real-time data and insight into the quality of leads
Inefficient business processes
Frequent downtimes with on-prem servers
System errors with on-prem servers
DDoS attacks
Managing performance issues and hardware maintenance
High resource demand on Hadoop clusters
High and unstable latency with previous cloud platform
Manual setup of network nodes across continents
Incompatible third-party data analytics tool with own data warehouse
Time-consuming data transfer and report generation
Frequent game downtime on shared hosting platform
Manual server addition process
Handling increased server load during COVID-19
Managing multiple providers
Complex deployment processes
Multi-vendor solution was cumbersome and time-consuming
Lack of visibility to ensure availability
Five-minute latency in queuing metrics
Issues with network bandwidth and throughput
Various outages ranging from a few minutes to several hours
No full-time operations staff to handle issues
Legacy infrastructure unable to handle frequent changes in user traffic
Had to add more physical servers to accommodate user load
Managing physical server and network infrastructure complexity
Aggregating data across multiple servers
Requirement for high data security
Managing and analyzing large volumes of data
Providing precise detail in data quality
Previous provider unable to meet capacity and performance needs
Storage limitations forcing data to be trashed
Need for secure cloud infrastructure
Preventing vendor lock-in
Complexity of integrating disparate applications in DevOps
Historical constraint of monitoring vessels in small regions
Lack of resources in many countries to crack down on unlawful fishing
Insufficient data to design and manage sustainable fishery programs
Transforming the company culture to foster collaboration and innovation
"Integrating tools, systems, and data centers from acquired vertical software entities"
"Handling data residency, GDPR, and other regulations"
Justifying development with business cases prior to AppSheet
Managing customer visits during the pandemic
Gaining insights faster independently of parent company
Accessing and analyzing real-time data generated in-store
High cost of traditional laser cutter/engravers
Complexity of specialized firmware and hardware
Access to genuine medication in rural areas
Logistic challenges with reach
Low working capital of smaller chemists
Value leakage in distribution channels
Difficulty in running queries on SQL servers manually
Peaks in traffic
Setting up new servers was time-consuming and error-prone
Operational burden with previous hosted data center
Business units losing valuable time looking for accurate and reliable data
Manual queries required by non-technical business owners
Finance team spending four days on monthly revenue numbers
Duplicating work and difficulty in finding required data
Manpower to manage Kubernetes container technologies
Processing data from multiple sources
Ensuring data integrity
Scaling infrastructure to meet growth demands
Ensuring 24/7 availability
Maintaining security for children's data
Use of several hard-to-operate and enhance legacy systems
Expensive support for legacy systems
"Legacy infrastructure not scaling reliably, securely, and effectively"
Addressing expectations among colleagues with new Google Cloud
Running out of physical server space
"Latency, performance, and connectivity requirements"
Protecting core trading systems
Need for sophisticated advertising targeting and content delivery
Requirement for external assistance to access full range of cloud products
Operational burden with legacy infrastructure
Handling spikes in traffic with legacy systems
Previous dashboarding solution failed to deliver data reports quickly
Difficulty in harmonizing data sources
Managing and securing digital assets
Complying with regulations
Migrating applications and data lake
Integration with multiple third parties
Managing unpredictable traffic spikes
long processing times on on-premises server
difficulty in file indexing
handling various image formats
massive volume of files
Unifying and adapting all the brands’ data
Establishing a data pipeline
Segregated regional networks caused poor cross-region connectivity
Time-consuming setup for on-prem ML training systems
Frequent capacity expansion for on-prem data storage
Scalability issues with on-premises infrastructure
Stability issues with IT staff managing servers
Reducing infrastructure and system management load
Lowering costs
Improving agility and speed of software development
"Manual, labor-intensive way of recommending products"
Legacy approach with complicated spreadsheets
Original hosting limitations
Billing inflexibility
Delay in server activation
Load issues and backup of databases
Long wait times for new server capacity
Unpredictable surges in demand
Significant engineering bandwidth and maintenance costs for in-house dashboarding solution
Requirement for customizable and scalable third-party BI tool
Evolving requirements necessitating updated architecture
Relied on labor-intensive statistical analysis in R or Excel
Need for an analytics solution that could support multi-tiered product offerings and multi-tenancy
Transitioning from a legacy system
No national portal offering nationwide perspective of COVID-19
Scale to handle over one million visitors simultaneously
Strong security and identity and access management policies required
Reliability issues with previous hosting provider
High-traffic periods causing website slowdowns
Avoiding business disruption during peaks
Responding quickly to new online promotion opportunities
Integrating with multiple platforms
Reducing downtime
Implementing a disaster recovery system
Increased competitive pressure from fast fashion and new designer brands
Costly and inefficient process of storing and photographing textile designs
Reducing latency between cloud data centers
Single points of failure in technology stack
Time-consuming and error-prone manual visual inspection
Need for easy-to-use AI service for on-site operators
Speed of identification and integration with existing infrastructure
Handling high traffic and high variability
Meeting data localization requirements
High percentage of crew duty swap requests
Maintaining a complete maintenance history for leased aircraft
Patchy online analytics
Difficulty integrating data from multiple sources
On-premises infrastructure was an impediment
Developers spending too much time on basic tasks
Customer-centric applications couldn't run with 100% availability
Service outages and disruptions with previous provider
Difficulty in scaling with previous locally hosted provider
Manual process taking hours
Limited capacity of existing on-premises hardware
Network disruptions due to metro construction
Service downtime and system crashes with high traffic
Information security issues with multiple software developers
Database reliability issues with previous provider
Unwieldy logins
Unpredictable customer support
Fragmented data across multiple systems
Traditional on-prem data platform limiting transformation
Lack of agility in existing infrastructure
Gap between business unit requirements and data architecture capabilities
Solving day-to-day technical questions
Ensuring efficient use of Google Cloud to control costs
need to restructure architecture
requirement to modernize and migrate to Google Cloud
Fragmented on-premises IT system.
Separate systems that didn't communicate well.
Manual and unreliable processes.
Performance issues with initial platform due to legacy decisions
Scaling up to meet demand while ensuring security
Managing and scaling on-premises infrastructure
Dealing with version discrepancies and fraud in Excel-based models
Ensuring data privacy and security compliance with Swiss Federal Data Protection Law
Integrating different components and technologies
Finding valuable information from large data sets
Handling data from diverse sources like ecommerce platforms
Ensuring high availability during peak periods
Traditional insurance companies' monolithic architecture using non-cloud tech-stacks
Need for accurate and reliable weather data for weather index insurance
On-premises infrastructure with little integration
Long time to get new services up and running
High costs and reliance on external consultants
handling traffic peaks
service crashes during high traffic
maintaining service availability
Processing time for tests
Security and regulation concerns with patient data
High network latency
Access speeds and reliability issues
Handling mobile traffic growth
Inconsistent network coverage
Scale infrastructure quickly
Stay agile with innovative solutions and cost efficiency
Migrate clients without interrupting the service
initial migration of data and applications
ensuring high availability and performance
coping with variations in volume of processing
cumbersome and time-consuming licensing process
lack of proper tool for data collection and analysis before Google Cloud
Limited scalability of on-premises setup
Traffic doubling due to COVID-19
Network stability issues with legacy CDN
High egress costs
Time-consuming DevOps provisioning
Maintaining servers manually and troubleshooting at odd hours
Migrating user information due to massive user base
lack of visibility and collaboration
long response times and high costs with other generative AI services
moving from prototype to production with other models
Scaling operations manually
Handling large spikes in traffic
Scaling to hundreds of thousands of stores and billions of transactions
Providing fast search performance for large product catalogs
Analyzing large amounts of data securely and compliantly
Data silos within different organizations
High costs of managing on-premises infrastructure
Performing large scale and complex interpretations of genomic sequencing data
Reaching the limits of on-premises infrastructure capabilities
Data warehouse and ETL were on-premises in a multi-tenant infrastructure
Lack of direct query access
Day-old reporting with overnight batch updates
Lack of transparency and scalability
Manual management of services
System load issues
Scaling and upgrading challenges
Downtime due to VM clusters scaling
Data loss and downtime due to hardware failure
Handling sensitive student data
Ensuring low IT costs
Scaling up API clusters
Ensuring security and reliability in operations
Lack of in-house AI and data engineers
"Understanding business domains, statistics, and time-series models"
Handling hundreds of millions of records of purchase data
Disparate systems and siloed information
Maintenance and support of variable hybrid infrastructure
Reducing delivery times
Improving platform interfaces
Scalability and efficiency of infrastructure
High maintenance costs
Lack of flexibility
Conventional data analytics approach could not respond fast enough
Need for real-time alerts on viewing issues
Adapt to a cookie-less world in advertising
Ensure higher priority to owned channels
"Inability to query, analyze, or generate reports from logged data"
Need for timely insights without backlog
Manually recommending products was time-consuming
Need for a reliable and fast implementation
Scaling infrastructure
Availability issues
Running out of storage space
Long analysis time
Need for powerful data processing with flexibility to control costs
Constant market analysis for appropriate bidding
Managing job post inventory data
Improve software deployment
"Automate scale, manage software, get alerts"
Rebuilding business from scratch during the pandemic
Scale and latency issues
Need for real-time data analytics
Compatibility with hybrid infrastructure
Need for powerful data analytics tools
Manage traffic surges
Scaling technology for big data
Understanding how people consume and search for books
Integrating dozens of independent and disparate data exchanges
Building highly available and low-latency systems
Need for better control over infrastructure
Zero-downtime migration
Managing high data volumes
Existing infrastructure was manual and cumbersome
High operational costs of bare metal infrastructure
Scalability issues with the old setup
Differences in implementation processes between iOS and Android
Persistent stability problems with Kubernetes cluster
Limited language documentation for NLP researchers
Difficulty in finding Indonesian NLP specialists
Network intermittently going down with previous provider
Managing on-premises data centers and disparate technologies
"High cost, time, and effort to scale compute resources"
Need for improved API management to handle high traffic
Administrative burdens
Disparate systems
Lack of process standards
Long activation times
Limitations of the previous platform as traffic increased
Need for server and maintenance support
Complexity of the insurance market
Cost of retraining models in a more expensive machine learning environment
Data isolated in physical servers
"Slow, manual data analytics process"
Legacy data solutions
Aging on-premises data center
Reduced performance and downtime
Manual administrative tasks
Inconsistent uptimes with second-tier cloud hosting companies
Unplanned maintenance windows
Lack of live migration
High data warehousing costs
Time-consuming data import process
Maintaining high QA standards at scale
Complex and expensive QA process for large companies
Doubling cloud consumption costs
Ensuring applications run properly on managed cloud services
Scalability and speed for real-time bidding
Minimizing latency for distributed users
Inadequate support from previous cloud provider
Control costs while supporting growth
exhausted memory resources
slowed CPU performance
exceeded time and resources requirements managing Hadoop and Cassandra
Record peaks of users
Managing performance bottlenecks
Delivering seamless experience in low-bandwidth areas
Handling high number of cloud database operations
Managing DevOps demand and resource management
Risk of stunting growth with insufficient infrastructure
"Started operations with another cloud provider but needed more scalability, reliability, and higher performance"
weekly outages caused by scaling issues
latency beyond 250 milliseconds
platform crashing during celebrity streams
Increasing data maintenance costs
Performance issues with the private cloud network
Limited support from domestic data center operators for overseas bases
Difficulty responding to rapid changes in data volumes and compute resources
Loosely connected IT systems
On-premises mainframes and distributed servers
Cost-prohibitive on-premises upgrades
Manual and time-consuming audits
Access and query limitations with SQL
Scattered and inconsistent spreadsheets
Data silos across organization
Delayed access to financial data
Staffing challenges and funding pressure
"Need for a more organized, central data solution"
Concerns about security and reliability
Requirement to handle rapid growth and scalability
Handling traffic spikes
Outdated website
Strain on servers during high traffic
Handling a fluctuating volume of users
Ensuring security while managing payments
Performing migration without disrupting service
Managing the complexity of an increasing user base
Manual processes for gathering and analyzing information
Financial cost due to low return on investment from player transfers
Scant opportunities for players
Handling large streams of data
Running powerful queries on an ad hoc basis
Ensuring reliable platform performance
Complexity of infrastructure during migration
Original hybrid system struggled to meet demand
Problems with compute availability and capacity
Infrastructure as an obstacle due to lack of separation between storage and compute
"Existing internet data center infrastructure poorly equipped for data capture, storage, and analysis"
Traditional BI solution could not support growing need for real-time data analysis
Complex back-office processes
Managing Kubernetes clusters on existing cloud provider
Regulatory and security requirements
Existing systems lacked processing power for large data volumes
Complexity of on-premises systems
High resource demand for image recognition
Costly spikes in server traffic due to prepaid contracts
existing setup became a bottleneck
difficulty in capacity planning during peak seasons
existing infrastructure unable to sustain another year of growth
Complexities of the grocery sector
"Need for a robust, multi-tenant scalable solution"
Requirement for real-time data processing
Legacy infrastructure struggled to handle volume
Scaling infrastructure during peak periods
high investment in self-managed platform
"issues with stability, reliability, and scalability"
need for zero downtime during migration
Managing infrastructure
Scaling operations
Increasing cloud hosting costs
Resource-intensive data processing
Previous cloud providers preference
Managing infrastructure operations
increasing GPS accuracy
Keeping ahead of evolving user expectations
Developing solutions that do not burden developers with operations
Ongoing company growth
Increased volume of digital transactions
Scaling and performance issues with previous data platform
Operational overheads of the previous platform
Scaling database requirements
Legacy database solution not keeping pace with growth
Inability to instantly activate/deactivate computing power with hosted servers
Need to reserve computing power with hosting company
Time-consuming nightly batches
Handling higher traffic loads during the pandemic
innovation in a tradition-rich sport
engaging younger generations
Piecemeal IT infrastructure
High costs and management complexity of Kubernetes
Features and lack of features of current cloud platforms
"Reinventing many services, especially in security network installation"
Handling much more data without growing the platform team
Difficulty in predicting capacity and workload requirements
Cloud provider's difficult-to-navigate user interface
Data was siloed in different systems
Getting the right items in front of the right customers
Getting internal work processes sorted during rapid growth
Aligning with staff and clients using different conferencing software
item duplication or loss
processing speed and reliability
Difficulty in quickly and effectively answering customer service requests using an outdated switchboard model
Increased call volume leading to significant cost impact
Need for platform adaptability and scalability
Transitioning from traditional IT infrastructures to the cloud
Need to recommend suitable products based on customer goals
Ensuring chatbot interactivity to keep customers on the website longer
Legacy infrastructure contract expiration
Adapting to rapid changes in consumer behavior
Ensuring the platform fulfills several criteria before deployment
Orchestrating a modular architecture
Proactively preparing the exposure of services
Difficulty configuring server instances
Slow network performance
Increased costs due to data growth
Maintaining server hardware and data storage
Managing highly regulated environment
Need for fast solution delivery
Local data center could not scale to support demand fluctuations
Need to maintain high performance and uptime
Ease maintenance burden on IT team
scaling clusters manually
complex and costly task of maintaining IT infrastructure
accurate data collection in poor network conditions
Complex 12-hour data restores
importing data from antiquated healthcare systems
streaming terabytes of data from self-driving cars
maintaining large datasets and computational workloads
Migrating more than 100 services
large and unwieldy monolithic architecture
difficulty in making marketplace compelling for different markets
Data governance in microservices architecture
Inconsistent style and quality of queries
Unknown data source relationships
Redundant dashboards increasing cost
Manual classification was not scalable
Need for greater efficiency
Compliance obligations for businesses and exchanges
Standardization limiting educational potential
Identifying student passions and matching them with learning pathways
Handling diverse needs due to increased growth and rise in ESL students
Fixed ways of working in the HoReCa industry
Limitations of on-premises data center for advanced ML techniques
Need for individualized customer data
Need to offer seamless experience for thousands of global attendees
High server costs
Large volumes of data with large processing requirements
Burst pattern data processing
unstable data center operations
inability to automate and improve scalability
system unavailability
Slow and unstable SQL server
High costs requiring large investments
Need for scaling
Infrastructure maintenance and administration
Relatively expensive cloud-hosted infrastructure
Low traffic volumes initially within budget
In-house developed PostGRES database not scalable
"Slow report generation for marketing, analyst, or business teams"
First data management service was labor intensive
Outdated payment infrastructure
Scaling issues with traditional financial solutions
Supporting daily news services with an on-premise environment
Traffic spikes from breaking news
Slow response to customer inquiries
Misunderstanding IoT as just SoR
Cultural shift within the organization
Lack of experience in software development
Need for continuous improvement in software
Frequent and easy access to newly organized data
Creating what-if scenarios
Diagnosing problems
Offering a range of policies on tight deadlines
High technical barrier to entry for users
Reliance on BI team for reports
Tool crashing and lost work
Issues with interface and usability of legacy tools
Metric consistency and trust problems with visualization tools
Lagging progress in DevOps process
Inefficient cross-team communication
Inadequate use of DevOps products or tools
High traffic in VOD services leading to inaccessibility
Handling massive data traffic fluctuations
Need for predictive analytics to anticipate traffic trends and network stability
Complex technology management
Developing intuitive interfaces for digital novices
Manual data manipulation and data silos
High barrier to entry for data analysis
Exceeding budgets for shipping requests
Difficulty in analyzing unstructured user feedback
Building and training excellent machine learning models
Developing surrounding systems for infrastructure
Rising costs with traditional cloud service
Lack of local support
Extremely fractured data
Manual data aggregation prone to errors
Legacy analytics platform limiting scalability
High management requirements with existing cloud provider
Expensive on-premises solutions
Siloed data environment
Streaming data not being captured
Complicated data ingestion framework
Maintaining infrastructure and data pipelines
"Data coming in at different times, from different silos, in different formats"
Lack of stock visibility hindering fast adaptation to online retail
Handling unpredictable traffic patterns during large game launches
Ensuring low latency and high performance for superior gaming experiences
Meeting high-privacy HIPAA standards for patient health records
Processing large amounts of daily data
Existing infrastructure was buckling under demand
Outages in data pipelines and slow performance
Spent more time fixing data pipelines than building features
limits on call center availability
need for nuanced customer interactions
requirement for high-quality linguistic processing
Improving system availability
Accelerating rollout of new services
Addressing outdated infrastructure
Existing monolithic architecture was not scalable
Frequent deployment delays
Outdated on-premises environment
High costs of updating on-premises servers
Ensuring no disruptions during peak sales period
Traditional intelligence gathering is slow and expensive
optimizing delivery
managing infrastructure for scale-up
accurately forecasting demand
Distinguishing quality from hype in rapidly generated crypto assets
Processing petabytes of trading data daily
Handling product and team growth globally
Maintaining focus on customer experience platform while managing backend administration
"Labor-intensive, manual system for measuring research outcomes"
Limited ability to track digital engagement and evaluate impact
Insufficient local computational resources
Need for data reanalysis for consistency
Disparate platforms making unified analysis difficult
Extensive data pipeline work
Administering various cloud environments
Manual resource management to match demand peaks and troughs
Processing larger volumes of data at speed on aging servers
Operational complexity with traditional data warehouses
Managing regulatory relationships and compliance
Connecting first-party customer datasets with data from customer interactions
Maintaining two parallel data warehouses
Maintaining infrastructure and running concurrent pipelines
Managing large volumes of data for AI workloads
Hosted SAP applications on-premises
Stringent requirements for finance and accounting
"Need for high availability, accessibility, and security of SAP apps"
Variable hospital broadband speeds
Legacy systems not supporting secure DICOM transfers
Differences in scanning equipment and small datasets for model training
Unplanned surges in demand
Predicting resource needs for discounted rates
Managing payments and distributing vouchers in remote areas
Ensuring high reliability due to thin margins in business operations
Absorbing large spikes in traffic
Ensuring no downtime during peak seasons
Staying connected internally as a coalition
Ensuring smooth migration to cloud-based tools
Supporting employees whose tasks didn't involve IT tools
Maintaining collaboration during the COVID-19 pandemic
Processing and manipulating enormous datasets
Extracting useful data from non-electronic sources
Need for agility and scalability
Cost of storing unused data
Building out a data center
Capacity planning
Scalability
High software costs
Reliable communications infrastructure
Sizable processing workloads
Scalability for millions of customer queries
Time and labor-consuming virtual machine provisioning
Need for secure and scalable data storage
Managing underlying operations in microservices migration
Data residing in different marketing systems
Lack of a single platform to track and analyze digital data
Managing growth in the user base
Dealing with infrastructure expenses and maintaining machines
expensive maintenance of previous infrastructure
poor data isolation support
slow query performance
overloaded data warehouse dropping queries
Proactively addressing cybersecurity risks
Managing cloud infrastructure with a small team
Initial system struggled to cope with the amount of data
Portal experienced latency
Needed infrastructure that could scale and provide immediate data insights
Minimize downtime for mission-critical applications
Compliance with storing business-related records for seven years
Integrate local accounting systems with global headquarters
high cost and complexity with previous cloud provider
Traditional rules-based monitoring system
Rising transaction volumes
Meeting high demand
Scalability concerns
Escalating costs with initial public cloud service
Requirement for high availability and low network latency
Delivering up-to-date profile and incentive data
Migrating clients onto a centralized infrastructure
Developing bespoke or ad hoc applications
managing different sources of data from different channels
maintaining a clear idea of customer needs during growth
Ensuring high performance and availability
Enhanced security requirements
Handling dynamic and resource-heavy JavaScript websites
Running tasks in perfectly automated sequences
Fragmented digital market
Need for a scalable and cost-effective platform
Infrastructure management and scalability challenges due to high ad traffic
"Keeping physical data centers performant, available, secure, and up-to-date"
Limited flexibility in development due to monolithic application
"Need for dependable, robust infrastructure with global reach"
Maintain compliance and security
Open the door to using AI
Reduce complexity around managing data
Scaling quickly and reliably without draining resources
Managing infrastructure with a small team
On-premises solution holding back scalability
Need for faster query response time
Difficulty in seeing and analyzing all expenditures in one place
Render time on physical machines
Handling higher pixel ratios and workloads
Adapting to highly demanding and volatile end customer base
Handling unexpected spikes in demand or traffic
Establishing best practices for cloud solutions
Identifying untapped savings potential
Manual handling of over 200 virtual servers
"Long ETL process taking 14 hours to load data, 10 hours to transform it, and 4 hours to run reports"
Experiencing major failures 40 percent of the time
Handling demand peaks that made the network inaccessible
High API latency of 1 second
Need to scale without expanding team size
10X increase in users within two months
Overloaded production database due to query volume
Initial simple infrastructure was insufficient
Need to handle high traffic and maintain uptime
Initial use of virtual machines became expensive and cumbersome
Hard to manage platform without orchestration tools
Network instability under heavy load
Need for a more innovative cloud services provider
Insufficient on-prem computing resources
Need for abundant computing resources for developing and testing AI products
Augmenting on-premises solutions with cloud-based services
Ensuring security and gaining customer confidence in new cloud product
Lack of pre-existing tools
Radical changes due to COVID-19 for remote learning
High workload to build curricular resources
Manual provisioning became overwhelming
Latency and scalability issues affecting customer experience
Handling multiple platforms with conflicting protocols
Spending more time on infrastructure settings than building code
Maintenance of monolithic applications in a data center
Need for a more nimble and scalable environment
Long server acquisition time with on-premise servers
Lack of good monitoring systems
Service crashes due to storage overload
Keeping up with increasing demands for analytics
Understanding if query errors were due to performance issues or resource depletion
Hosting many applications on own data centers
Accelerated need for omnichannel solutions due to COVID-19
Adding capacity resulted in downtime in traditional third-party private cloud model
Need for optimal performance and streamlined IT administration cost-effectively
Multiple sources of truth from siloed tools
Manual and time-consuming reporting processes
Conflicting results from different data sources
Optimize speed of SaaS solution
Move from a multi-cloud approach to a single provider
Need for minimal latency and rapid response times
Handling large volumes of data
managing millions of smart objects globally
"ensuring security, high performance, and scalability uniformly"
maintaining low latencies
handling huge variations in load
building data centers quickly
managing operational costs
Repetitive integration processes
Long time to market (18 months)
Soaring cloud expenses
Physical infrastructure limitations in data center
Service interruptions during peak demand
Predicting future bandwidth needs
Identifying chokepoints in processes
Handling traffic spikes during peak times
Manual listings of customer addresses excluding some customers
Manual creation of trade zones delaying delivery services
"Manual, self-reporting mechanism for tracking delivery times"
Handling immense satellite data
Processing 7 to 10 terabytes of data daily
Scaling compute and storage platform
Higher data storage costs
Slower data processing
Increase in tech support requests
Previous infrastructure stability issues leading to downtimes
Network unavailability and server crashes
Starting with ML can be challenging for companies with lean engineering resources
Aggregation became a real problem
Queries slowed to a crawl
Specialist SQL knowledge required
Executive staff reliance on developers
Withstand huge spikes in traffic
Maintain high performance with limited resources
"Data queries becoming stuck, delaying resolution and completion of other queries"
Need to assign engineers to resolve issues monthly
Scalability limitations of private cloud
Stability issues
High operational costs
Handling diverse and proprietary medical device data
Scaling monolithic server-based infrastructure
Ensuring data privacy and security
Handling unpredictable demand fluctuations in the blockchain market
Reducing blockchain data indexing time from 20 seconds to less than one second
Making PoD DataFabric available in Google Cloud Marketplace
Shortening the distance between stored data and data scientists
Maintaining physical infrastructure across jurisdictions
"Managing and maintaining servers, storage, and networks"
Aging on-premises infrastructure
High costs of compute power with on-premises servers
Lack of pre-production environments
Manual data-modeling processes
Lack of scalability
Need to optimize infrastructure
Managing and scaling VMs with a small IT team
Handling cross-region projects
Slow loading speeds and occasional downtime due to traffic fluctuations
High latency for users far from main servers
Legacy infrastructure spread across three data centers
Capital-intensive resource planning
Coordination for deployments and A/B testing
Transition from print to digital media
Manual and ad hoc data operations and management
Need for a more sophisticated data solution
Intermittent downtime and patchy speeds
Extensive maintenance requirements of previous product
Lack of single source of truth
Hosted servers did not provide flexibility for large peaks in traffic
Managing cloud infrastructure with a small team
High spending on infrastructure tasks with a previous provider
Labor-intensive task of measuring and analyzing traffic
Cost-effective storage and easy access to large amounts of CAV data
Performance and scalability limitations of on-premises servers
Navigating high-inventory data
Low conversion rates
Managing server infrastructure
Managing large amounts of data
Handling component requests during sales spikes
Ensuring correct pricing
Manual data aggregation was time-consuming
Scaling capability limitations of on-premises data centers
Downtimes and failed GPU tasks with previous provider
Need for real-time analysis
Stability issues with previous cloud providers
High cloud costs and data transfer bills with previous providers
Speed and scalability challenges
High costs and erratic management of the old data warehouse
Limited by relational database
High cloud hosting costs
Siloed data managed on-premises causing scalability issues
Sprawling IT resources making it difficult to streamline data access and insights
Fragmented and inconsistent data stores
Sudden traffic spikes
Over-provisioning cloud resources
Time spent monitoring platform
Servers were over provisioned
Engineers bogged down by infrastructure management
Managing multiple cloud providers
Latency issues
Outages with some cloud providers
Handling spikes in traffic effectively
"Maintaining exceptional speed, security, and scalability"
Previous cloud provider's unreliability
Incomplete and difficult documentation
Failed deployments
Slow and limited bandwidth in Taiwan
deduplicating location data
consolidating data to a single repository
making relevant views accessible to business stakeholders
Previous cloud provider outages
Rapid data growth
Need for fast data acquisition
Infrastructure scalability and dependability issues
High costs of existing log analysis solutions
Over-reliance on engineers with high technical skills
Initial trial and error with ETL processes
Managing the growing scale and complexity of data
Replacing existing batch processing solution with a distributed streaming model
providing medical services during COVID-19
ensuring the continuity of treatment and consultations
Preparing data for analysis
Scaling and allocating resources quickly during demand spikes
Infrastructure limitations impacting speed and scalability
Difficulty in generating relevant data products at speed
Managing volumes of transactional data
Building a scalable infrastructure
Balancing historical requirements with future needs
Finalizing migration during COVID-19 pandemic
Regulatory requirements like LMIV
Coordinating multiple logistics processes
Managing data from various sources and locations
Managing file attachments and repetitive tasks
Ensuring optimal use of underlying technology
Migrating from traditional to digital
Handling large workloads
Ensuring tags on websites do not fail or experience anomalies
Reducing operational overheads and pressure on the internal team
Translation of creative assets into multiple languages
Need for instant initiation of operations
Existing infrastructure struggling to keep up with demand
Uptime issues
Inefficient distribution of actionable data
Slow and cumbersome scaling with previous platform provider
Information not found
Matching address and search terms with variant spelling
Supporting six languages
Handling five terabytes of data daily
Unsustainable infrastructure of rented servers
"Multi-zone disaster recovery, high availability, automated failover issues"
Complex computations requiring robust infrastructure
Fragmented marketing ecosystem
Operating in silos
Adapting media plans during global pandemic
Convincing stakeholders to trust digital education platforms
Managing growth to accommodate increasing user base
Eliminating outages and improving user experience
High costs and operational challenges of maintaining on-premise IT infrastructure
Potential outages or data losses impacting customers
Managing expansion with a lean team
Handling fluctuating IT resource demand
Uncertainty in data model design
Need for rapid updates and redesigns
Scalability issues during traffic spikes
Global content delivery speed
Traditional IT environment with off-premises infrastructure
No access to data on the road for salespeople
dependency on SAP and legacy systems
manual and cumbersome customer quote process
unable to support rapid testing and rollout of new applications with legacy setup
"needed in-depth and actionable analytics, real-time reporting, secure and centralized storage, full enterprise visibility"
Initial cloud platform didn't support flexible computing resources management
Complicated development process due to rigid platform
Latency variation with initial public cloud service
Handling heavy data reading and complex matching queries with consistent performance
Traffic spikes causing downtime
Excess costs from idle VMs
Slow load times for data-intensive content in rural areas
handling millions of interactions within seconds
performance issues with local hosted servers
traffic peaks misinterpreted as DDoS attacks
"High costs of project launch, implementation, and maintenance"
Limited scalability in on-prem environments
Service disruptions due to insufficient computing resources
None explicitly mentioned
High degree of decentralization and disconnection within the group
Balancing enthusiasm for new technology with security and compliance
Heavy regulatory requirements
Ensuring security for clients’ financial data
scaling and storing data
validating data
Previous hosted servers could not cope with sudden additional traffic
High downtime (24 hours annually) with previous infrastructure
Scalability issues with PC-based data collection
Manual checking of images from insect traps
Unplanned downtime due to pump failures
Visually inspecting every well daily
Building a data lake from scratch
Managing large datasets
Reducing setup time for in-store sensors
Improving speed to market
On-premise servers struggling to avoid crashes
Difficulty scaling seamlessly amid wild swings in concurrent users
Need to build and evolve game tokenomics
Manual management of big sets of data
Difficulty in dealing with diverse linguistic accents in speech-to-text systems
Directing ground operations at airports toward internet technology
Handling an average of more than 800 flights a day with previous reporting processes
In-person visits dried up due to COVID-19 pandemic
Providers losing revenue
Expensive and complex on-premises infrastructure
Integrating data from different banks
Data security and regulatory compliance
High costs and time consumption with the previous setup
Clash of mindsets with the initial cloud provider
Over-provisioning compute and storage
Traffic growth to 500% over a few days
Limitations with NoSQL database
Migration of 70 terabytes of data with zero downtime
Poor network infrastructure in some regions
Connectivity issues when sharing files
latency in image rendering
manual data extraction for business reports
Handling flow surges during promotion campaigns
Scalability issues with on-premises servers
Lack of centralized data platform
Inconsistent metrics leading to confusion
Difficulty in scaling with multiple hypervisors and virtual machines
Maintenance requirements of old infrastructure
Spikes in traffic overwhelming the system
Patients in developing regions lacking formal ID
Technical errors due to limited infrastructure scalability
Long preparation time for new product or feature deployment
Deployments not being synchronized
Downtime and deployment delays
Ensuring high compatibility with other applications on OTT devices
Meeting local audience's content needs
Handling traffic surges and updating nodes
Managing routine tasks with virtual machines
Align infrastructure investments with business growth
balancing powerful resources with startup limitations
Need to manage and react to information from a connected network of IoT devices
Complex interactions like predicting traffic impact from parking structure exits
Outdated on-premises hardware
Risk of central point of failure in case of a power outage
Processing large data volumes with high availability
Complexity in processing data volumes
Manual processes
Data leveraged mostly by technical teams
scattered data sources
legacy infrastructure
irrelevant or out-of-date recommendations
Managing high traffic spikes
Building a robust app within a short timeframe
Previous cloud solution put too much administrative burden on technical team
Lack of power and features needed for fast decisions at scale
Difficulty in scaling shared models and dashboards across customers
Loss of key modeling capabilities with prior BI tool
Laborious manual tagging of images
Inefficient search for relevant images due to lack of tagging
Prohibitively expensive and time-consuming manual processes
Transition from 20-year-old legacy equipment
Need to learn new terminologies and troubleshooting methods
limited experience with infrastructure
database replication and failover issues
Need to handle seasonal traffic spikes
Difficulties in quickly responding to infrastructure and traffic fluctuations
Pragmatic juggling of higher-level strategic work and routine database management
Finding areas that offered large performance improvements
Managing a large volume of image assets
Inefficiency with a static hard disk system
Time-consuming manual processes
Issues with platform uptime on previous on-premise and VM infrastructure
Burden of manually configuring VMs
Increasing availability of growing operational and IT data
Streamlining connectivity in a specific network and handling large data volumes in on-premise database
"Managing data, algorithms, and computations"
Deploying and managing hardware
Procuring and configuring infrastructure
Managing infrastructure with other analytics tools
Need for accessibility as much as raw power
Existing infrastructure poorly equipped to enable transition
Disparate systems making timely reporting and analysis difficult
Problematic data siloing
Lengthy query execution times
Standardizing a huge diversity of data types and formats
Freeing up data from individual systems and third-party vendors
Exponential spike in user traffic
Manual scaling of servers
Entire infrastructure went down for a day
Unprecedented surge in online orders during COVID-19
Ensure platform resilience and scalability
Overprovisioning infrastructure was expensive
Rented infrastructure led to additional concerns
Maintaining infrastructure with minimal downtime
Commoditized fiber optics market
Orchestrating global network efficiently
Inconsistent data access across departments
Scalability limitations with previous hosted services
Monolithic architecture required coordinated updates and maintenance
Building DevOps and production environments from the ground up
Manual deployment of multiple services with different requirements
Managing physical resources and lack of convenient scaling
High operational costs before adopting Google Cloud
Cost to increase capacity in on-prem data center
Accelerated timeline for migration
Need to efficiently run Oracle databases
Long deployment times
Bottlenecks due to single server
Delivery accuracy in Indonesia
Shifting from a large enterprise mindset to a startup mindset
Teething issues with out-of-the-box ecommerce options from other providers
Inadequate workstations for full-scale research into deep learning
Time and effort required to introduce new workstations
Running on outdated legacy systems
Internal resistance to the cloud
data decentralization impaired communication
needed a platform to unify and accelerate environment
Decentralized data from partners
Different languages and terminologies
Data formatting issues
Long processing time to send data to CRM
Time-consuming internal project execution
Navigating local country policies around data privacy
Managing and scaling infrastructure manually
Handling large amounts of player-generated events
change in infrastructure
previous reliance on another cloud provider
Previous solution was outdated and standalone
Need to keep consumers on the website during navigation
Complex IT system
Overcoming constraints on the deployment and operation of the infrastructure
Meeting increasing analysis demands from businesses
Lack of cloud automation services
Storage of large data volumes
Need for migration without shutting down services
Slow VM start-up times
APAC region connection issues
Need for powerful VMs with strong connectivity
"Need to guarantee more than 30,000 kilometers between basic inspection stops"
Ensure 99% availability of the fleet
Supporting a wide array of customers with different IT infrastructures
High-resource performance demands such as sorting customer records and verifying identities
scaling automatically with growing user base
managing everything manually
SAP system outages
Time-consuming and expensive bare-metal server scaling
Strained IT infrastructure during peak periods
Migration of interfaces to legacy backend systems
Oracle-to-Cloud SQL migration with zero downtime
Previous data analytics solution was manually maintained and lacked forecast quality
Managing supply and demand for a wide range of non-food items
Complex and difficult to scale previous database infrastructure
High cardinality of data making it difficult to cross-reference
Difficulty tracking return on ad spend due to large amounts of data
Manual data pulling process from multiple sources
Difficulty in directly building all IT workflows as a startup
Handling the shutdown of Google's IoT Core service
Complex infrastructure of physical servers
Maintaining infrastructure without impacting service availability
incorporating more cohesive cloud tools
scaling resources flexibly
enhancing data analytics
ensuring data security and business continuity
Processing large volumes of data
Maintaining low latencies for mobile advertising
Incorporating 3 TB of data in six months
Detecting difficulties in customer incident management
Managing complexity in data from multiple countries
Ensuring data quality across diverse needs and standards
Transitioning from a capex-heavy legacy model to an opex-based cloud model
Insufficient viewing data for digital programming
Convincing the organization about data security in cloud solutions
Variation in standards across viewers' TV devices
Data silos
Lack of data management across teams
Long query processing times
Zero-downtime migration for business operations
Orchestrating dependencies across servers and tools
Reducing release efforts for new digital products
Performance and reliability issues with third-party provider
Nodes freezing due to high data and transaction load
Monitoring stack issues causing platform downtime
Maintaining multiple IT systems from various acquisitions
Need for data integration across different systems
Quickly identify and quarantine those with symptoms
Need for scalable digital solution
Limited time for design and build
Ensuring the application works for any variable
Predicting resource availability on hosts
"Previous service provider fell short in terms of costs, flexibility, and suitable payment methods"
Initial focus on infrastructure improvement
Existing infrastructure limitations
Need for zero downtime during rebuild
Overscaling due to large database
Technical limitations during peak traffic
Scalability and reliability issues with previous service provider
Partial downtime after configuration changes
Handling traffic spikes during high-profile events
Capacity issues with data center infrastructure
Website lag and crash during traffic surge
Migrating an unchanged estate with a remote workforce
Improving financial access in rural areas
Compliance with regulatory requirements
Secure data exchange with partners
High man-hours and costs for data migration in on-premises environment
Limited capacity of on-premises storage
Manual tracking of campaigns and payments
Optimizing network for query speed
improving data collection practices in marketplaces
creating robust platforms and systems
achieving cost savings while ensuring scalability
Need for rapid scalability with security
Compliance with financial regulations across various regions
Concurrency
Scalability
Stability
High availability requirements
Outages with initial cloud provider
Slow processing of complex queries
Difficulty in adding and merging data quickly
Trust issues with data due to transformations
Complexity of the existing ETL solution
Fragmented data
Inconsistency across terminology
Non-scalable data pipelines
Lack of data catalogue and semantic layer
struggling with existing IT infrastructure
costly IT maintenance
Handling seasonal traffic spikes
Managing scaling across zones
Avoiding downtime
"Capturing, processing, and utilizing data"
Fragmented ecosystems and dated technologies
Communicating objectives to multiple vendors
Security and commercial concerns of retail clients
Technical challenges of scale and complexity
Legacy systems were not flexible and restricted agility
Legacy systems faced conversion issues such as major operating system or database upgrades
Initial challenge to design and build architectures suited for Kubernetes
Handling fluctuating load volumes
Rapid addition of data
Ensuring reliability and scalability
Complex in-house technology
Integration with obsolete technologies of partners
Infrastructure unable to support growth
Minimizing disruption to product development
Need to provide real-time insights from reviews
Handling large datasets quickly
Training data models to improve accuracy
Reconcile CRM data and Google Marketing Platform data stored separately and in different formats
High costs and difficulty scaling with previous cloud provider
"Need to buy capacity upfront for peak demand, leading to idle time"
Poor servicing experiences for vehicle owners
Managing physical servers and associated hardware issues
Long maintenance windows for updates and patches
Data was locked away in archival storage and rarely accessed
Needed an approach to pull insights from data
keeping agility in building new features
meeting cybersecurity requirements
scaling IT processes
creating an easier experience for customers
lack of visibility over pipelines before adopting GKE
Need for affordable cloud solutions
Concerns about migration length and complexity
data computing and updating challenges due to expansion
need to change architecture to handle growth
Homegrown data solution inefficiencies
Manual data preparation and cleaning
Long turnaround times for data requests
Difficulty in testing hypotheses quickly
Securing sensitive data
Managing rapid growth and international expansion
Navigating bureaucracy in the healthcare sector
No capacity from the engineering team to help
Need for fast implementation
Scalability on traditional cloud services
Cost of running deep learning workloads
Overprovisioning of data centers
High operating costs due to excess capacity
Inconsistent data across multiple brands and locations
Mix of on-premises architecture and some cloud services
Adapting from on-premises architecture to the cloud
Ensuring data security during the cloud transition
Lack of scalable analytics and insights
Millions of siloed donor records utilized only for aggregate reporting
Teams of unknown sizes
Partial observability in real-world applications
Handling large volumes of data
Meeting data security demands of large clients
Creating a reliable pipeline across different environments
Locating best data sources and AI algorithms globally
Handling massive geospatial data uploads and processing
High infrastructure costs
Inability to plan for customer usage peaks and troughs
Early implementation challenges
Creation of new services and projects
limitations with on-premises Hadoop clusters
performance and maintenance issues
high cost of data egress
Dated on-premise data warehouse
Rapid data growth and frequent query failures
Potential creation of data silos
"Storing high-velocity, high-volume data streams"
Synchronizing and joining highly unstructured and irregular data
Unexpected high costs from large unused VM
Hungry App Engine resources due to bugs
"Small but cumulative costs from unused static IPs, over-retention of backups, and egress charges"
Deciding between centralized or decentralized development environment
Managing hundreds of data solutions in different business units
Scaling as more customers sign up
Securing the network with Google Cloud tools
Minimizing operating costs
Costly translation service
Several hours delay in product uploads
Scaling difficulty with private cloud
Network and hardware issues in private cloud
Manual server capacity planning
Restricted deployments to avoid disruptions
Limited access to aggregated data
Manual and repetitive tasks bogging down the Operations team
Expanding data warehouse and supporting hardware
Tight six-month deadline
"Migrating 600 virtual machines, 200+TB of data, 80+ MSSQL databases, and 100 physical servers"
Dealing with a variety of OSs and database software
Managing peak business hours with unavoidable queues
Ensuring correct routing of customer calls
Fluctuation in consumption patterns
Handling traffic spikes during live streaming events
Exceeded capacity of domestic cloud service
Little elasticity with co-located service
Insufficient network performance
Expensive compute resources
Lack of visibility into billing
Constantly monitoring resources
Maintaining similar production and testing environments
Maintaining data infrastructure
Ensuring data security
Strict and varying banking regulations
Need for rapid adaptation
Ensuring platform stability across regions
Navigating intense competition
Handling changes in consumption habits and formats
Data stored in discrete locations
Slow video uploads to Singapore
Managing two storage buckets and paying two network charges
Whitelisting IP addresses for telco carrier partners
Migrating systems without creating maintenance windows
Ensuring no service interruptions during migration
"Inability to view and manage expenses, profits, and inventory"
Extended distribution chains adding cost and inefficiency
Uncertainty over product availability and supply
Inflexibility and time consumption of Excel-based data models
High development costs for new or amended reports
Difficulty in updating static Excel reports with real-time data
Ensuring good connectivity in regions with less ideal internet infrastructures
Offering glitchless gaming experiences during high traffic times
Predicting traffic well in advance
Provisioning equipment and ensuring servers are available and functional
Accommodating bursts in demand during discount events
Managing extensive patient data and provider data
Handling significant infrastructure needs for real-time patient interactions
Developing dynamic services
Hiring data scientists in a tight labor market
Managing risk
Structuring and approving loans quickly
Predicting compute and storage needs in advance
Stability issues with previous cloud provider
Difficulty in quickly identifying and moderating content
standardizing data sources
managing increased volume of data post-merger
Needed better answers to questions about viewership
Required flexibility to track video consumption across platforms
Planned downtime with previous providers
Ensuring performance and reliability during migration
Original technology platform was not 100 percent API-driven
Significant manual management required for data wrangling and producing reports
Difficulty in building a single source of truth across channels
"handle complaints, questions, product searches, flight/booking quotes and many other needs"
deal with traffic peaks without compromising service quality
Handling complexity at scale with limited resources
Establishing a multi-sided marketplace in cash-centered markets
Building trust in digital financial services
Technical issues during first month of deployment
Handled by Google Cloud's and CloudMile's support
Cross-region connection disruption
In-game latency issues
Game loading time
In-game errors
Data held in silos across multiple organizations
Need for robust data security and governance
Over-complex tooling causing slow data flow
Difficulties in managing large data exports
Managing dual-cloud compatibility
Handling spikes in traffic
Need to connect incompatible hardware with another cloud provider.
Requirement to solve network problems quickly and efficiently.
Lack of support over performance issues with previous provider
Communication challenges with previous provider during migration
Managing large-scale datasets.
Eliminating bottlenecks in the data pipeline.
Maintaining operations during migration with 405 data pipelines and terabytes of data.
Manually tagging millions of images
Resources spent on labelling instead of conservation work
Slow reporting on endangered species
Processing power and bandwidth limitations of other services
School leaders wanted a version of the solution for themselves
managing and gaining detailed insights into operational data
tracking and analyzing flight performance
managing resources like aircraft and crew
High cost of ownership with previous CRM tools
Fragmented CRM landscape
Multiple on-premises solutions
Employees losing time switching between applications
Legacy system showing its age and holding the company back
Manual accounting reconciliations
Employees had to pull in an IT resource for monthly and quarterly reports
Data silos and multiple copies
Time and cost of installing hardware
Longer time to market
Lack of cohesive reporting system
Manual or scheduled report generation
Scalability challenges with on-premise applications
Rapidly changing pace of work in schools and academics outpacing available solutions
Cumbersome and laborious data entry and collection
"Managing cash, investments, and risk"
Integration with clients' own software
Tracking and complying with federal regulations
heavily customized system hindering upgrades
transition during global pandemic
Managing highly sensitive information
Urgency in digital transformation due to COVID
Need for a secure and cost-effective log management solution
"Over 1,800 applications running in diverse environments"
Ensuring data availability
Compressed transition and transformation timelines
Complexity and cost of maintaining private cloud
Scaling up private datacenter
"Managing a large number of files, systems, and stakeholders"
Fragmented process involving external stakeholders and various departments
High percentage of duplicate and inaccessible files
cost management in the previous environment
availability of global platform in previous environment
Manual processes causing inefficiencies and lack of visibility
Time-consuming approval processes
Colocation datacenters reaching end of life
Need for multicast networking feature in core application
Requirement for a high I/O operations database
Manual and repetitive tasks in processing dispatch documents
Manual aggregation of data for quality assurance
Manual data upload for retail services
Overtime activity reporting and validation
Thousands of disparate dashboards
Difficulty in finding the most accurate version of data
Need for flexible dashboards to adapt to changing business needs
Lack of industry-wide digital transformation adoption
High contact center call volumes
Ensuring correct understanding and response in the Czech language
Integration was a difficult task when dealing with a range of customers with different data requirements
Developing new apps or specialized integrations was labor-intensive and costly
Database couldn't easily scale naturally
transition from legacy platform
lack of business analytics culture
cybersecurity improvements
Manual data compilation process taking two to three days
Generating reports manually which did not always reflect the most up-to-date information
Handling customer objections effectively
Tailoring responses based on other agents’ past experiences
Limited visibility on competitive benchmarking
Creating a compliant and robust data model
strict regulatory landscape
need for a highly secure system
"Lack of a comprehensive view of processes, customers, and partners"
Multiple systems for customer information
Delayed refresh rates in Power BI dashboards
Strained Power BI capacity due to underestimated user uptake
Maintenance and run cost
Infrastructure cost
Revenue loss due to data processing constraints
Maintaining comfortable temperatures in extreme heat
Efficient operation of chiller plant
precise orchestration of grain movement
strategic allocation of grains considering constraints
adapting to different business scenarios
scaling computing resources efficiently
Difficult to onboard new clients
Lack of visibility into important metrics
Labor-intensive manual process to track cloud costs
Data silos and heterogeneous reporting tools
Limited storage and memory capacity
Existing systems not designed for cloud-scale data
Difficulty in querying data
substantial reliance on manual processes
inadequate data exchange between systems
operational inefficiencies
Legacy ERP system limitations
Updates on on-premises taking up to 24 hours
Need for a thorough understanding of the system's capabilities
Concerns about cloud performance
"Collecting, translating, and sharing information globally"
Ensuring equal access to technologies
Addressing questions like bias
Responding to the work-from-home conditions during COVID-19
Adapting traditional reporting methods to real-time dashboards
Managing large volumes of records in real time
Incorporating ways to manipulate and refactor data
Disjointed CRM system and departmental silos
Highly customized legacy system without a common data model
Separate departments not speaking the same language
Excessive cost of previous hybrid solution
Regulatory compliance and risk management
Time-consuming device updates
Driver update problems on laptops
Tracing untraceable diamonds due to industry segmentation
Creating incentives for both producers and consumers
Need for a unified platform to support volunteer-based workforce across multiple locations
Requirement for cohesive user experience
Potential halt in operations due to incorrect maintenance prediction
Need for constant innovation and vigilance in the energy sector
Fragmented ERP systems
Lack of standardization
Internet latency and connectivity in remote areas
"Providing more detailed insights into customers’ security, compliance, and costs"
Optimizing cloud estate
Mixed technology landscape hindering calendar visibility and collaboration
Time-consuming process of gathering insights before Teams
Cloud not being as cost-effective as initially thought
Ensuring development teams have agility while keeping costs low
Legacy technology and multiple applications
Difficulty in making changes to applications
Complex environment due to patched systems
"Lack of a real-time, joined-up picture of river health"
Data exists in many different pockets with many different organizations in lots of different formats
Managing the growing volume and complexity of data
Ensuring compliance with regulatory mandates
Integration of various technology tools
High expectations on timelines and turnaround
Difficulty building an enterprise data warehouse due to legacy systems
Empowering actuaries with modern data analytics solutions
Balancing data security with accessibility
Manual workflow nightmare
Mountains of paperwork
Drawn-out approvals
mountains of disparate data
need to break free of ever-increasing number of data silos
"sharing information across teams in a secure, role-based environment"
Need for secure collaboration during COVID-19
Decision to expand on-premises or migrate to the cloud
Managing various communication methods
Language barriers with up to 22 languages spoken
Historically relying on time-consuming radio communication systems
High costs of third-party software maintenance
Manual and complex process for managing internships
System failures or forced rollbacks
Reducing complexity and improving stability for SAP environments
Handling vast amounts of unstructured and hard-to-find tender data
Need for more granularity in tender detail extraction
severely damaged infrastructure
limited communications
difficulty in getting a fix on specific needs
Running out of space on the backend
Potential slowdown in analysis
Patchwork of processes and tools reducing collaboration and visibility
Non-standardized management tools and multiple systems for tracking
Manual processes were time-consuming
Difficulty in efficiently tracking data
Multiple systems in the value chain
Employees not having extensive IT backgrounds
"Decentralized, inconsistent, and manual processes"
No visibility into status without engaging IT or another stakeholder
Heavy reliance on tribal knowledge
No automated way to disable accounts when employees left
Manual equipment requests through inconsistent spreadsheets
Need for better control over cloud costs
Low reserved instance (RI) coverage
Complex and large volumes of financial data
Manual labor to process and record results from difficult-to-read PDFs
Lack of analytics to interpret data in SharePoint library
Handling high transaction volumes
Improving data security and privacy
Implementing a robust back-up solution
Dispersed data storage
Compliance and security requirements
Managing extensive multi-petabyte data sets
Reducing manual work in business processes
Price wars in the mining domain
Remote location of East Kalimantan project
Risk of serious problem requiring travel to on-premise server location
Hardware and OS maintenance expiration
Coordinating operational schedules during system migration
Ensuring ship operations were not affected by weather conditions
Struggled with category and space management
Accurately implementing planograms with printed instructions
Existing solutions did not fit the bill
Disparate tools and services not communicating well
Legacy technology requiring VPN access
overwhelmed Revenue Services
managing large amounts of data
Limited resources as a regional bank
Disaster response and maintaining connections between branches
Data exposure risk
"Vast, diverse, heterogeneous, and distributed data assets"
Ensuring CRM data integrity
Reducing manual tasks for sales teams
Lack of transparency and understanding regarding cloud spending
Complexity in managing resources and expenses effectively
Managing expectations for AI capabilities
Ensuring security and governance in AI use
Clunky and non-user-friendly initial video consultation system
Manual rekeying of data
Need for clinician and patient acceptance and ease of use
Outdated ERP system causing purchase inaccuracies
Database synchronization issues leading to mismatches between sales and stock
Overloaded in-house networks causing screen freezing
Need for a suitable HR system and flexible work styles
Legacy IT system required teams to use spreadsheets for lengthy and complicated manual analysis
outdated system
lack of automation
limited financial visibility
supply chain inconsistencies
Ensuring more professors could effectively teach applications of digital technology
Needed to replace legacy systems
Required integration among core systems
Fragmented knowledge across 35 ERP systems
Lack of central repository
Need for considerable expert knowledge to interpret data
Meeting the evolving needs of students and faculty
Scaling and modernizing offerings quickly and sustainably
Opaque network of numerous suppliers
Data dispersed in various lists and formats
Difficulty in monitoring second- and third-tier suppliers
Costly and time-consuming hardware and software upgrades
Managing VMware environment and keeping it current
Transition to tech-based processes during the COVID pandemic
Ensure seamless continuity of learning and teaching
Maintain real-time interactivity between teachers and students
Managing a costly patchwork of on-premises digital systems
Modernizing numerous aging systems and applications
Secondary datacenter approaching end of support
overwhelming workloads on managers
outmoded joint use systems leading to longer project timelines
"laborious processing methods with paper submissions, emails, and spreadsheets"
manual data entry increasing likelihood of errors
Overdue accounts receivables
Urgent need to implement a solution without traditional vendor selection procedures
Need for real-time data and analytics
"Visualizing, slicing, and dicing data"
Investing in industrial IoT
Difficulty accessing Power BI on Apple devices
Time-consuming workarounds for data integration
Fragmented technical estate with dozens of disconnected apps
Manage data access while maintaining security
Selecting business applications that provide flexibility
Ensuring smooth transition of operations and business systems
Compliance with local data privacy regulations
Paradigm shift in accompanying customers throughout their health journey
Economic context and demographic pressure in Italy
Inability to easily share data and best practices
Inefficiencies and redundancies in data utilization
Impact on quality leading to customer equipment returns
Fragmentation of internal systems
Inconsistency of codes
Difficulty in visualizing data with BI tools
Technical issues with setting up private connections
Inefficient billing and invoicing schedules
Increased employee workload and customer inquiries at month-end
Insufficient visibility into property ownership and rentals
Scalability issues with rented physical servers
Inefficiencies in deployment and infrastructure management
Handling billions of transactions
Analyzing networks of customers for money laundering
Previous system had long wait times and wasn't meeting targets
"Old, unstandardized requests slowing data analysis"
Sensitive nature of health data requiring high security
Slow data sharing processes between siloed departments
Technically deficient IT configuration
Supply chain activities being the main concern
Avoid a simple lift and shift
Address pain points and modernize architecture
complex timing between order and delivery of parts
monitoring real-time stocks
streamlining order management
Slow and limiting on-premises hypervisors with failover clustering
Potential data downtime causing major liabilities and loss of trust
Manual and complex backup processes
"Overwhelmed IT departments with major, business-critical projects"
Postponing digital initiatives due to unavailability of developers or unjustifiable costs
"Fragmented project data across APIs, data lakes, network storage, and databases"
Tedious and time-consuming data consolidation
Using various software solutions that didn't work together seamlessly
Time wasted switching between apps
Legacy system not meeting data storage needs
Managing and standardizing green energy projects’ bidding process
Lack of centralized system for managing offers and storing data
Transitioning from legacy systems
SMEs' lack of IT expertise and human resources to promote DX
Lack of clarity in business design for the project
Needed to deliver extraordinary information to audiences at lightning speed
Wanted to use massive amounts of data to give fans deeper and more engaging real-time insights
Transforming unstructured receipt images into actionable data
Initial choice of SQL-based platform didn't meet performance needs
Running IT ecosystem on on-prem legacy systems
Siloed applications making orchestration difficult
Subjectivity and non-response issues in traditional surveys
Ensuring privacy and anonymity of data
Time-consuming manual data compilation
Broken homegrown financial reporting system
Handling large amounts of data across software systems
Maintaining personalized contact with clients at any stage of the customer journey
Collecting and analyzing customer feedback efficiently
Outages impacting millions of people
"Maintaining the existing hypervisor, VMware"
inefficient manual processes
massive turnover and pressure in recruitment
complex manual processes in finance
"Ensuring reliability for 17,000 businesses in more than 100 countries"
Need for quick support and access to knowledge
generation of data silos
difficulty in gathering data to existing ERP or core system
loss of interest in data due to insufficient data update and quality
Complex and time-consuming management decisions
Dependence on legacy systems
Legacy on-premises equipment couldn't handle the required number of concurrent users
Security concerns
Difficulty in getting an accurate overview due to rapid growth
Knowledge sharing was time-consuming
Different CRM and ERP systems across acquisitions
Teams hesitant about change
"Managing time, scope, and labor costs"
Ensuring regulatory compliance
Lack of global standard process for using or sharing data
Manual financial processes resulting in inaccuracies
Managing a wide portfolio of data processing platforms
Lack of a single point for collecting data or self-service analytics
Need for unified KPIs and standardized reports
No specific challenges mentioned
keeping entire inventory in stock across catalog
Ensuring maximum scalability for global reach
Maintaining best-in-class data security and privacy
Integration with customers’ existing infrastructure
monitoring and maintaining the health of dunes
connecting IoT sensor data with SAP system
water shortages and increased salinity
High hardware costs
Increasing data volumes
Managing complexity with multiple systems
Dependence on IT resources
"Overloading of systems with 3,000 daily users"
Overwhelming amount of data to sift through
Performing menial tasks manually
limited business performance insights scattered across three BI platforms
inconsistent KPIs methodologies
no on-the-go access to analytics or insights for traveling executives
Significant delays due to the requisition of information from specific departments
Data silos kept departmental information separate
Limited workforce during the COVID-19 pandemic
Shifting to remote work in a single weekend
Ensuring attorneys had access to case files from home
Protecting staff and clients from Covid-19
Managing a lot of data that changes by the hour
Operating across different platforms and retailers
highly dynamic automotive sector
rapidly changing customer needs
generate insight-driven data analytics
maintain the health of the properties
monitor energy consumption
measure thermal efficiency
maintain air quality
reduce carbon footprint
detect damp & mould
disconnected systems
need for customer engagement platform
replacing siloed SAS solutions
Consolidation of data across multiple systems
Establishing strict data and BI governance
Legacy system incompatible with growth ambitions
Thousands of region-specific customizations
Limited business visibility
No operations support between regions
Adjusting system for new operations or markets
disparate sources and manual entry errors
process inefficiencies leading to inconsistent data interpretation
initial resistance to new systems
Scaling on-premises data warehouse
Managing complex systems for patient care and payment disbursement
Significant time spent on data management
Scarcity of ESG data for privately-owned companies
Manually sorting through enormous amounts of data
Facilitating preliminary examinations remotely
Ensuring HIPAA compliance
User-friendliness and efficiency of the telehealth system
Lack of flexibility
Lack of visibility into inventory
Outdated demand planning and forecasting
Manual processes for demand planning and purchase orders
Managing credit limits and approvals for special orders
Connectivity issues due to infrastructure in the Middle East
Fragmented application landscapes
"Effort, cost, and value mismatch in maintaining on-premises datacenters"
"Size, scale, and complexity of SAP in global datacenters"
Migrating 350 systems in 18 months
Need to modify and adapt global core technology systems for local markets
Transitioning from wholesale to direct-to-consumer sales
Managing multiple e-commerce platforms
Increasing single-shipment orders significantly
Infrastructure management
High availability and disaster recovery requirements
Security challenges
Multi-cloud compatibility
Operating costs
Outdated and slow loyalty program targeting households
Need for advanced analytics platforms
Ensuring privacy compliance
Integrating siloed data sources
shortage of foster families
system inefficiency
complex and fragmented state systems
low digital literacy among staff
need to deploy new hardware
adaptation to new tools
Manually collecting security status data
IT indicators spread across multiple platforms
Time-consuming email reminders
High student attrition rate
Unique set of circumstances and challenges faced by students
Bias in data interpretation
Lack of measurement and regulation systems in distribution networks
Manual reading of consumption levels
Existing business planning process became an exercise in futility
Traditional business plans quickly became outdated and less relevant
Writing good goals and key results
Outdated and disparate systems
Costly and inefficient infrastructure with managed service datacenter
Complex migration of ERP products and core business processes
Paper-based production order system
No cost information or timestamps
Need to maintain quality and customer satisfaction
Minimal disruption to workflow during transition
Need to manage sensitive data access
Requirement for efficient and intuitive query language
"Software management of 1,300 student devices"
150 different database systems with insufficient quality
No consolidated reporting
Limited SAP business warehouse performance
High usage of Excel and Access-based tools
Lack of self-service culture
Need for integration of data from various sources
Highly complex and heterogenous digital infrastructure
Visibility and management challenges
Applications tied to specific operating systems or database platforms
Unwieldy and inconsistent websites and email/form-based processes
Suboptimal digital experience
Difficulty in delivering system-wide changes
Legacy data spread across multiple systems
"Manual, paper-based processes"
Legacy systems limiting ability to scale
Need for greater operational efficiency and resilience
Difficulty scaling and meeting timelines with outdated hardware
Expiring on-premises datacenter
Rapidly growing amount of data
Complex environment
heavy asset costs in traditional freight companies
resistance to change due to market uncertainties
need for security and reliability in IT systems
flexibility at the level of underlying resources
Complexity of content translation and localization projects
Coordination across various cross-functional teams and time zones
Demanding timelines and internal challenges
"Handling new opportunities, project statuses, financial data, and other business functions"
Maintaining on-premises data used for reporting
Choosing technology that could integrate with disparate enterprise solutions
Infrastructure susceptibility in remote areas
Mobile phone coverage and broadband access challenges
Designing data security parameters
Quick adaptation to remote work due to COVID-19
Balancing the volume of changes with customer preparation time
Manually collecting security status data from multiple systems
Time-consuming reminders to update systems
Interpreting data from five or six systems
Different data governance systems in each hospital
Every hospital having its own data language and governance
siloed inventory for e-commerce
high inventory holding costs
long lead time to fulfill orders
Need for a data platform with instant access capability
tracking patients across multiple calls and teams
lack of centralized view into patient data
Highly labor-intensive video review process
Lack of prior experience with AI and Azure
Steep learning curve for using AI
Fragmented fan base
Managing a sophisticated infrastructure
"Manual, paper-based audit processes"
Time-consuming back-and-forth for task management
Disparate data homes across the organization
Difficulty keeping up with customer and retail demands in real time
Strain and costs of on-premises systems
relying on distributed systems and data centers
ensuring security with previous setup
Time-consuming paper-based process
Lengthy delay from submission to approval
Difficulty in accessing and mining nomination data
Handling large volumes of data (up to 5 billion data points yearly)
Disparate quality levels of data
Decentralization of data producers
Reliance on a legacy on-premises solution
Managing growing volume of data
high volume of data requiring high-performance infrastructure
complications with existing data system not adapting to all needs
On-premises solution lacked scalability and elasticity
Recurrent performance issues
Inability to support new types of data like video and images
System didn't support real-time analytics
Data silos across multiple CRMs
Different CRM platforms for different departments
Standardizing business processes
Initial migration to O365
Complicated stress test processes
Data preparation and computing efforts
Out of date and unsupported legacy financial system
Manual processes and data integrity issues
Difficulty integrating with other systems
Excessive staff time required
Heavy reliance on documents and spreadsheets
Revisiting priorities due to COVID-19
Remote work necessitating document signing automation
Digital complexity and inefficient allocation of resources
"Governance, certifications, and data sovereignty"
Decentralized support and different security controls and frameworks
Lengthy and expensive process to make data available in new EPR platform
Impractical timeline for data migration window
High cost of data feed from another supplier
Combining data from different systems
Creating an automated process for finding and sharing samples
Ensuring privacy and security in data sharing
"Connecting people, systems, and locations seamlessly"
Maintaining employee engagement and connectivity
Integration of multiple business systems
Need for instant access to data
Support analytical users
Facilitate information flow through the organization
Address all current and potential future information needs
"Processing and analyzing over 150,000 records per month"
Supporting clients in the most effective way possible
Measuring and insuring against strategic human resources risk
Traditional assessment methods being time-consuming and incomplete
Inefficiency of each plant creating its own IT solution
Manual and time-consuming data entry processes
Integration of older machines into new digital landscape
Cumbersome approach with outsourced IT and third-party apps
Siloed data and inconsistencies in reporting
Lack of easy access to data for field representatives
Massive burden on sales staff and managers due to manual data retrieval
High cost and time if data visualization development was outsourced to a third party
Need for a scalable customer portal
Reliable data collection
Limited capacity and expertise for simultaneous implementation
Manual data entry and merging spreadsheets
Searching for information across different platforms
Siloed data and applications
"Building singular, non-reusable data solutions"
Finding a way to measure student performance
Managing different learning styles and needs online
"Demand for more computing power, higher speeds, and greater data storage"
Inefficiency of existing system (BioNet built on SharePoint 2010)
"Difficulty in tracking financials, resource utilization, status, and KPIs"
Siloed solutions resulting in duplicate work and less-than-ideal collaboration
Risk of keeping business-critical data in an outdated system
Functionally divided and polarized working ways
Data inconsistency across legacy systems
Complexity and scale of projects
"Manual, error-prone data organization"
"Need for centralized, streamlined program and project mapping tool"
Running infrastructure in a colocated datacenter
Manual processes for updates and hardware upgrades
Limited supply of laptops during the COVID-19 pandemic
Time-consuming project setup
Time-consuming and error-prone manual processes with Excel
Negative impact on team spirit due to relocation
Managing desk assignments efficiently
Transitioning from large data lakes to a modular architecture
Enterprise-wide data governance and security
Educating non-technical staff on the new architecture
Establishing two companies and their infrastructure in parallel
Building a digital infrastructure in just nine months
Avoiding high costs and time-consuming processes of traditional technology
Need for highly skilled staff
Transition to digital tools
Inefficient and difficult to scale architecture
Multiple steps in data processing
Need for reusable assets and reduced replication
Consolidating nearly 10 different visualization tools and two cloud platforms
Elimination of traveling due to the coronavirus pandemic
fragmented architecture
need for training during lockdowns
Communicating with teams and colleagues spread around the globe
Establishing communication in remote areas
Pressure from competing products and evolving marketing channels
Difficulty maintaining competitiveness with traditional local deployment models
"Scattered, siloed, and difficult-to-access data"
Data quality issues
Complex data management from various sources
Volume and diversity of data
Complexity of exchanging and organizing data
Lack of efficient means and person in charge of promoting the data platform
Rapid advancements in technology
Data not well used with previous systems
Determining valuable data from substantial amounts
"Collaboration difficulties between regional, TICs, and overseas offices"
Resistance to change from partners and vendors
Overloaded servers and full inboxes
Tedious manual paperwork causing delays
Manual consolidation of data from different supervisors
Time-consuming reporting process
Costly infrastructure maintenance and upgrades
Complexities of running an on-premises datacenter
Limited flexibility and growth
Security concerns
Data silos
Manual data linking
Need for a central platform for data
Maintaining separate systems for chat and voice support
Duplicative work and inflexible on-premises platform
Handling vast and growing data
Ensuring compliance with GxP requirements
Fragmented IT infrastructure
Managing highly sensitive data
Difficulty in centralized data and analytics
Overwhelming requests for dashboards and reports
Merging two parallel Active Directories
Divergent IT security policies and local data storage rules
Data compliance across various countries
manual adjustment of inventory levels
disparate systems for front-end and back-office teams
limited visibility on sales performance
High barrier to application due to multiple screening processes
Administrative burden of 10-15 minutes per applicant
Difficulty in integrating aging infrastructure
Original cloud architecture reached its limit
Self-developed microservices required huge amounts of code
Inefficient manual data backup process
Slow retrieval process
Poor internet infrastructure at racetracks
Significant administrative burden with manual processes
Duplication of data entry
Potential disconnect in document approval process
Data silos and decentralized information.
Manual updating processes creating inefficiencies and opportunities for error.
Current landscape of disparate data sources and systems
Time-consuming data pulling process taking over 12 hours
"Data not processed quickly enough, report access not well-defined"
Time-consuming manual processes
Distributed inputs among several systems before centralization
Lack of flexibility in original on-premises data platform
"Manual, error-prone tasks"
Requiring database administrators for physical upgrades
Understanding how small things can lead to big changes
disparate systems and data structures
security and efficiency challenges with shared folders
untapped potential of data
managing large volumes of structured and semi-structured data
data warehouse governance
Three different case management systems that didn't talk to one another
Manual processing and storage of 70 million paper files
Supply chain issues due to COVID-19
Global inflation
Labor shortages
Limited resources
Dependence on external providers
"Concerns regarding security, availability, compliance, and governance"
Transition from paper-based records to Electronic Health Records
Difficulty accessing information for risk and cost assessments
"Managing increased documents, data, and disparate data silos"
Manual data collection processes for pollution remediation
Numerous stand-alone systems
Time-consuming processes
Lack of real-time data insights
Tools tailored mostly for large restaurant chains
understanding customers' habits better
concerns about older customers' interest in the app
Legacy core systems
Need for real-time data visualization
Insufficient personnel for certain projects
Data quality improvement
"Delivering personalized, consent-aware communications at scale with growing customer data"
"Ensuring transparency, accountability, and security"
Complex and decentralized IT environment
Disruptions caused by COVID-19
Difficulty to stay up to date with previous deployments
"Differing corporate cultures, workflows, and technologies"
Crucial business information was dispersed and hard to access
Centralizing massive amounts of data
Democratizing data from multiple sources
Ensuring data accuracy and eliminating reporting silos
paper-based old communication infrastructure
bureaucratic interactions with regulatory institutions
Current backend couldn't support ambitious data goals
Disparate data sources hindering innovation
manual personnel hiring process
need for urgent implementation of solutions
Access to data is often limited to leadership roles
Data quality issues due to fragmented systems
High workload for manual audits
Transitioning from outdated IT to cloud
Maintaining and repairing machines worldwide
Technology risks associated with healthcare solutions
Nurse and physician burnout
Handling enormous volumes
Ensuring patient privacy and cybersecurity
Legacy issues due to rapid growth and acquisitions
Existing business system was creaking at the seams
Difficulty collaborating internally and with external parties
Lack of visibility and transparency due to unstructured data
Using multiple tools and files for project management
Need for critical mobility and availability-dependent services during the pandemic
Requirement for higher security measures
Old ERP system unable to handle new requirements
Complexity in subscription co-terming
Manual effort in renewing subscriptions
Incomplete data due to pen and paper methods
Lack of standard software catering to specific needs
Analyzing data from seven business units and 100+ sources
Manual preparation of daily MIS using Excel
Varied KPI update cadences causing mismatched reports
Inconsistent and tedious queries for business figures
Decentralized BI and reporting solutions
Difficulty in obtaining valid cost estimates
"Managing and securing 90,000 student devices"
Ensuring data safety and regulatory compliance
Emotional stressors among students due to COVID-19
Scale platform globally
Provide security and controls
Siloed and relatively difficult to access data
Proving technology in a live production environment
Managing numerous processes and vast amounts of data
Pivot to remote work during COVID-19
Organizing influx of donations during the pandemic
Scheduling and coordinating volunteer efforts
On-site information sharing
Checking construction progress
Managing disagreements regarding communication
Systematization of DX across the entire organization
Establishing full-scale environment within Mitsui’s system
Additional tasks due to COVID-19 pandemic
Limited IT resources
High volume of meetings
Need for simplifying operations and releasing capacity
Transforming meeting culture
Converting analytics into simplified visual insights for stakeholders without a math and statistics background
Generation of information silos
Cost and lead time for data linkage
Operational issues with complex systems
Compliance with international data protection regulations
Disjointed communications overwhelming supporters
Fragmented supporter experiences
Previous analytics software had limited capabilities and lacked user dashboards with meaningful visuals
Lack of a centralized data warehouse
Manual data-entry processes were error-prone
Limited usability of data for accurate reporting
Inability to create data visualizations and surface actionable insights
OLAP cube couldn't store historical data
"Human-made issues such as civil wars, illegal logging, and poaching"
Encroachment and exploitation of resources
ERP system couldn't support the expansion
Multiple different reporting tools
Data being on-premises and not easily accessible
Gaining access to the right data at the right time
Managing and migrating legacy data systems
Complicated and slow process of adding memory capacity and compute for physical servers
Inflated procurement timeline due to hardware restrictions
Supply chain issues related to COVID-19
Difficulty managing supply chain and dependencies
Double effort due to different software systems
Manual workflow leading to human errors
System downtime preventing sales
"Inefficient paper forms, emails, phone calls"
National and local processes did not scale with volume
Declining student satisfaction
Lack of basic telemetry
Cost constraints in higher education
Quick transition to online operations due to COVID-19
Initial resistance from public safety officials
Skepticism towards cloud-based systems
costly on-premises datacenter upgrades
minimizing impact on student outcomes
Legacy systems were disconnected and siloed
Different ERP systems in Electrical Appliances Division and Gilman
On-premises CRM didn’t allow any customization or automation
Manual efforts on tasks including upgrades
"Missing key features such as drop shipments, partial receiving, advance payments, and bank account reconciliation"
Decreasing public transport usage
Concerns over backup/restore processes
Visibility of security configurations
Length of time taken for tickets to be enacted
Oversee and manage customers’ activities and requests in a single platform
Integrate with existing solutions for accessibility and usability
cyberthreats
divestitures
acquisitions
Transition to cloud
Automating payment and order processing
managing incoming payments manually
processing orders manually
Disjointed project portfolio management system
Manual processes and multiple data sources
Insufficient visibility of project data
Efficiency impacted by old systems
Managing a large fleet of IoT devices
Real-time processing of data
high value of McLaren manufacturing leading to costly outages
need for greater versatility and agility in data handling
staff furlough during the migration
Convincing some teachers to adopt digital solutions
Managing interactions with third parties in a classroom setting
Loose and disconnected ICT infrastructure
Communication difficulties during the COVID-19 pandemic
Ensuring CSV compliance in the pharmaceutical industry
Concerns about the security and performance of the cloud
Difficulty learning about career opportunities in other areas across Disney
Understanding what everyone does in a large organization
Subjecting each part produced to quality assurance
Handling vast batch sizes and ensuring quality in each part
Cumbersome financial management
Disjointed order approval process
Manual processes and inefficiencies
Complex and hard-to-understand triage guidelines during the pandemic
Data silos
Intermediation in accessing data
Lack of data-driven culture
"slow, reactive, and centralized application development"
laborious and manual salary negotiation process
time-consuming and error-prone Excel spreadsheet usage
complex calculations on data from multiple sources
data housed in multiple siloed applications
difficulty in understanding key metrics
Multiple and highly manual methods in use across the enterprise
Sales teams using different methodologies with mixed results
managing fluctuating student numbers and IT costs
tedious and manual financial processes
lack of centralized data
High administration burden on HSEQ team
Low usability of previous Oracle-based HSEQ system
Slow data refresh times and failure
System unable to handle high volumes of data
Legacy systems causing additional effort for sales teams
Need for better integration and support for disaster recovery and IoT products
Existing landscapes not feasible for current needs
Navigating global supply chain challenges
Legacy infrastructure limitations
Spending numerous hours manually consolidating data
Questioning the validity of self-reported data
Need for a single source of truth
Managing centralized loan origination and management system
Handling complex processes
Generating large PDF documents
Integration with external fintech partners
Initial processes and documentation stretched to their limits
Managing increasing complexity of HR tasks
Maintaining system with growing employee count
Data residing in siloes across multiple systems
Multiple handoffs and manual interventions to bring relevant intelligence to stakeholders
Inefficient process of managing offline and online data streams
Pandemic-related funding challenges
Increased need for social services
Too many apps and flows running in the default environment
"Ensuring security, privacy, and data-loss prevention (DLP) policies"
Previous provider not meeting high level of speech recognition
Callers critical of bots due to bad past experiences
Ensuring no data loss during migration
Information not found
No automated data analysis
High cost pressure
Heterogeneous ways of acquiring and using data
Maintaining timely availability of business data
Significant delay due to in-memory compute of the on-premise EDW solution
Skills gap in cloud technology
Limited initial physical space for training
"manual, paper-based processes"
time-consuming image analysis using local servers
emergency nature of the science requiring rapid response
Competently recording claims around the clock
Initial voice control programming limitations
Requirement to expand functionality beyond storm damage
managing changes and resource planning
manual revisions between Excel and Access
expensive errors when timelines were not accurate
modernization without constant rebuilds
secure data integration across different network environments
Needed more scalability and less complexity
Couldn’t get things working during back-end infrastructure migration
Need for secure and effective remote collaboration
Eliminating reliance on physical meetings during the pandemic
Manual forecasting was time-consuming and inefficient
Inability to perform periodic analysis without wasting time
Scattered constituent data
Unresponsive website interface
Low retention and engagement
High volume of manual coordination and paperwork
Frequent schedule and patient status changes
Scaling user base and channel limits in Teams
Complex systems and user interfaces (UI)
Need for a proper governance model
Decentralized and siloed data
brand unfamiliarity in certain regions
limited budgets in undeveloped regions
Manual and paper-intensive processes
Specialized underwriter expertise
Legacy system upgrade eliminating critical functionality
Lag in adoption of digital solutions in the manufacturing industry
Ensuring SAP stayed up and running during cloud migration
End-of-life infrastructure under SAP environment
Ensuring a healthy culture for remote work
Employees feeling stressed and working longer hours
Maintaining work-life balance
Highly variable power generation from renewable energy sources
Cultural shift from static documents to dynamic content
Engaging with officers across the force
copious amounts of data with no unified way to manage and utilize
need for a solid technological structure to support rapid development
Slow insights and report generation
Labor-intensive report updates
Cockpit system unable to handle large data loads
Time-consuming and costly in-house development
Disjointed CRM systems
Requirement for utmost reliability and flexibility from tools
Need for secure and seamless employee access
Requirement to swiftly build and provide solutions for changing business requirements
Manual data collection and reporting being too slow and costly
Need for the right technology to gather data at the source
Handling multiple customizations
Managing warehouses and external transportation companies
Inefficient data entry and compilation process
Time-consuming quality control process
Manual processes using email and spreadsheets
Managing security and data privacy for a growing number of apps
"Providing uninterrupted, high-quality, fast logistic services"
Creating a coordinated and secure service infrastructure
Ensuring efficient internal communication and connectivity during pandemic periods
Development and support requests growing beyond team capacity
Incorrect deployment of premium connectors
Need to pivot to remote instruction due to COVID-19
Problems with a disjointed IT environment
Working with spreadsheets and specialized solutions
Lack of standardization across different distribution centers and warehouses
Struggle with consolidating data for labor planning and forecasting
Difficult social distancing during driver check-in
"Manual, paper-based claims processing"
Pandemic impacted supply chain and export position
Brexit legislation affected the business
Initially had three siloed systems across regions
Needed to integrate CRM with existing systems
Manual processes and tactical systems
Nervousness about remote training and support during COVID-19
Adjusting to remote rollout and business change support
Manual adjustments of prosthetic sockets being time-consuming and error-prone
Ensuring the perfect fit to avoid long-term discomfort and repeated clinic visits
Balancing the need for independence with the use of managed services
Avoiding vendor lock-in
network connectivity restrictions and client software setup issues
performance issues with legacy on-premise systems
managing internal dependencies during data integration
reduced team capacity due to COVID-19
"issues specific to the food and beverage industry regarding supply chain management, logistics, production, financials, and sales"
Too much data in monthly reports
Data often arrived too late
Data was not targeted enough
Coping with increasing data volumes
Monitoring each individual device closely
Integrating external data like weather information
Ensuring data encryption and compliance with data handling regulations
Clients' tendency to prefer direct interaction with lawyers
Employees slower to try new digital processes
Repeatedly building similar types of solutions
Complex and labor-intensive solution development
Managing compliance standards for the integration of multiple tools
Inability to rapidly use a broader range of deep analytics
Inefficient phone system with multiple platforms
High costs of maintaining the old phone system
Concerns about employee adaptation to losing desk phones
Lacked formal project portfolio management solution after branching off from BBA Aviation
Planview software lacked customization options and wasn't built for product line transitions
Needed compatibility with Ontic’s current systems and support for better transitioning of legacy product lines
Need for business unit personnel to acquire data analysis skills
Handling and processing over 1.7 billion transactions
Global consistency vs. decentralized agility
Reliability of solutions due to telecommunications quality
"Need for real-time, accurate information across the supply chain"
scattered collaboration landscape
ensuring everyone could work from home
connecting team members in virtual meetings
Handling an influx of cancellations due to COVID-19
Maintaining customer trust
Overwhelmed customer service teams
Managing Kubernetes or cluster downtime
Need for quick migration
Lack of agility and scalability in existing infrastructure
Managing large IT infrastructure centered on SAP solutions
Acceleration of digitalization
Connectivity issues and infrastructure challenges in Indonesia
Legacy email system
siloed solutions from multiple vendors
achieving digital transformation outcomes
supporting company growth
vast amounts of data needing to be migrated
Time-consuming and error-prone traditional appraisals
Difficult tracking and retrieval of documents
Inefficiency of spreadsheets for smaller offices
Database migration and integrity
Efficiency and security of existing infrastructure
Significant effort in on-premises maintenance tasks
Manual reporting through various Microsoft Excel spreadsheets and different reporting tools
Loss of time handling hundreds of calls and emails requesting information
Requirement for a centralized collection of customer information
Implementation without physical meetings
Shutting down old on-premises infrastructure
Migrating SAP systems without business interruptions
Handling project remotely due to COVID-19
Maintaining IT security and data protection
Change management and organizational adjustments
Uncertainty about who was developing apps and their locations
Unused test apps and flows
Need to audit connections and implement data loss prevention policies
High cost of digitizing numerous administrative procedures
Complexity of different administrative procedures
Slow development speed of traditional waterfall model
Lack of existing CRM system
Disparate communication channels
Low sales conversion rate
Daily duties spread out between several disconnected platforms
Difficulty in gaining actionable insights
Strain on day-to-day business due to remote operations
Difficulty in aligning priorities and objectives across multiple time zones
Varying effectiveness of meetings
Reverse logistics operations
Finding an ERP tool that made sense for their unique needs
Highly customized but limiting old system
Numerous data and analytics teams working across disparate environments
Requirement for full governance and compliance without limiting analytics teams
Migrating from traditional to digital construction management
Finding a suitable site for testing drones
Time lag in real-time data analysis
Dependency on IT resources for data extraction and modeling
Limited scope of cross-functional study due to tedious data analysis process
Manual processes insufficiency
Losing customer data due to sales turnaround
Time-consuming data collection with Excel
Innovative business solutions needed for the pandemic
Managing and securing patient data
Manual data collection and reporting being too slow and costly
Integrating non-Microsoft technologies
"Downtime due to maintenance, sanitation, and product changes"
Highly competitive retail market
Upskilling engineers in cloud technology and machine learning
"Issues with devices, connectivity, and access"
Creating new lesson plans for remote learning
time-consuming search for relevant information
decentralized data storage
limited search functionality in previous BI tool
complex licensing scheme
Connecting to the latest and greatest solutions from partners
"Manual spreadsheets leading to missed opportunities, stockouts, and overproduction/underproduction"
"In-house, custom-coded, desktop-dependent system requiring additional workflows and nightly downtime"
Old IT solution became a bottleneck
Manual import of donations
Lack of proper relationship management with supporters
data security and privacy compliance
projecting hardware and software needs
maintaining trust of members
need for quick and seamless solution during the pandemic
ensuring stable connection for large scale events
Dependencies on legacy apps
Limitations of using Microsoft SharePoint as the data source
Integration with third-party tools like SAP
Generating granular and segmented audience data
Maximizing user experience with optimal response times
High student dropout rate
Identifying risk factors for dropouts
Existing MySQL database struggled to handle massive amounts of incoming data
Need to manage unexpected data surges
Long processing times in former environment
Need for sophisticated systems and platforms to respond to consumer needs
Issues related to data analysis such as insufficient app-enabled members and need for consolidated analysis
Multiple customer rights and accesses requiring different communications
Complex and time-consuming previous BI tool
Delayed dashboard refreshes
Accessibility issues with dashboards
Functionality limitations
Increased security concerns
Data comes from multiple sources and is consolidated through Excel files
Difficulty tracking data sources over time
Consolidation process involving multiple preparers
Limited time for analytics after data consolidation
Decentralized records and data
Need for data integration
Manually sifting through Excel sheets and PDFs for data
Employees less familiar with reports had trouble locating information
Massive amounts of data from multiple sources
Unstable and large files requiring long processing times
Reducing stress for medical staff and patients
Creating a secure and compliant data platform
Different systems for customers and employees made information sharing difficult.
Separate databases for nearly every site limited data sharing and updates.
"Security challenges and issues unifying and reporting from siloed, on-premises applications."
Aging nuclear plants
High maintenance costs
Siloed technology strategies
"More than 3,000 applications and 2,300 databases"
Lack of ability to quickly turn data into insights
Time-consuming data analysis and archiving
Inability to rapidly follow up or fix problems
Real-time out-of-stock monitoring
updating legacy infrastructure
"managing complex billings, accounts payable, and receivable"
accommodating rapid and anticipated business growth
Organizing vaccine distribution during COVID-19
Managing multiple employer information systems
Coordinating responses with a surge in vaccine call center activity
Slow generation of daily analytic country sales reports
Big capital investments required for on-premises server resources
Inability to update reports with fresh numbers in real-time
Knowledge sharing across the company
Scalability of infrastructure
Data security requirements
Initial system only accelerated post-incident maintenance
Additional work and problems with sensor-based monitoring
Frequent factory visits to check sensors
Excessive data storage on site
Information not found
Scaling IT setup while maintaining security
Fragmented email servers and inconsistent identity protection
Manual and inefficient processes during COVID-19
Managing data security and governance
Inability to quantify the probability of churning for any individual customer
Need for more computing capacity and machine learning power
"Complex, time-consuming, manually written Python code slowing down model creation and reporting"
Crowded market with many similar solutions
Bespoke platforms are expensive
Lack of flexibility in existing solutions
Difficulty in building customized options
Accessibility to data is lacking
Concerns about security and data ownership
complex flow of processing and sorting end-of-life assets
lack of native front-end UI for license plate reservations in Dynamics 365 when processing ITAD orders
Data processing speed with on-premises hardware
Complex operational requirements of healthcare
Initial tool lacked necessary connectors
Disparate training systems
Manual intervention on every level
Inconsistent tracking and reporting methods
"Errors such as misspellings, abbreviations, and incomplete responses"
Eliminated in-person collaboration due to COVID-19.
Measuring the impact of digital transformation initiatives on the employee experience.
Securing distributed devices and virtual workspaces.
"Manual, paper-based processes"
"Costly, custom-coded websites"
Need for quick development during COVID-19
High competition in the low-cost air travel market
Initial low traffic on the e-commerce site
Fraudulent purchases
Previous cloud service did not meet HIPAA-level security standards
Protecting secure data on-premises was too expensive
Solving compliance and identity management issues
Data dispersed across different systems
Inconsistent system access
Confusion about project information storage and request methods
Inefficient information-sharing environment
Resistance to new tools
Complying with different regulatory systems
Dispersed data and processes across multiple systems
Days-long reporting challenges
Disconnected colleagues
Time-consuming manual tasks
Communication during COVID-19 lockdown
Securely exposing internal data to customers
Protecting sensitive data and complying with security standards
High volume of COVID-related questions
Difficulty navigating OPH’s website
Need for a rapid deployment of solutions
Siloed data management and storage
Limited IT framework reducing agility
Low employee productivity due to time spent on low-value tasks
Restrictive laws on data storage
High-performance infrastructure reaching capacity limits
Increasing data volumes and regulatory requirements
Need for business agility
disparities existed across K–12 system
uneven access to digital services
"multiple, disparate data sets across many different locations"
lack of documentation or a data dictionary
need to change mindsets about development work
Actionable data was historically siloed and top-down
Information wasn't available in real-time
Limited distribution and access to data
Concerns about redundancy of roles
Disparate data sources
Inconsistent data formats
Erratic data delivery
Different people received different data
global pandemic and rapidly shifting circumstances
Power BI was an unknown tool for Yapı Kredi teams
Validating the tool against imagery and video
Lack of effective centralized storage
No good email system
Risk of data loss
"Cash leakage, anomalies, and deposit delays"
"Disconnected processes for collection, deposit, and reconciliation"
Disparate environment of multiple Microsoft volume licensing agreements
Managing multiple iterations of Microsoft products
Increased call volumes and longer response times
Need for 24/7/365 operations support
Security concerns during COVID-19 transition
Manual editing and updating of Microsoft Excel files
Prone to human error
Need for data integration and automation
Quick transition to remote work
Handling bandwidth challenges
Driving adoption of new tools
Manual recording of calls sometimes incomplete
Need for high interoperability between systems
Month-end closing consumed a lot of resources
Generating account cards and trial balances took from 40 minutes to more than one hour per report
Merging operations of four bottling businesses
Introducing new systems without disrupting daily tasks
Transitioning users from legacy systems to new ones
Providing consistency across several different business units
Completing Dynamics 365 migrations while working remotely during COVID-19
Ensuring consultants could work on the move
Seamless transition to home working environment during COVID-19
not sure where to begin with modernization
required an end-to-end business solution
Rise in computer hacking incidents
Needed holistic and simple-to-use security solutions
Previous infrastructure's complexity and high costs
Data processing delays
Historical debate between IT and OT departments
"Different schedules, timescales, and objectives between IT and OT"
Ensuring OT security while maintaining operational availability
Operating in a heavily regulated industry
Need for compliance with regulations and certifications
Building conversational AI-powered virtual assistants quickly
Handling complex and critical resident requests during COVID-19
Managing multichannel retail challenges
Updating long-term strategy with cloud adoption plans
Disparate IT infrastructure
Increased pressure on data warehouse post-merger
Need for cloud-based modern data warehouse
Managing massive amounts of data
Complexity of consolidating legacy systems
Previous mixed reality systems were too complex
Training bottlenecks with traditional one-on-one methods
Disparate and complex systems for infrequent users
Managing office space during COVID-19 with reduced capacity
Tracking activity and offering live reporting for testing and vaccination
Initial uncertainty about cloud adoption
Pandemic requiring quick adaptation
Struggle to equalize time spent working versus personal life
Complex workflows
High-volume and multichannel transactions
Needed remote working capabilities during COVID-19
Magnitude of data volume and processing power needed not feasible with on-premises infrastructure
Needed more compute power at certain times
Balancing processing speed and cost with provisioned compute
Manual adjustment and optimization of queries
Traditional email and paper-based processes
Need to quickly implement remote collaboration
Disjointed on-premises systems
Complex regulations and frequent policy amendments
Manual and time-consuming reporting processes
handling incredibly sensitive data
Time spent on generating performance reports
Busy in-house development teams with backlog of custom projects
Lack of cost assignment to teams
Disconnected operations and engineering teams
Accelerating the digitization project
Ensuring the viability of a large-scale digital project that could be deployed remotely
Training judicial servants without prior knowledge of Office 365
Managing a huge patient data store
Tailoring information formats for specialty dental areas and brands
Traditional paper-based processes were upended by COVID-19
Needed to find new operational methods with businesses and schools closed
Illegible handwritten IPPs
Double entry of data
Lost or delayed paper forms
Low-quality audio recordings
Apps developed with no Data Loss Prevention (DLP) policies
Use of premium connectors leading to potential cost overruns
High-usage apps with only one owner
Siloed data
Manual and slow reporting process
Meeting performance SLAs
data resided in geographically dispersed silos
legacy systems couldn’t link disparate data types
Difficulty managing traditional relational databases and middleware on-premises
"Labor-intensive old-school, point-to-point solutions"
"Expensive engineering hours for patching, maintenance, and infrastructure work"
Scaling and upgrading on-premises were big asks
Previously siloed data processes
Laborious data collection process taking two weeks
Maintaining service level agreements across multiple time zones
Increased headcount and budget for vendor support
Bridging the gap between different platforms and tools
Obsolete business management system with no adaptability
Need for real-time information processing
Significant technical complexity due to customizations
Maintaining technology infrastructure
Broadening on-demand IT resources
Guaranteeing a secure and scalable platform
Ensuring information traceability
Identifying and resolving issues related to energy consumption
Managing assets based on data collected from various systems
User Acceptance Testing (UAT) for system updates
Handling multiple language versions for regional offices
Legacy task management system was slow and manual
No easy way to enter and update tasks
Limited customization capabilities
Manual process for facility incident management
Service agents spending too much time on routine inquiries
Increased call volume due to COVID-19
Staffing challenges due to workplace restrictions
Manual and inconsistent sourcing process
Email-centric and layered return approvals process
Need for timely solutions with tight deadlines
Disparate legacy systems
Integration of various systems
"Reactive process with imprecise, uncoordinated, and delayed information"
Costly third-party data with limited useful lifetime
"Time-consuming preparation, consolidation, and verification of data"
Lack of overall visibility
Paper-based processes
Time-consuming and inefficient onboarding
Manual data collection and reporting
Doctors not always signed into Teams
Unsustainable COVID-19 test scheduling process
Balancing educational needs with health and safety
Providing resources for remote education
Planning for safe reopening
Productivity-dampening issues
Growing number of projects and dearth of resources
Difficulty finding relevant documents
"Complex data management due to new products, service lines, and business processes"
Need to build a customer data platform quickly during the COVID-19 pandemic
Manual safety and compliance management
Inconsistent standards and requirements across sites
Lengthy onboarding times
Work delays
Outdated compliance documentation
Lack of standardization
Generating sales quotes from email inquiries
Managing customer data in disparate systems
Understanding complex customer needs
Disparate systems creating silos of information
IT seen as a 'back office' function
Using up to 40 different applications for daily tasks
Need to update point-of-sale (POS) systems
Staff training on multiple complicated systems
Multiple CRM systems and spreadsheets
Need for more consistency with tools
Securing data flow while adhering to network isolation needs
Manually calculating compliance scores with unreliable data
Electricity and internet cuts causing on-premise server downtime
"Managing data for more than 200,000 refugees"
Linking operational sectors together
"Existing CRM solution was lacking in connectivity, insight into the customer base, and relationship building"
Manual data export for remote evaluation
Need for a centralized cloud solution for plant data
Retrofit IoT sensors in existing plants
Managing development freedom while maintaining data security and quality control
Preventing duplicated functionality and application sprawl
Fragmentation in digital health
Reactive healthcare system
Economic sense of building on-premises infrastructure
Ensuring security and patient privacy
responding to healthcare challenges
Lack of systems to understand customer product usage
Transitioning from a traditional B2B model to a customer-focused model
Aggressive timeline to deploy the platform
Evolving grant rules and requirements
Unknown workflows and processes
Handling the rise of IT workload from increased inquiries
Improving chatbot reply accuracy to a variety of phrasings
Hard to track and manage data across a hybrid environment
Manual data curation was time-consuming and cumbersome
Lack of automated data cataloging and analytics component
Analyzing data from various customer source systems
Requirement for a highly scalable database
Legacy technology systems
Incomplete and inconsistent data
Difficulty in securing sensitive data
Inefficient reporting processes
Data getting lost or being incorrect when shared via email
Manual data entry errors
Time-consuming report preparation
Complexity arising from funds investing in other funds
Need for rapid delivery to meet internal deadlines
Addressing volunteer deployments effectively
"Managing large quantities of information (120,000 volunteers)"
Managing accounts manually without centralized structure
Transitioning employees from onsite to centralized model
Previously used many disconnected business platforms
Independent financial systems in global facilities
Need for integrated hazardous materials shipping documentation
Outdated systems and software not agile or mobile-friendly
"Lack of a unified, dynamic platform for collaboration"
IT systems holding back innovation and quick progress
Changing eligibility criteria for PPP loan forgiveness
Time sensitivity of developing a loan forgiveness solution
Transition to remote work due to COVID-19
Maintaining growth-oriented operations during the pandemic
Managing specialized systems
High turnover of service technicians
Training new technicians during COVID-19
Boost availability and performance of legacy data historian
Manage ever-increasing amounts of data
Inefficient and redundant manual processes
Lack of streamlined communication
Difficulty in accessing real-time data
Data security concerns
Traditional time-consuming data organization methods
Need for actionable insights from vast repositories of data
Scalability issues with legacy AS/400
Lack of expertise from legacy cloud provider
Cumbersome maintenance of legacy platform
Ensuring foundations are stable and scalable enough for more growth
Previous system was difficult to extract any value-add information
IoT data stored in a US datacenter
Need for faster data responses
Scalability concerns
Legacy systems difficult to use
Complexity of shipment and warehouse management software
Platform crashed every few days
Slowed down with more than a dozen users
Load time of up to two minutes for the home page
Help tickets increased by 400 percent due to staff’s unfamiliarity with technology
rapid growth during the pandemic
need for scalable technology
Tenant-to-tenant migration complexity
Pandemic disrupting original plans
Employees dispersed across homes
Siloed systems with isolated data sources
Long load time for visuals in Power BI
Inconsistency of information across different channels
Near-constant attacks from hackers
Paper-intensive processes
Ensuring HIPAA compliance
Rolling out the solution in less than four days
Transition to remote work
Limited visibility on training program outcomes
Need for real-time analytics and insights
Integrating numerous legacy data sources
Ensuring accurate staffing levels and availability during COVID-19
Handling duplicate entries
Difficulty in updating contact information
Tracking supporter status changes
Managing increasing storage and computing costs
Maintaining hundreds of individual valuation models for different legal entities
Handling extreme demand spikes for computing power
Capturing customer needs not previously on radar
Need for centralized data repository
Ensuring data quality
Vast amount of data in different silos
Difficulty in moving the right data to the right place at the right time
Requirement to comply with security and GDPR
Detect chiller system anomalies in commercial buildings
Create a personalized lighting environment according to customer requirements
On-premises datacenter couldn’t handle increasing data demands
Lack of agility to spin clusters up and down as necessary
Remote work adaptation
Large volumes of data arriving in 10-second intervals
Data management and reporting tools challenges
Need for real-time BI reporting
Unprecedented healthcare crisis
Sourcing and conserving PPE
Managing logistics and labor resources
Tracking and resolving operational issues
Siloed student data
Time-consuming manual data analysis
Adoption during remote work due to COVID-19
Managing cultural shift after acquisition
regionally distributed ERP system barred economies of scale and global standardization
needlessly repeated visits (truck rolls)
Meeting information security and compliance requirements
Bridging building analytics with clinical analytics
Low adoption rates of on-premises solutions
Need for a cloud-based solution to comply with local regulations
Migrating SAP system to Azure
Maintaining business continuity during the COVID-19 pandemic
massive undertaking of migrating SAP landscape
required careful planning and close coordination with a network of skilled partners
Lack of development experience
Time-consuming and inaccurate sales budgeting process
legacy on-premises technology
demand on IT teams to manage legacy systems
Inadvertently leaking confidential data due to incorrect classification
Protection from targeted attacks from outside
Lack of integration and connections within the customer portal
Different working methods without a common data basis
Conflicts among different working methods
Lack of a common data basis slowing processes
Need for a flexible and easy to adapt CRM system
Manual processes slowing customer service
Adapting to crisis situations like bushfires and floods
Transitioning to remote work due to COVID-19
High per-seat pricing structure leading to limited licenses
Time-consuming data management with Excel
Existing program couldn't scale with company growth
Hidden costs and limitations in replacement software
Difficulty in pulling data together in a timely way
Shifting organizational culture from Excel and PDFs to interactive dashboards
Aging supervisory control and data acquisition system
Requirement for a system that could automatically scale and be robust
Disparate data sources causing a spaghetti-like structure
High maintenance costs for individual systems
Duplication and inconsistencies in data
Overloaded Power BI capacity and failing applications
Manual and time-consuming data compilation processes
manual or batch process data exchange
handling large datasets
Disjointed approach with over 60 different systems
Daunting cloud migration process
Existing CRM platform offered limited functionality
Sales team preferred using Excel spreadsheets for tracking information
Lack of transparent reporting across the globe
Errors and slow productivity with on-premises SAP
Database reaching its capacity
managing sensitive information
developing custom-designed products without compromising security
Manual processes that limited production capacity
On-premises infrastructure could not accommodate vast quantities of data or provide necessary analytical resources
Handling daily spikes in demand with batch processing
Scaling up to meet data volumes approaching 5 terabytes
Ensuring data consistency and availability
Digitizing processes to optimize the customer journey
legacy member-engagement environment
multiple siloed systems
employees had to sign on to multiple applications
slow resolution and multiple contacts for the same issue
Employees had to manually extract data from many sources
Different reporting systems and data storage with a total of 800 applications across the company
Lack of dedicated fraud protection layer in current system
Potential increase in fraudulent activities with new platform
Citizens travelling to offices and finding them closed
Citizens going to government offices for print-outs they could print themselves
Limited operational hours of the call center (8-4)
Expensive network access for unemployed people
Maintain business momentum while scaling
Upgrade data structure
Information not found
Costly and slower to scale on-premises data warehouse
Long time to expand physical datacenter storage
"High dependence on ensuring availability, proper utilization, and optimum productivity of the equipment fleet"
Complex production processes
"Strict compliance requirements regarding product traceability, quality, and food security"
Manual investigation is time-consuming
Need for thorough research before discussing with employees
Difficulty in finding and retaining private donors and volunteers
Data housed in disparate silos making it hard to connect
Siloed data from multiple systems
Meeting rising customer expectations
Increase in regulations and rapid expansion of healthcare information
Information overload from multiple communication systems
reliance on numerous disparate systems and manual processes
increasing costs and decisions based on limited data
Fragmented customer data across various systems
Inefficiency of sending spreadsheets via email
Siloed information causing reactive instead of proactive decision-making
Lack of a single source of real-time truth about all aspects of supply chain and manufacturing operations
"Need to manage complex, ever-changing information in the supply chain"
Reliance on siloed data
Lack of central governance
Inability to react quickly to data
Compliance issues with data usage visibility
Manual forecasting tasks were slow and inaccurate
Paper-heavy and complex processes
Economic fluctuations affecting demand
Nationwide worker shortage in Japan
Security measures for factories
"Traditional methods of data sharing lacking speed, capacity, control, and security"
Need for better internal organization
"Running six different systems interconnected by convoluted, manual, and volatile integrations"
Need for robust and future-proof core business processes
"Securely capture, analyze, and integrate real-time data from different sources"
Unreliable or impossible connectivity
Managing capacity for a small team
Maintaining clear insight into the project portfolio
Tracking staff accomplishments and reporting them to executives
Prioritizing and planning projects accurately
datacenter lease expiration
migrating business-critical applications within a six-month window
e-commerce and ERP migration complexities
network setup
no scheduling system
reliance on spreadsheets for data tracking and analysis
lack of employee experience with advanced ERP solutions
High costs and lengthy development times for mobile app development
Special set of skills required for traditional mobile app development
Finding a solution for paper-based safety and compliance audits
Aging mainframe-based applications
Need to consolidate multiple mainframe applications
Managing large volumes of data
Older ERP system took hours to generate reports
Data was often inaccurate
High costs of maintaining on-premises software
Limited technical staff
Flat productivity
Decreased margins
Schedule overruns
Increased competition
Difficulty finding available conference rooms and workspaces
Only rooms on specific floors could be reserved for external meetings
Expensive proposition to build an app during a Challenge Day
Handling variability in daily operations
Managing unexpected passenger influxes
Forced to move faster due to rapid technological advancements
Rising IT operating costs for SAP
Inefficiency of having multiple hosting contracts
Limitations to innovation from inflexible hosting models
Lack of in-house expertise for the migration project
Initial concern about deploying Java-based Experience Manager in Azure
Security concerns for mission-critical applications in the cloud
Old and siloed systems
Separate 20-year-old systems for different sales channels
Overwhelming challenge for a small IT team
Decide between building own analytics or buying existing technology
Trusting the security of student data to third parties
Managing overwhelming volumes of data
Need for global infrastructure and on-demand scalability
"Navigating the wide range of 50,000 pages of HP product information"
Difficulty connecting to headquarters via VPN
Booming storage costs
Limited IT staff to manage infrastructure
Capturing real-time information from processes anywhere around the world
Controlling and managing processes remotely
Connecting and controlling all operations within a plant to reduce water usage
Manual extraction of fixed costs from SAP into Excel
Complex and tedious forecasting process involving 50 legal entities
Few staff capable of managing actual and forecast data in reports
Users could only access region-specific applications
Need for a collaborative and inclusive model
Multiple different reports and spreadsheets
Miscellany of data sources from over 90 facilities
Integrate data from different IT systems
Create a central point for data integration and analysis
Time-consuming and expensive quality assurance process
Resource-intensive test environment
"Static, labor-intensive and inefficient reporting processes"
"Data in the hands of a few, not available throughout teams"
High volumes of customer and product data
Complex supply-chain issues and customer behavior analysis
Managing high volumes of data quickly without consuming mainframe resources
Retrieving data from ERP systems and creating reports was time-consuming
Disparate environment of legacy finance systems
Dirty data and undocumented processes
"Inaccuracies, inconsistencies, misinterpretation and lack of trust in numbers"
Stepping up data and analytics processes
Information not found
most of its data storage facilities were on-premises
capacities were limited and structures excessively complex
Struggled to find a BI platform with the capacity to process high volumes of information
Needed to extract data directly from new platform
Deliver data to business users for multiple purposes
Siloed and underused data
Difficulty in managing digitization data
Lack of transparency on carbon emissions and energy costs
Batch-processed SAP data had delay issues
Difficulty in creating a unified environment due to mixed usage of QlikView and spreadsheets
Acquiring data quickly and accurately from a broad range of sources and locations
Rising costs and decreasing performance of data warehouse
Running analytics on retail data was challenging
lack of centralized data and strategic reporting
inefficient vehicle fleet deployment
Data was spread across 30 siloed internal and external platforms
Different media partners speaking slightly different data language
High cognitive load on employees
Outdated information in reports
Lack of standard way to ingest data
IT architecture limitations
"Small personal loan space is fast moving, fraught with risk, and extremely competitive"
Leads often acquired en masse with as little as 30 seconds to vet
Gathering actionable data from more than 650 stores
Different time zones and operating styles
Different information systems
Impacts of Covid-19
