strategies_texts
implementing Epicor and Spreadsheet Server
leveraging existing Excel skills
Direct connection to ERP data with Spreadsheet Server
Adoption and integration of Bizview for automated workflows and executive dashboards
Hiring a data analyst skilled in Excel
"Search for a solution that provides a single source of truth, ease of use, real-time data"
Fast installation and implementation
Leveraged insightsoftware to create a scalable and flexible platform
Integrated predictive analytics and trend analysis capabilities
Bizview implementation
Development of advanced features
Data warehouse creation
Replacement of old BI system with Qlik Sense
Attending MRI conference
Smooth implementation with insightsoftware account manager
Staff training sessions
Partnered with Logi Symphony
Provided flexible and scalable reporting and analytics
Enabled self-service for customers
Integration of Power ON within Power BI.
Focus on familiar interfaces like Power BI and Excel.
Replaced legacy BI platform with Microsoft Power BI
Added Power ON for enhanced sales forecasting and write-back capabilities
Migrated from Excel to Power BI
Added write-back to Power BI with Visual Planning from Power ON
Implemented Power ON Visual Table Editor
Utilized Prima Consulting for support
Replacing SAP Business Objects with Microsoft Power BI
Adding Power ON to Power BI stack
Integration with Microsoft Power BI
Using Power ON BI's Visual Planner
Extending capabilities of Power BI with Power ON
Developing a new method of forecasting
Allowing users to remain in familiar tools like Excel and Power Pivot
Capitalized on investment in Microsoft Power BI
Replaced Excel spreadsheets and Access database with Power BI + Power ON Power Planner
Partnered with Power ON
Used Power BI since 2015
Built modular low-code tool sets
Using Viewpoint Vista
Leveraging Spreadsheet Server
Product training and support
Partnering with insightsoftware to develop Angles Professional
Consulting with insightsoftware to solve reporting challenges
Added Logi Symphony to Responder Manager program
Integrated Microsoft SQL Server Enterprise for performance and flexibility
Connected to third-party data sources
Choosing Logi Symphony for embedded analytics
Implemented Spreadsheet Server with Query Designer
Conducted an hourlong training session for project managers
Using Logi Symphony to create an end-to-end reporting platform
Seamless integration with Oracle database and Argus pharmacovigilance platform
Skipping intermediate data modeling processes to save time and ensure data integrity
Centralizing report generation tasks
Turned to Trabon Solutions for BI strategy and software requirements
Evaluated four different software options
"Selected Logi Symphony for its features, ease of use, and flexible pricing model"
Embed Self-Service from Logi Symphony
Integrate SSM with business system permission structure
"Partnering with Logi Symphony for scalable, integrated reporting and dashboard tool"
Embedding Logi Symphony into existing anti-fraud and regulatory compliance software architecture
Chose Logi Symphony for its affordability and capability
Met with departments to understand analytic requirements
Created standardized reports and views
Utilizing a strategic partner in insightsoftware
Using Logi Symphony’s elemental design for rapid development and deployment
Using Logi Symphony to replace the existing reporting tool
Leveraging Logi Symphony to redesign and deploy over 100 reports
Integrating CSS styles for custom layouts and designs
Using Logi Symphony’s Designer and drag-and-drop interface
Embedded Logi Symphony's APIs into existing environment
Created pre-built report packages for quick adoption
"Chose Logi Symphony for easy embedding, improved scheduling, and secure sharing"
"Integrated Logi Symphony with existing application, data, and stored procedures"
Selected Logi Symphony for its user-friendly interface and easy data connection
Customized self-service capabilities for customers
Integrated analytics into application workflows
incorporated Logi Symphony layer into existing CMS
recommended as an additional module to potential customers
Pairing Angles with Oracle EBS to minimize change management
Utilizing existing BI tools to aid transition
Leveraging Angles for real-time and self-service data access
Consultation with BI partner Dynamics4Business
Implementation of Jet Analytics
Proof of concept by Dynamics4Business
Quick and efficient integration of Jet Analytics
Training staff on Jet Analytics
Using Qlik Sense and QlikView for data processing and front-end presentations
Standardizing the look and feel across Qlik Sense apps with Vizlib's customization
Providing basic training on dashboards to boost adoption
Use of visualizations for data simplification
Creation of a patient-on-a-page app
Customization with Vizlib Library
Approached a third-party consultant to identify best solution
Implemented Qlik Sense and Vizlib
Upgrading from QlikView to Qlik Sense
Adding Vizlib to Qlik applications
Testing Vizlib’s input and visual power in the finance team
Adoption of QlikView in 2009
Transition from QlikView to Qlik Sense
Purchase of Vizlib products
human-centred approach to analytics
leveraging Vizlib for customization and design
balancing app customization with performance
Migrating from QlikView to Qlik Sense
Using Qlik Sense + Vizlib for customization
Replicating exact specs with Vizlib Table
Adoption of Qlik Sense for data analytics
Enhancing Qlik Sense with Vizlib Library for better visualizations
partnered with Catalyst BI
recommended Vizlib’s Collaboration suite
implemented single application for data management
Adopting Vizlib to elevate reporting
Integrating Vizlib with Qlik Sense for better visual appeal
Using Vizlib Templates for faster dashboard development
Co-developed a three-phase approach
Deployment of Qlik Sense BI solutions
Introduction of Vizlib products
Using Vizlib products to enhance Qlik Sense
Implementing visual charts for management
Migrating from QlikView to Qlik Sense with Vizlib
Adoption of Qlik Sense and Vizlib
Implementing near real-time data processing
Use of Vizlib’s advanced value-added products for Qlik Sense
Standardized way to customize data visualizations
Utilizing Vizlib's solution to expand Qlik capabilities
Customizing and styling Vizlib Sheet Menu items
Partnering with Vizlib and Qlik Sense
Using Qlik Sense extensions
Deploying Vizlib’s value-added products
Migrating from QlikView to Qlik Sense
Using Vizlib for added functionalities
"Initial trial and implementation of Vizlib Writeback, Library, and Finance"
Training sessions orchestrated by JBS
Thorough vetting process for ERP system
Implementation of Jet Reports
Automation of customer-facing communications
"Adoption of Spreadsheet Server, an Excel add-in that integrates with their ERP"
Training new teams on Spreadsheet Server as needed
Leveraging Magnitude Angles for Oracle
Using Oracle Discoverer
Dedicated content manager
Second pass through product data to improve quality
Creation of microsites for targeted markets
use of Agility's out-of-the-box XML export tools
facilitate scheduled updates to the ecommerce site
purchase of Agility
use of Agility's web services API
Attended SAP User Conference
Chose Process Runner GLSU based on user feedback and examination
Evaluated and chose Magnitude Process Runner GLSU for its simplicity and ease of use
Used Process Runner GLSU to handle complex Excel templates and calculations
selection of Agility's Oracle-based PIM system
integration with multiple upstream ERP and merchandising systems
"Ensuring Spark ODBC connectivity to BI tools like Tableau, Excel, Microstrategy, Qlik"
Utilizing Magnitude's ODBC 3.8 Data Connector for Databricks Spark SQL
introduction to GLSU by client
writing a step-by-step guide for creating templates in GLSU
becoming the main point of contact for training and support
hiring an IT Analyst familiar with Process Runner GLSU
demonstration of the product to key stakeholders
Evaluated off-the-shelf software
Selected GLSU based on consultant's recommendation
Evaluated known available options in the market
Downloaded free trial of Process Runner
Leveraged Excel familiarity for data automation
Utilized ready-to-run templates on Process Runner Cloud
Relied on Magnitude's support team
Consolidation of data using Hadoop
Visualization with SAP Lumira
Using Simba connectivity solutions for SQL capabilities
Running Agility on existing Microsoft SQLServer environment
Integrating SAP ERP through Agility's APIs and Content Gateway
Evaluation of Simba Netezza ODBC Data Connector
Choosing Magnitude Simba Netezza ODBC Data Connector for Mac
Quick and fluid implementation
Interactive setup with product team
Pre-built reports and content packs
empowering healthcare professionals with real-time access to data
custom reporting capabilities
ability to make one-off queries
freedom to use familiar tools such as Microsoft Excel and Microsoft Access to query data
Chose Magnitude for its suite of ODBC connectors and Simba Development Kit (SDK)
Built a custom ODBC driver using Magnitude's SDK and technical assistance
Integration of connectivity testing services into the development cycle
Automating more than 100 test cases
Partnership with Magnitude starting in 2014
Managed services model introduced in 2019
Adoption of Magnitude Process Runner
Use of native integration to Microsoft Excel
Receiving excellent customer support
use of CATT and Custom Transactions
implementation of Process Runner
Implemented SAP PLM
Modernized logistics
Plant floor remediation project
Rewrote RF transactions
Retrofitted warehouse facilities
Migration to SAP S/4HANA
Implemented Magnitude Angles for SAP
Implemented SAP WMS for efficient warehouse management
Added Magnitude Angles for SAP to existing SAP BI landscape for daily analytics
Started a warehouse improvement project
Implemented Pick by Voice in the warehouse
Reorganized warehouse for logic picking of orders
Global ERP implementation of Oracle Release 12
Purchased Oracle BI Applications
Adopted Angles for Oracle (formerly Noetix) with Magnitude Professional Services
eliminating a massive amount of report customizations
replacing existing reporting platform
"selecting insightsoftware for its flexible, customizable user interface"
Conducting a cost/savings analysis of manual data entry vs. Process Runner GLSU licensing and maintenance
Creating and distributing various spreadsheet templates stored on the SAP server
Selected Simba for its market-leading set of Simba ODBC drivers
Collaborative partnership to integrate Simba data connectors into RStudio’s flagship offerings
Expanded licensing of the Simba SDK and Simba ODBC library of connectors
Minimized footprint of ODBC connectors for embedding into the platform
Thorough analysis of available options
Selection of Angles for Oracle after benchmarking other solutions
Adoption of Jet Reports
Collaboration with Consulendo (Microsoft-related services partner)
Creating a self-service portal for finance
Choosing tools that integrate seamlessly with J.D. Edwards
Dedicated two weeks to training post-installation
Training with Ujuzitech
Upskilling in Jet Reports
Testing the Pilot
Involvement of Calumo Consultants
Training staff to become super users
Extensive training for staff
Implementation of GL Wand across the entire group
Implemented Viewpoint Vista ERP
Adopted Spreadsheet Server
Worked with insightsoftware team for implementation
Adopting Jet Reports for automation
Centralizing data with Jet Analytics
Conducting a brief two-day training session
Collaborating with EC Consulting for implementation
Training staff extensively
Using Jet Hub for automated report scheduling
Company-wide technology transformation initiative named ‘Cornerstone’
Re-implementation of Oracle E-Business Suite for core ERP system
Selection of Angles for Oracle for BI and analytics
Implemented an integrated tenancy management system
"Selected Calumo for its usability, web delivery, and integration with Microsoft Office"
Worked with tight timeframes and delivered monthly reports ahead of schedule
Automate the reporting process
Establish consistent data structures and formats
Provide self-service capabilities
Support drill down into data details
Iterative implementation process
Reviewed tax reporting processes
Scheduled demos with potential software vendors
Evaluated different options
Partnered with Taxvibes for implementation
Implemented Angles for SAP’s Operational Performance Management add-on
Used Angles for SAP to improve data quality and reliability
Utilized Angles for SAP KPI Dashboard for S&OP process
Embedded analytics layer from Logi Symphony
Operationalized systems for ease of use
Using Process Runner for data migration
Reengineering master data with Excel and Process Runner
Disaggregating business processes before migration
Updated formulas to pull from the general ledger
Implemented Bizview to reduce human error and pull information directly from ERP
Worked closely with insightsoftware staff for a seamless implementation
Embedding Logi Symphony into their LMS
Multi-tenancy scenario for client management
Creating a library of interactive dashboards
Integration of a seamless embedded experience
Collaboration with healthcare customer advisory board to define criteria
Customization of dashboards to match branding
Leverage existing technology stack to connect to any data source
recommendation of Logi Symphony
building custom dashboards
Implementation by one developer
Bringing in data from three different applications
Assembling a team including IT Manager and Director of Plant Operations
Using dashboards for real-time data presentation
Embedding Logi Symphony within the application
Creating pre-defined and customizable report templates
Seamless integration with core product
Accelerated requirements definition process
Rapid market evaluation and vendor selection
Preserving existing management reporting process
Adaptation to broader ongoing data transformation program
Pilot group with key departments
Training session
Collaboration between IT and Finance
Engaging Calumo in late 2018
Low-risk proof of concept program
Integration with existing Xero instances
Connecting Calumo to internal AWS data warehouse
two-phase project approach
data cleansing exercise
leverage existing metadata from Longview Tax
Selected Logi Symphony for ease of use and agility
Utilized drag-and-drop and configuration-based development
"Integrated GIS, GPS, and SOA technologies"
Embedded analytics from Logi Symphony
Integration of tokenization capabilities
integrate a third-party solution
ensure compliance and security
customize the solution for seamless user experience
Decided to buy an embedded analytics solution
Partnered with Logi Symphony
Focused on core competencies and avoided building analytics in-house
Implementation of Logi Symphony
Minimal training for users
Integration with existing underwriting and claims systems
Excel integration and specific data uploads
External distribution of assets
"Self-service, automated generation of centrally approved template Powerpoint slides"
Adoption of Bizview as a planning solution
Streamlining planning processes across different areas of operation
Adoption of Microsoft Dynamics NAV
Use of Jet Reports for easy report creation
Training and industry-specific knowledge provided by Harmonize IT
Implemented Jet Analytics with CrossPoint 365
Training from CrossPoint empowered staff
Implemented CXO for easier data collection and combination
Trained 150 business users across the organization
Implementation of Davisware Analytics powered by Spreadsheet Server
Use of Query Designer feature in Davisware Analytics
Integration with existing finance systems
Selection of a flexible and comprehensive tool
Replaced Excel-based system with Longview Tax
Adapted Longview Tax to new role
Streamlined data into one GL
"Partnered with CPM consulting experts, Satriun"
Implemented CXO software from insightsoftware
sought help from a business consultant
implemented Jet Analytics with the help of insightsoftware partner datenkultur
translated ERP tables and developed specialized Power BI dashboards
Adopted Jet Reports from insightsoftware for reporting
Implementation of CXO in 2017 as a proof-of-concept project
Full rollout to other users in 2018
Transferring metadata structures easily from the consolidation tool to CXO
Shift to Jet Analytics for better control
Three days on-site training with BSH consultant
Engaging with BSH for support on tricky issues
Implemented Jet Reports
Added Jet Analytics and Power BI
"Deployment of Bizview for budgeting, planning, forecasting, and reporting"
Consolidation of data from various legacy ERPs
Using Atlas for data uploading from Microsoft Excel
Retaining Atlas during migration to Dynamics 365 Finance and Supply Chain
Developed a suite of pre-built Power BI dashboards
Integrated Jet Reports as part of standard offer
Created a library of best practice reports
Sought help from a certified public accountant
Selected Viareport after a comparative analysis
Adopt a user-friendly reference consolidation solution
Predefined configuration for quick installation
Extend the scope of SAP FC to budget reporting
Engagement with different market players
Weekly project committee meetings
Dedicated consultant overseeing the project
Remote deployment during COVID-19 lockdown
Working with insightsoftware customer solutions team expert for proper scoping
Virtual training for the new system
Digitalized report processes
Replaced Excel packages
Implementing IDL Workflow Function
Introduction of IDL Konsis
Connection of SAP via an interface
Specialist support from IDL
Adoption of comprehensive IDL system solution
Standardization and automation of reporting processes
joined early adopter program
one training session for implementation
fantastic training tools and tips sheets
Worked with insightsoftware partner Addedo
Remote training sessions
Building templates and structure during training
Implemented insightsoftware for self-service reporting
Chose tools with familiar Excel interface for ease of deployment
Comparison of different solutions on the market
"Selection of insightsoftware’s Jet Analytics for data preparation, automation, and modeling"
Implementation of Microsoft Dynamics 365 Business Central
Adoption of Jet Reports and Jet Analytics
Collaboration with Jet partner Atlytix
Implementation of Spreadsheet Server
Direct connection into Yardi Voyager ERP system
Selecting Spreadsheet Server for its user-friendly interface
Implementing a self-service reporting tool
Utilizing out-of-the-box financial reports
Integration with Sage and later with Xero’s cloud-based accounting software
Smooth and stable implementation with consistent product experts
"Expanded use of Bizview for financial reporting, rolling forecasts, dashboards, and KPI analysis"
Deploy Jet Analytics and Jet Reports
Leverage pre-built content to quickly create a data warehouse
Use existing Excel skills to create reports
Implemented Spreadsheet Server
Provided virtual training during COVID-19 pandemic
Shifted report generation responsibility from IT to Accounting
Adopted Certent DisclosureNet as a primary tool
Integrated Certent DisclosureNet into daily workflow routines
Initial and ongoing support from client success team
One hour of training with client success specialist
Utilization of automatic alerts and document compare function
Set up a working party to review Ernst & Young report and identify improvement areas
Implemented Certent CDM
implemented Certent Disclosure Management (CDM)
utilize Excel® to update tables instantly
"Adopting a streamlined, automated approach supported by CDM"
Using Certent CDM to consolidate and streamline financial reporting processes
Rapid implementation with the help of Addedo
Deployment of CDM in the cloud
Evaluated multiple solutions
Selected Certent CDM for comprehensive automation
Worked closely with insightsoftware and iOCO for implementation
Reviewed multiple providers
Selected insightsoftware for Microsoft integration and features
Ensured on-premise solution for internal security requirements
Leveraged familiar platform for ease of transition
Partnered with insightsoftware
Implemented a disclosure research solution
Partnered with insightsoftware
Implemented disclosure research solution
I
n
f
o
r
m
a
t
i
o
n
 
n
o
t
 
f
o
u
n
d
Researching major players for functionality
Selecting a tool that auditors are comfortable with
Thorough data testing against prior reports
Researched available software
Met with insightsoftware to discuss requirements
Adopted Certent Equity Management
Investing in a software solution
Choosing Certent Equity Management for comprehensive management
Partnering with insightsoftware
Offering Jet Reports and Jet Analytics
Importing legacy data into Jet Analytics’ pre-built cubes and tabular models
Introduced to insightsoftware’s Wands by Oracle consultant
"Purchased GL Wand, Budget Wand, and Reports Wand"
Upgraded from older versions of CXO Software to the newest release
Trained key personnel as Administrators
Created new reports and views for remote access
Using Spreadsheet Server for Epicor ERP reporting
Migrating from Epicor to NetSuite with Spreadsheet Server
Building segment lists and category codes in NetSuite
Requesting a demo of Spreadsheet Server
Installation and immediate usage of Spreadsheet Server
Carefully setting up and checking report structures
Automated reporting templates pulling live data from JD Edwards
Implemented Spreadsheet Server into MRI reporting processes
Implemented a single instance of Longview Plan
Formed a selection committee
Conducted RFP exercise and custom demos
"Design, development, and deployment advisory services"
Implement Longview Close and Longview Plan
Rollout of Longview CPM to more users
Internal training by Longview enthusiasts
"Planning, budgeting, and forecasting"
Consolidation
Longview Integration Suite (LVIS)
"Design, development, and deployment advisory services"
Annual budgets and quarterly and weekly forecasts
Driver-based planning using operational KPIs (bottom-up)
Implemented multiple global version controls
Mill level what-if scenario planning
Profitability Modeling and Margin Analysis by Product at the SKU level
Consolidation and management reporting (implemented)
"Planning, budgeting, and forecasting (future implementation)"
"Implementation Services including Application Design, Configuration, Development, and Testing"
Planning
Consolidation
"Design, development, and deployment advisory services"
Implement Longview Tax
Integrate with accounting ledger system and Corptax compliance system
Automated Data Import
Foreign Currency Translation
Cash Flow
Intercompany Eliminations
Statistical and KPI calculations
Proportionate Consolidation Calculation
Statistical and KPI information (input and calculated)
Conducted an RFP exercise including proposals and demos
"Phased implementation: Expense Budgeting first, then Revenue Forecasting, followed by Capital Project Cash Flow Forecasting and Balance and Income Statement Reporting"
Assigned experienced technicians and internal staff for system design and implementation
Automating as many processes as possible
Leveraging historical data on the central platform
Tailoring the tool for data visualization
Demonstrations and training sessions
Migration from Dynamics AX to Dynamics 365
Implementation of Jet Analytics
Training and support from insightsoftware consultant
Acquired company already using Bizview
"Reviewed and decided to implement Bizview for financial planning, reporting, and analytics"
Extensive training in local offices
Delegating responsibility to selected 'super users'
Simplifying the solution
Adopted Spreadsheet Server by insightsoftware
Rebuilt reporting packages in Spreadsheet Server
Utilized Spreadsheet Server’s Distribution Manager tool
Implementation of Bizview
Integration with Microsoft Dynamics AX
Implemented BizView Reporting and Dashboard
Added Budget and Forecasting in late 2019
Decentralized management
Faster access to information
Shorter time used on producing financial reports
Significantly less time managing ongoing planning processes
Implemented Bizview
Decentralized budget process
Involved experienced implementation consultant
Leveraged Dalakraft’s own employees
Implemented Bizview for budget preparation
Integrated with human resource system (Stafftuner) and accounting system (Microsoft Dynamics AX)
Using Python and Dash for real-time analytics
Creating custom views for property managers
Dynamic Pricing tool
Using Dash Enterprise's Workspaces code editor
Deploying apps quickly without disrupting service
Connecting to live data streams
Using the Webviz platform to operationalize data science
Coupling Webviz with modeling stack
Developing custom components for specific applications
Took ownership over their own technology stack
Built interactive analytic applications tailored for data science using Dash Enterprise
Use of Dash Enterprise
Standardize data from over 1.5 million patients
Provide no-code analytics capabilities
Utilizing Dash Enterprise for quantitative analytics and data science
Developing custom client analytics applications
Deploying production-grade data applications
adopt Dash Enterprise for building interactive experimentation tools
Use of Dash to create interactive analytic applications
Sharing of Dash applications for managerial insights
Securing Dash Enterprise team license
Developing a prototype using Dash’s Python framework
Incorporating feedback quickly during beta testing
Making necessary adjustments for WCAG 2.x standards
Adopted Plotly Python graphing library for data analysis
Leveraged Dash for quick experimentation and development
Partnering with Plotly’s Professional Services
Developing and presenting a proof-of-concept Capacity Application
Adopt Dash Enterprise
Simplify ETL pipeline
Automate backend workflow
Utilize AI insights for dynamic dashboards
Coordination across multiple stakeholders and diverse data systems
"Building dashboards, reports, and web applications with Python and Dash"
"Using Dash features like JavaScript, Python, pattern matching callbacks"
Engaged Plotly’s Professional Services
Evaluated more than 10 open-source graphing libraries
Use of Dash to adjust model parameters directly in the application
Building custom interfaces using Dash DataTable and other visualization components
Use of Plotly for creating custom visualizations
Integrating visualizations into web applications for remote access
Integration of liquid handlers alongside manual laboratory processes
Development of Dash apps to integrate human and scientific processes
Using open-source Dash for quick development
Invested in Dash Enterprise for enhanced capabilities
Embedding middleware for seamless deployment
Employing Python modeling
Using Dash Enterprise
Integrating Snowflake for live data updates
Integration of Dash Enterprise with cloud-based technology stack
Empowerment of analysts and traders to develop their own tools
Used Plotly's Dash for quick development of interactive applications
Developed custom templates to save time
leveraged Python and Dash
"single, free, open-source platform"
develop multiple data apps
use Dash Enterprise to streamline and accelerate work
Invested in Plotly’s Dash Enterprise
Utilized Microsoft’s Azure Databricks
Built a custom end-user focused solution
Adoption of KNIME as a core analytics tool
Ingestion of data from various biomedical knowledge bases
Validation of data through an interactive dashboard by clinicians and subject matter experts
Adopted KNIME as the core analytics tool
Ingested data from multiple biomedical knowledge bases
Built all-inclusive master datasets
embrace KNIME as central data science platform
partner with Redfield to build a recommendation engine
adopted KNIME as the central data science platform
built a recommendation engine with the help of KNIME partner Redfield
connected to main data sources in the cloud and on-premises
built an automated data pipeline
categorized data into key customer segments
built and deployed the content recommendation model in the same visual environment
tune and adapt the model when new consumption data surfaces
Building an in-house fraud detection platform (The SOBE Platform)
"Using KNIME to collect, transform, organize, and stabilize data"
Implementing machine learning models for fraud detection
Building a rule engine for claim review process
Building a tailored fraud detection platform with KNIME
Using machine learning and social network analysis
Creating automated workflows
Identify a baseline to quantify savings
Use topic modeling for work order analysis
Implement KNIME Analytics Platform
Identifying a baseline for retrofit phases
Using data science and NLP for topic modeling
Comparing retrofit and non-retrofit site performance
Implementation of a collaborative and interactive infrastructure
Use of KNIME Analytics Platform and KNIME Server
"Integration with databases, processes, and other software"
Use of KNIME Analytics Platform
Deploy workflows to KNIME WebPortal via KNIME Server
Seamless integration with databases and other software
Automation of workflows
Implementation of automated workflow testing
Integration of physico-chemical properties to the corporate database
User-friendly data visualization for pKa values
Automation of the process without human intervention
Use of KNIME workflow and KNIME Server for automation
Integrate physico-chemical properties to the corporate database
Implement user-friendly data visualization format for pKa values
Automate the entire process without human intervention
"Run the workflow once a day, during the night"
Extract newly registered molecules from ORACLE™ view
Automate property calculation with KNIME Analytics Platform and KNIME Server
Attrition analysis
Using data and machine learning for HR decisions
KNIME Guided Analytics application
Using attrition analysis
Defining optimal talent retention strategy
Training machine learning models
Use of tensor completion and matrix factorization
Incorporating publicly available data into the system
Creation of over 15 custom nodes in KNIME
No-coding workflow building environment
Adding custom code when needed
Tensor completion using matrix factorization
Pilot project in Chassis & Safety division
Agile setup of starting small and expanding quickly
Positive word of mouth
High bottom-up demand for trainings
agile setup of starting small and expanding quickly
positive word of mouth
bottom-up demand for trainings
creating more user-specific training content
standardizing knowledge sharing across teams and departments
Automated reports delivered straight to inboxes
Using KNIME Python Integration and KNIME Server
Building a workflow in KNIME Analytics Platform
Using native KNIME Python Integration to collect stock information
Deploying the workflow on KNIME Server
Using built-in scheduling feature for daily execution
Centralizing financial processes across multiple account teams
Consolidating the cost accrual process across 20 accounts
Using KNIME for automation
Centralizing financial processes
Automating financial cost accruals
Creating a transaction scheduling solution
Introducing KNIME through formal training programs
Automating the provision of comments and analyses for monthly closing reports
Utilizing KNIME Analytics Platform for data aggregation and time series creation
Automated commenting of deviations
Automating the provision of comments and analyses for monthly closing reports
Using KNIME Analytics Platform for data aggregation and time series creation
Use of data analytics platform KNIME
Standardized six-step process
Annotation of workflows
Using KNIME for data analytics
Creating standard workflows
Implementing logic and business rules
Introducing a standardized six-step process
"Use of data analytics platform, KNIME"
Create a recommendation engine
Deploy workflow on KNIME Server as a Guided Analytics Application
Recommendation engine
Guided Analytics
Automating FLC model with KNIME
Integration with other applications like Oracle-based budget applications
Evaluating multiple options
Partnering with Equinoxys
Using KNIME to automate workflows
Develop end-to-end automated data wrangling and reporting engine
Automation of data aggregation from the dealer management system
"Cleansing, blending, and transforming raw data"
Storage of raw data in a cloud data warehouse
Creation and update of warehouse tables for reports
Presentation of data in interactive and scheduled dashboards
Develop an end-to-end automated data wrangling and reporting engine
"Automate data extraction, transformation, reporting, and visualization"
Integration with Snowflake for cloud-based data storage
Use of PowerBI for dashboards
Reproducing Excel processes as KNIME workflows
Combining KNIME and PowerBI
Implementing automatic execution of workflows on KNIME Server
Automating processing and delivery of KPIs
"Using KNIME Analytics Platform, KNIME Server, and PowerBI"
Reproducing data transformation processes as KNIME workflows
Developing complex models manageable by a single data scientist
"Using a free, open-source platform (KNIME Analytics Platform) for organization-wide access"
Use of KNIME Analytics Platform
Development of connections between KNIME Server and data centers
Expert team performs advanced analytics and provides support and training
"User-friendly, intuitive, low (or no) code data analytics tool"
"Tool supports reusability and sharing, and is flexible and scalable"
Proficiently apply analytics to audits by all staff
Use KNIME for automating review work
Develop reusable workflows for process mining
Establish a dedicated department for data science and analytics
Collect pain points from across the company
Use KNIME as a core analytics solution
Establishing a dedicated department for data science and analytics
Collecting pain points across the company
Adopting predictive and prescriptive analytics
Using KNIME for advanced analytics
Centralizing overstock measurement
Developing a solution in KNIME
Integrating expertise from logistics experts
Creating a dashboard updated fortnightly
Centralizing how overstock was measured
Developed a solution in KNIME to gather and analyze data
Integrated expertise from logistics experts
Creating a model to forecast sales
Deploying the KFP on KNIME Server as a web application
Creating a model to forecast sales and tune it for accuracy
Deploying on KNIME Server as a web application via the KNIME WebPortal
Adoption of KNIME as a low-code tool
Use of components to standardize and repeat operations
Automatic data fetching and harmonization
Using KNIME for self-service analytics
Implementing automation for data fetching and harmonization
Investing in KNIME Server for automation and job scheduling
Automation of data reporting practices
Collaboration with analytics provider phData
Collaboration with analytics provider phData
Automation of data reporting practices
Development of KNIME solution
Removing business and application logic from the database
Conducting a head-to-head proof of concept
Switching to a Kimball-data model
Using KNIME and PowerBI to optimize data processes
Conducted a head-to-head proof of concept
Selected KNIME for its flexibility and extensibility
Implemented PowerBI for optimized performance
Switched to a Kimball-data model
Interviewing senior auditors to define business rules
Developing reusable KNIME workflows
Training 300 auditors in data literacy and KNIME usage
Interviewing senior auditors
Translating hypotheses into business rules
Developing reusable KNIME workflows
Training all auditors in data literacy
Rolling out KNIME across the organization
Citizen Data Scientist (CDS) training program
Integration with existing tools and infrastructure
Tailored Citizen Data Scientist (CDS) training program
Mix of online and onsite trainings and workshops
Developing analytical products
Driving a data citizen approach
Using KNIME Analytics Platform
Integration with other software
Developed analytical products to support Digital Industries strategy
Drive a data citizen approach for future collaboration
Utilizing KNIME Analytics Platform
Deploying workflow on KNIME Server for additional computational power
Deployment on KNIME Server for more computational power
Use of Tika Parser node for meta information retrieval
Try/catch errors construct to avoid workflow errors
Regex code in Java snippet for isolating CAS numbers
Defined Excel template for accounting files
Deployment of KNIME workflow as an analytical application in KNIME WebPortal
Use of a predefined Excel template
Deployment of a KNIME workflow via KNIME Server
"Using KNIME's low-code, no-code interface"
KNIME Hub and data-scientist-led tutorials
End-to-end data science platform
provided a common ground for team members with diverse backgrounds
used pre-built workflows and tutorials for upskilling
leveraged KNIME Hub for resources
experimented with prototypes and compared results
Automation of data collection and processing
Use of KNIME Analytics Platform for data workflows
Integration of KNIME with SQL database and Power BI
Use of KNIME Server for scheduling and workflow execution
Creating workflows in KNIME Analytics Platform
Scheduling data collection with KNIME Server
Using KNIME WebPortal for supplementary data uploads
Integrating SQL database on Azure and visualizing in Power BI
Utilizing Python nodes for data retrieval
Merging and transforming data from different data sources
"Using grouping, pivoting, and data visualization nodes for deep data dives"
Systematic price adjustment using value-based pricing and price line optimization approaches
Simulating effects of price adjustments
Using KNIME to merge and transform data from different data sources
Grouping and pivoting data
Data visualization
Applying value-based pricing approach
Applying price line optimization approach
Import and combine timesheet data
Perform sales-price analysis by comparing prices across periods and similar products
Perform extensive reconciliation of product master data
Import and combine timesheet data
Perform sales-price analysis
Extensive reconciliation of product master data
"Blending various data sources including business data, customer ratings, and news data"
Building a KNIME workflow for data crawling and integration
Deploying the workflow in an Azure cloud environment as a Guided Analytics Application
Blending different data sources
Using KNIME Python integration
Deploying a Guided Analytics application in Azure cloud environment
Reorganizing central public administration
Reforming district offices
Modernizing and reducing bureaucracy
Creating 'Government Windows'
Building a low-cost data warehouse on PostgreSQL
Using KNIME workflows for ETL processes
Integrating with Network Access Manager for authentication
Using Ansible for automated deployment and configuration
Building a low-cost data warehouse on PostgreSQL databases
Using KNIME workflows for automated ETL processes
Integrating the system with the government Network Access Manager (NAM)
Drug repurposing
Use of KNIME for data integration and advanced analytics
drug repurposing strategies
molecular similarity principle
Adoption of KNIME Analytics Platform
Utilizing KNIME Server and KNIME WebPortal
Integration of R and custom visualizations
"Adoption of KNIME Analytics Platform, KNIME Server, and KNIME WebPortal"
Combining KNIME with advanced statistical features and custom visualizations in R
Integration with 1C and KNIME Server
Creation of new decision-making service using KNIME Analytics Platform and KNIME Server
Automated testing system for the new service
Adoption of KNIME for visual programming and model development
Integration with existing decision-making system
Implementation of automated testing systems
Analyze information from the online shop and other sources
Develop a Big Data Recommendation Engine with Miriade
Use a collaborative filtering algorithm
Collect and clean data in Apache Hadoop system
Categorize data based on business needs
Install KNIME Server and secure infrastructure
Ingest data using Spark and KNIME nodes
Conduct predictive analytics with KNIME workflow
Schedule workflows using KNIME Server Scheduler
Create visual dashboards with KNIME WebPortal
Analyze information from the online shop and other sources
Develop Big Data Recommendation Engine using collaborative filtering algorithm
Collect data in a single Apache Hadoop system
Clean data and categorize based on predetermined rules
Install KNIME Server
Secure the infrastructure
Ingest data from selected sources using Spark and KNIME nodes
Conduct predictive analytics with KNIME workflow
Schedule workflows using KNIME Server Scheduler
Collaboration with the Department of Life Cycle Engineering at the Fraunhofer Institute for Building Physics IBP
Adoption of the Sustainability Data Science Life Cycle (S-DSLC) approach
Development of a mapping algorithm using KNIME Analytics Platform
Collaboration with Fraunhofer Institute for Building Physics IBP
Use of KNIME Analytics Platform
Implementation of mapping algorithm for data blending
Following the Sustainability Data Science Life Cycle (S-DSLC) approach
Phase one: Clean out excess products
Phase two: Use predictive analytics for reallocation
Phase one: clean out excess products from health care facilities
Phase two: use predictive analytics to reallocate products
Use KNIME Integration with R for demand forecasting
Continuous evaluation and feedback with stakeholders
Advanced analytics
Machine learning model for virtual testing
Integration with existing traceability data
Use of advanced analytics
Machine learning model integration
Visualization through browser-based data app
Collaboration with KNIME
Adopted Tableau for data visualization and reporting
Centralized data from more than 100 sources using ChannelMix by Alight Analytics
Automated and scaled data reporting infrastructure with Tableau
Hackathons and datathons for data literacy
Building and extending a data literacy programme
Forming a data and innovation office
"Focusing initially on business analysts, then rolling out across the bank"
Implemented Tableau
Full appraisal of available solutions
using AWS for backend infrastructure
using Tableau for data visualization
rapid scaling of resources
creating secure infrastructure for patient-level data
using Modern Cloud Analytics
Deploying Tableau for unified data access
Increasing self-service analytics capabilities
Using the Data Management Add-on for automation
Adoption of Tableau’s visual analytics platform
Decentralized approach to BI
Development of self-service options
Collaboration with Infotopics for foundational setup and training
Installed own Tableau server
Enrolled dashboard developers in the Tableau Academy
Involved future users in the development of new dashboards
used Tableau Server for accessibility
deployed in Amazon Web Services (AWS) environment
leveraged pre-existing internal development knowledge
partnership with Tableau
development of more than 600 dashboards and visualisations
organizing data & analytics course programmes
creating a community called Mandiri Tableau User Group (MTUG)
create interactive dashboards based on maps
import current data twice a day
allow colleagues to use filters to customize views
purchase Tableau desktop licenses for multiple team members
Replaced disparate analytics tools with Tableau
Conducted Tableau training sessions
Created comprehensive best practice guides
Implemented peer-review process for dashboards
opt for a structured data source
construct or enrich data marts upstream
set up a team to manage and publish indicators
rely on Tableau partner Actinvision for integration and support
Used Tableau Desktop for development
Used Tableau Server for secure access
Implemented Integrated Reporting and Analytic Metrics (iRAM)
Automated data processes
Consolidating data sources
Creating a dedicated dashboard
Automating reporting processes
Investing in a new data warehouse
Adopting Tableau for analytics and visualisation
Centralising all data
"Teaming up with IT service management partner, InterWorks"
Developing a new overarching BI function
Providing bespoke Tableau training using real NHS data
adopted Tableau for self-service analytics
connected disparate data sources
organic growth of Tableau through user advocacy
embedded dashboards in web portal
Automating data refreshes
Creating mobile dashboards for field use
Collaborating between IT and business teams
Mapping point-of-sale data
Animating sales data over time
Using dashboards for monitoring
Combining point-of-sale and guest experience data
Using Tableau Server for automated dashboarding
Connecting to multiple data sources
Blending data from different sources
Using Tableau Desktop for large data sets
Deploying Tableau Server for wider company use
Embedding Tableau outputs in reports and presentations
Implemented Tableau for fast and accurate data insights
Support from The Information Lab during initial rollout phase
Creation of comprehensive online training courses
Decommissioned several other BI platforms and replaced them with Tableau Cloud
Used Tableau Blueprint to guide their deployment strategy
Dispersed data leaders across business units
Created a new organizational model aligning resources to business areas
Focused on internal community engagement activities
Data extraction and reduction
Building dashboards in Tableau
Planning sessions with stakeholders
Use of geo-spatial mapping features
Developed a system-wide data and analytics strategy
Created new dashboards in Tableau for executives and leadership
Shifted focus to the creation of sustainable new tools
Identified universal problems that could be solved with data
Created unified views to highlight best practices
Built dashboards accessible to the entire hospital system
Tested Tableau integration with EMR environment
Gained corporate leadership support from the beginning
Implemented Tableau Cloud in 2017
Adopted Tableau Server and Tableau Prep Conductor
turned to Tableau
sorted through data to find meaning
adopted Tableau as its embedded analytics platform
user-driven design
pairing clinical expertise with data-driven insights
joint ideation and development
Adopted Tableau and Tableau Prep
Centralized data from more than 100 sources using ChannelMix by Alight Analytics
"Created a scalable, repeatable, cost-effective, and automated reporting infrastructure"
upgrading technology
upskilling employees
using Tableau Cloud and Snowflake for data integration
Moving to Tableau Cloud
Creating a migration and governance roadmap
Conducting education sessions with Tableau Prep tool
Leveraging educational materials via Tableau Blueprint
Leveraging Tableau's robust capabilities
Creating tailored dashboards
Early customization at scale
Switch from spreadsheets to Tableau
Integration of Tableau Server across the organization
Deploy Tableau to connect and visualize data
Work with Tableau consulting and implementation partner Uneeops
Create dashboards without coding
Training users on how to use the solution through global webinars
Working with a focus group to capture feedback and adapt the solution
Developed close partner relationships based on data analysis production
Empowered more users with self-service dashboards in Tableau
Built new models that align better with business needs
Leveraged real-time internal and external data for critical insights
Realigned data strategy
Empowered analysts within business units
Collaborated with human resources to build structured approach
Centralized and scaled data analytics education
Revamped internal core system
Consolidated scattered internal data
Implemented Tableau for data visualization
Switch to an all-electric portfolio by 2030
Commitment to effective digital transformation
Use Tableau for data visualisation
Focus on data literacy
Integrate customer data sources in Salesforce and Tableau
Migrated to Tableau Server running on AWS
Selected Commerce Cloud from Salesforce as its retail CRM
Established a Tableau Center of Enablement (CoE)
Engaged in department-focused quarterly projects and 1-on-1 sessions for training
Creation of Data College and DANCU (Data Analytics and Culture Unit)
Formation of internal Tableau user group 'tablab'
leveraging Tableau's embedded analytics platform
using Tableau’s APIs and developer tools to extend and enrich capabilities
automating onboarding process for new users
Developed its own Data Viz curriculum leveraging Tableau
Students encouraged to work with real datasets that relate to issues they're passionate about
use of Salesforce CRM products
Tableau Embedded Analytics for self-service analytics
Implementing Tableau Server in 2012
Migrating to Tableau Cloud in 2020
Using Tableau Blueprint assessment framework
Establishing a central data success team
Creating an analytics center of excellence
Launching Tableau in 2020
Implementing Tableau Embedded Analytics
Role-based landing page
Partnering with Naya Technologies
Reorganization of reporting activities initiated by top management
Training users to build confidence in the data and tool
Creating reusable data models
Creation of Digital Business And Transformation team (DBAT)
Setting up a data lab
Use of Tableau for data analytics and visualization
Role-based access control for data protection
Partnering with Tableau
Using Tableau products and expertise
Integrating visual analytics into vulnerability assessments
Shaping efforts around better uses of data
Using Tableau Server
Establishment of a Digital Transformation Office (DTO)
Use of Tableau for data management and visualization
Started with a small data analytics project in the telesales department
Rolled out the platform to other departments following success
Adopted Tableau for its ease of use and accessibility on any device
Leveraging Salesforce and Tableau across multiple business units
Developing a Group catalogue with skills assigned to each role
Scaling the project from the Italian region to Group level
Incorporated Tableau for self-service embedded analytics
Utilized Tableau Server for data integration and user privilege management
Collaborated with Tableau implementation partner for quick deployment
Deployed Tableau Server and moved to Tableau Cloud
Engaged Tableau technical support
Automated data processes
Rolling out Tableau within IT and using it to create dashboards for the business
Building data literacy across the organization
Two-month trial period to evaluate tools
Four-month migration phase with a dedicated 20-person team
Creation of 50 documents and an extensive FAQ for guidance
Dedicated Slack channel for support
Deployed Tableau to one division before rolling it out across the company
Set up a Center of Excellence (CoE)
Offered monthly training sessions
Introduced the Skill Belt program
Brought training online
creation of a single dashboard for the sales team
training and introduction of more dashboards
building a centralized data platform
Deployment of Tableau for self-service BI
Implementation of Tableau Bridge for data visualization
Optimization of parts inventory using Tableau
Top leadership championed
Partnership with Metrodata Group
Deploying Tableau
Connecting multiple systems to Google BigQuery
Providing platform support
Developing and refining Tableau visualizations
Overlaying social determinants of health factors
Integrating technology across departments
Data governance
"Cross-functional, data-driven initiatives"
Real-time data discussions
Predictive analytics and logic
Integration with Tableau
Creation of dashboards
Collaboration with Mid-Ohio Food Collective
Participation in Tableau Foundation data fellowship program
Implementing Tableau analytics platform
Creating dozens of basic reports
Providing granular reports on orders processed
Creating a comprehensive training program
Providing self-service tools and training environments
Holding regular hands-on seminars
Deploying dashboards company-wide
Implemented Tableau Online in 2017
Integrated multiple data sources
Created visualizations and provided access to a limited number of users initially
Scaled adoption to nearly all departments
Regular training and internal wiki resources
Integrated Tableau with Slack
Implemented Tableau Server and Tableau Desktop
Integrated data from multiple sources
Leveraged Tableau’s native integration with Amazon Redshift and Snowflake
Conducted regular training sessions
Created internal support group 'Tableau users'
Adopting Tableau for data visualization
Creating the Prometeia Mio portal
"Working with Tableau partner, Visualitics"
Rolled out Tableau to nominated stakeholders
Created videos and starter packs of reports
Provided training and support via implementation partner DXC
Nominated champions to test the solution
Using Tableau Cloud to unlock insights
Building dashboards for visualising data
Choosing Tableau embedded analytics rather than building a custom solution
Preparing multiple dashboards for specific metrics
Expanding to nearly thirty dashboards for a holistic performance view
Transform years of accumulated data into meaningful knowledge
Empower management with data-based decision-making tools
Set up a responsive BI portal accessible on mobile devices
Consulted the 2019 Gartner Magic Quadrant for Business Intelligence and Analytics Platforms
Contacted organizations in the 'leaders' quadrant
Tableau Blueprint to validate key learnings and embrace self-service analytics
Centralizing technology under one group
Using Salesforce Financial Services Cloud as the CRM foundation
Embedding dashboards from CRM Analytics
Using Salesforce technologies like MuleSoft and Slack
Selected Tableau as the tool of choice in June 2019
Launched MTableau project to consolidate Tableau instances
Centralized data repository
Alignment with business and IT stakeholders
Adoption metrics and user engagement
Adopting Tableau for robust analytics and data visualization
"Implementing Salesforce Customer 360 solution including Sales Cloud, Marketing Cloud, and NPSP"
Appointed team members to lead implementation
Comprehensive plan for training and adoption
Monthly training sessions and homework assignments
Executive sponsorship and leading by example
"Investing in digital, technological, and data-driven future"
Redeploying on-premises data infrastructure to the deployment
Equipping employees with necessary tools
Integrating analytics into business decision making
Creating awareness about the benefits and power of business intelligence among senior leaders
Promoting success stories combined with structured training programs
Implemented Tableau dashboards
Centralized data with a new ERP integration
Automated regular weekly reporting
Consolidation of diverse analytics tools
Migration to Tableau
Focus on department-specific needs
Proactive and preventive decision-making
use of Tableau for data visualization
cultivating a focus on data analytics combined with actionable insights and technology
pivoted to an Agile way of working
daily standups to refine data management
centralize analysis and reporting
onboarding leaders first
transitioning ownership to divisional and departmental MIS teams
"partnering with VISIDATA for implementation, server setup, and training"
Landscape analysis
Stakeholder and power mapping
Data engagement and advocacy
Incorporating Emergency Rental Assistance data from the US Treasury
Digital transformation initiative
Agile transformation
Implementation of Tableau with btProvider specialists
Standardized on Tableau embedded analytics
Moved data management in-house
Leveraged Biztory for development of visualizations
Implemented Tableau with the support of partner Infotopics
Training staff to use Tableau for data visualization
Reprogramming systems to better fit client needs
Creating a quality care dashboard to monitor client data
Use of Tableau embedded analytics
Development of E-Workbook and Polar platform
"Focus on Workflow, Integration, and Insights"
Adopting Tableau for data visualization and access
Engaging BAC for on-site training
Leveraging Tableau community resources and training videos
hiring people who believe in data
mining data for new insights
using insights to build new products and services
Full restructuring of SFA
Building data lakes
Introducing BI products as a data usage platform
Adopting Tableau for company-wide BI platform
Partnered with Oak Consulting to implement Tableau
Built dashboards to visualize data
Trialing Tableau to ensure it matched employees' needs
Internal engagement campaign promoting data as a valuable asset
"Partnering with Bara Advanced Infotech Co., Ltd"
Developing Tableau training and delivering user training at each implementation phase
Conducting a Tableau dashboard roadshow
Robust governance framework
Creation of an Analytics Centre of Excellence
Switching to a Software-as-a-Service (SaaS) model with Tableau Cloud
"Formation of a digital centre of excellence, InDigital"
"Adopted Tableau Desktop, Tableau Prep, and Tableau Server"
Built an environment using AWS
Conducted internal promotion to increase the number of users
Created a roadmap based on Tableau Blueprint
Provided official training programs hosted by Tableau
Standardizing data and KPIs
Adopting common definitions and formulas for indicators
Creating visual dashboards
Training and coaching business analysts
Centralizing data in cloud storage
Using Tableau for data analysis and visualization
Developed a plan for sharing information
Engaged outside consultants for guidance
Selected Tableau from three product suggestions
"Underwent training with Tableau Software, Inc."
Engaged Billigence to support implementation
Multiple training sessions and enablement workshops
Internal meetings to break down data silos and agree on metric definitions
Creating a formal data analytics group
Using Tableau Cloud as the foundation
Developing and tweaking dashboards on desktop or phone
Training all employees to ensure data consistency
Standardized on Tableau visual analytics platform
"Connected all managers to the same, shared, single view of data"
Empowered users with self-service analytics
Building interactive dashboards in Tableau
Transition from manual reports to real-time data dashboards
Data visualization technology
Predictive analytics
Leveraging existing infrastructure
Telehealth program expansion
Onboarding BI resources through a massive community of users
Frequent training sessions and weekly user groups
Deployment of Tableau Server in a couple of days
Leveraged existing university data
Developed new metrics like 'Non-awards Scholarly Research Index'
Collaborated with external firms
creating a data analytics community
leveraging Tableau Desktop and Tableau Server
partnering with existing groups
executive sponsorship
Modernized Business Intelligence
Self-service BI
Data culture establishment
Created Data Academy
partnership between customers and Tableau partners
Rebuilding and expanding the internal data team
Using a consultant to set up Tableau Server
Creating dashboards for various use cases
Building in-house data science and insights capabilities
Using Tableau for data visualization and access
Implementing a tool that can be used with Google BigQuery
Starting small to measure ROI
"Deploying Tableau in phases, starting with the Support Team"
Transition from Excel VBA to Tableau
Use of Tableau Desktop and Tableau Server
"Combining log data, process performance data, and product information data into a single database"
offering professional services to help customers define a strategy
"using Tableau to simplify collecting, aggregating, and visualising data"
providing access to data scientists and analytics capabilities
Developed more than 150 dashboards with Tableau
Focused on reducing complexity
Ran live online events for training
Promoted new features and dashboards
Created a portal on the university’s intranet
embedding Tableau into CIP solution
working with Goldstone Technologies
connecting Tableau to PostgreSQL database
running custom SQL
setting up new dashboards
"Rollout of the Sysco e-commerce app, Sysco Shop"
Formation of a product analytics team
Using Tableau to visualize order activity
Event tagging in Sysco Shop to capture and analyze business data
"Trainings, webinars, and recorded guidance for field reps"
use of Tableau for analytics
transition to Tableau Cloud
Implementing a customer-focused approach to service
Developing a social network platform to identify organized fraud cases
Replacing central fraud platform with inhouse developed fraud application
Using Tableau dashboards for data visualization
Centralizing systems through Basware’s P2P suite
Using Tableau embedded analytics
Integrating Basware Analytics into Basware’s P2P suite
Mandated a new data analytics program
Used Tableau as a central teaching tool
Started with a small 4-week Tableau module
Developed an undergraduate minor program in Data Analytics
Integration with graduate programs
Offered Tableau Desktop Specialist certification
Embedded analytics from Tableau
Automatic refresh of dashboards
Training session with customers
Introduction of Tableau in sync with Care8 DNA
Tableau Basic Training and online training sessions
competitive tendering
implementation of Tableau
communication and awareness-raising activities
hackathons and workshops
Support from management
Investment in the right technology
Open-minded approach from business users
Bridge builders for training and enablement
Fortnightly Fast Start Workshops
Advanced Workshops for power users
Templates for routine reporting and analysis
Participation in Tableau Data Doctor session
Migrating to Tableau Cloud
"Using a straightforward, four-step migration process"
Designing and following strict protocols and best practices for ongoing governance
using Tableau as the main source of truth for key business metrics
creating one key metric dashboard for all stakeholders
Evaluation of several BI solutions
Partnering with local expert G-able
Conducting workshops and training
Installing Tableau as the overarching visual analytics solution
Creating a single intuitive analytics platform
Focusing on back-end infrastructure
Top-down and bottom-up approaches for Tableau deployment
Hearing the needs of business divisions and offering a suited methodology
Publishing in-house DX case studies to promote use
Involving future users in tool evaluation and selection
Initial wave of Tableau training called Data Rockstars
Gamification strategy to motivate participation
Collaborating with educational firms like Udacity
Executive-level training for leaders
introducing Tableau in 2015
extending deployment and promoting adoption since 2018
"support from DKM ECO, a Tableau distributor"
Establish a Tableau Centre of Excellence
Hire a team of BI experts
Set up a private cloud infrastructure
Create a data mart
Set up a governance framework
"Provide training for 3,000 employees"
Use of Tableau for analytics
Centralized data platform
Enablement plan across the organization
Worked with each department to create simple dashboards
Training sessions once a quarter
Advanced training courses for deeper learning
using Tableau to trust data as a strategic asset
Tableau Blueprint methodology
creating a flourishing data community
"migrating multiple Tableau instances to a single, enterprise-wide instance"
Using Tableau for data visualization
Developing real-time visualizations
Consolidating data from various regions
Introducing Tableau for data analysis and visualization
Running internal training alongside Montage
Developing employee training sessions
Conducting weekly Q&A sessions
Supporting power users in various departments
Demonstrated capabilities of Tableau to the senior leadership team
Developed a series of Tableau dashboards in a short time
Switched to Curator from Interworks for a customized user experience
Roll out Tableau for broad visualization capabilities
Engage people through workshops
Provide dashboards refreshed automatically from data warehouse
Standardized various kinds of data
Established consistent standards for data-based collaboration
Built integrated data governance system
Connected Tableau to the enterprise-wide data warehouse
Deployment of Tableau to build a unified big data platform
"Integration with systems like SALIENT, MARGIN MINDER, SAP ERP, CRM, ODS"
Adoption started from a single department and expanded
assimilate data
move data to the cloud with Google Cloud Platform
added Tableau Cloud
supported rollout of Tableau by Team Computers
use dashboards to track performance metrics
Using Tableau from day one
Embedding Tableau analytics
Capitalizing on data opportunities
Incubating innovative new growth services
Use of multiple Tableau dashboards
Integration of Tableau COVID-19 Data Hub
Creation of new COVID-19 impact dashboards
Prototyping innovative solutions like heat maps
Brought in Tableau based on previous experience
"Rolled out Tableau with the help of PC Data Visidata Anugerah Mitra, PT"
Integration of Tableau with ZIGBANG RED
Use of Tableau’s drag and drop functionality
Beta testing of services
Building a modern and sustainable data warehouse
Strategic cooperation with Data Inc.
Implementing Tableau for fast and effective data delivery
consolidated data into a new enterprise data warehouse (EDW)
built a series of starter dashboards in Tableau
leveraged Tableau Cloud (powered by AWS) for dashboard access
Application of change management theory
Identification of change champions
Integration of a visual data analytics platform
Cross-functional project teams
Implemented Tableau
Utilized Tableau's flexibility and intuitive interface
Blended performance data from multiple sources
Developed custom workbooks
Building an internal data culture one department at a time
Using Tableau for visual analytics and reporting
Working with Tableau partner Infotopics for management and training
Implementing Scrum methodology for regular realignment and adjustments
Adopting Tableau for easier data visualization
Using Tableau Prep Builder to automate data flows
Implementing Tableau Prep Conductor for scheduling and monitoring data flows
Using Tableau Catalog’s data lineage feature for error resolution
Implemented visualization-based self-service BI
Expanded Tableau across the whole organization
Implemented enterprise-wide data-driven decision-making system
Centralizing everything through Tableau Cloud
Using Tableau Bridge for single reporting process
Running comprehensive training webinars for end-users
Choosing Tableau for its visualization capabilities and ease of use
Moving from Tableau Server to Tableau Cloud for scalable processing power and automatic upgrades
Traditional RFP process
Presentation to executives with real-time data interaction using Tableau Cloud
Promoted adoption through interest and excitement within teams
Efficient license allocation matching user's needs
Adopted Tableau Server for a new data warehouse.
Transitioned to Tableau Cloud to support automatic upgrades and streamlined reporting.
Piloted a program using Tableau in 2018
Shifted from manual reporting to interactive analysis
Implemented 30-minute refresh cycles for real-time data updates
Reduction of dashboards from over 100 to 25
Unified suite of visual dashboards
Integration of Tableau within Salesforce
Use of MuleSoft for data integration
Automating data processes
"Using SQL server databases, Alteryx, and Tableau for data staging, prepping, and visualization"
Refreshing data at the click of a button
Using visual analytics and mapping
Using Tableau for data insights
Creating a custom mobile analytics app
Providing round-the-clock access to data
Launching pilot projects quickly
Aggregate and visualise analysis of billions of rows of data
Integration with more than 30 data sources
Adopted Tableau for data analysis
Implemented real-time dashboards
Real-time data tracking and monitoring
Consolidated data into one unified system
Deployed Tableau as a continuation of the data-centric strategy
Internal training and contracting with companies
Hired the head of the Tableau User Group France
Implementing Tableau dashboards
Changing employee mindset towards data
Activated systemwide emergency response
"Assembled a 'tiger team' including operations, logistics, medical and technical specialists, and hospital leadership"
Deploying Tableau Server solution
Utilizing Tableau's intuitive user interface
Deploying Tableau in an Amazon Web Services (AWS) environment
Leveraging pre-existing internal development knowledge
Use of Tableau dashboards and analytics
Embedded analytics solution (Trender)
"Leveraging Tableau's embedded analytics platform with in-house fashion experts, marketers, and data analysts"
Moved from spreadsheets to Tableau for faster insights and more engaging presentations
Set up real-time monitoring and alerts for specific issues such as flooding
Developed PurpleCloud Covid Response (CR) platform
Incorporated Ecolab and CDC guidelines
Engaging local communities
Building the Global Health Data Analytics Hub with Tableau
Creating the enterprise-wide Data Fellowship program
Developing Tableau self-service dashboards
Pursuing a data education program
Partnership with Tableau
Development of data governance unit and data committee
Implementation of data analytics culture
Integrated visual analytics with Tableau
Pre-built eCRFs based on WHO standards
Real-time data retrieval through Castor’s API
"Embedding Tableau’s interactive, analytical capabilities directly into its portal"
Providing an easy-to-use solution for mobile operators
Building a service based on Salesforce.org's package for NGOs
Using Tableau for dashboard visualization
Scaling volunteer numbers from 5 to 30
Creating new account types for different user roles
Initial deployment driven by sports science
Centralized analytics team
Connecting key player databases
Adopting Tableau for data visualization
Conducting in-house orientation classes for Tableau adoption
Creating new Tableau dashboards to track telehealth change
Using Tableau for flexible and easy-to-use analytics
Automating reporting processes
Implementing Tableau champions in every department
Introduced Tableau in 2015 during the MERS crisis
Developed dashboards for data visualization and analysis
"Integrated data from multiple sources like KCDC, NEDIS, National Health Insurance Service"
Worked alongside Tableau partner Targa Consult and Ooredoo
Created dashboards in just 24 hours
Putting trusted data at the heart of the organization
Single ‘data universe’ for employees
Using Tableau for internal reporting
Introduction of SAP HANA in-memory database
Deployment of Tableau for data visualization
Top-down promotion of Tableau
Establishment of 'Ambassador Academy' for user training
Creation of a non-commercial map (Essential Supplies Exchange)
"Collaboration with other companies (Numer8, KrishiHub, ThinkAg)"
quickly create new dashboards
"partner with IT, Infection Control, ICU clinicians, and operational leadership"
leverage existing data infrastructure
Connecting factories worldwide to a central data repository
Migrating to Tableau for advanced data centralization
adoption of Tableau
creation of a single source of truth with financial and operations data
development of dashboards for performance tracking
Adoption of Tableau Advanced Management and Content Migration Tool
Standardizing code bases and data architecture
Dynamic default settings and value swaps in workbooks
Set up a task force headed by senior leaders
Created a sub-task force dedicated to employee needs
Set up a dashboard in Tableau to visualize employee health and safety
using Tableau to connect to all data sources
"creating a unified, single source of truth for data"
Upgraded technology
Built a data warehouse
Replaced manual reports with Tableau dashboards
Moved from on-premise to cloud with AWS
Introduced a rules engine on top of AWS
Replacing disparate analytics tools with Tableau
Running Tableau training sessions
Creating comprehensive best practice guides
Built a new data warehouse engineered for self-service analytics
Standardized on Tableau Server as the analysis layer
relying on already structured data sources
constructing/enriching data marts
engaging Actinvision for support and training
change management process
Adopted Tableau for data centralization
Implemented Tableau Blueprint for a structured approach
Use of Tableau for visualisation and analysis
Development of health index algorithm
Adopting Tableau for superior visual analytics and ease of sharing
Connecting to various data warehouses both on-premise and in the cloud
Providing internal training sessions on Tableau regularly
Creating an environment for general users to build reports easily
Created dashboards using Tableau Cloud
Developed personalized learning profiles with Tableau
consolidation of data sources
creating a dedicated dashboard
importing performance and learning data into Tableau
automating nearly all reporting
Using Tableau and Tableau Blueprint
Reimagining data-related roles
Implementing a support model
"Emphasizing data management, certification, and citizen development"
use of Tableau Desktop for development
use of Tableau Server for secure access to information and dashboards
implementation of Integrated Reporting and Analytic Metrics (iRAM) service
chose Tableau after analysing multiple solutions
standardised on Tableau throughout the organisation
consolidated Microsoft SQL Server data warehouse
Implementing Tableau Cloud for dashboard creation
Using test and control methodology for media incrementality
Partnering with Tableau to leverage existing client familiarity
Creating a shortlist of potential analytics solutions
Involving business champions to identify the best solution
Tackling a current data use case in each shortlisted solution
Leveraged Tableau Blueprint
Server reboot focusing on content management
Created a sandbox environment for administrators
Established a Tableau Champions Group
Spearheaded training programs
Consolidate 100+ data sources into a central platform
Overhaul corporate data warehouse
Enrich databases with new data sources and quality assurance procedures
Implement Alteryx and EasyMorph for data preparation and extraction
Promote Tableau as the primary data analytics tool
Evaluated multiple BI tools
Implemented Tableau Desktop for development
Used Tableau Reader for deployment
Started using Tableau Embedded for direct customer visualizations
Implementing a cloud-based data lake hosted in AWS
Using Tableau as the sole analytics and visualization platform
Standardized on Tableau analytics platform
Integration of Salesforce application with Tableau
Interactive maps for collaboration and transparency
Replacing outdated tools with Tableau and Alteryx
Installing Tableau Desktop and Server
Using Alteryx for data prep and enrichment
Establish one data repository for all purchase order and invoice transactions
Create an infrastructure to support and visualize analysis with Tableau
Integrate data to a central repository
Develop an agile analytics model
Centralised all data through one platform (Tableau)
Conducted a major analysis to choose the best tool
Embedded Tableau in Salesforce
Using dozens of predictive data models from DataRobot
Deploying nearly 200 Tableau dashboards
Analyzing variables that correlate with student success
Using Tableau Server to connect and integrate data sources
Adopt Tableau
Introduce various data analytics and visualization programs
Host annual AIS Analytics Day
Use of Tableau for interactive graphics and evaluations
Geo-referencing of mines on maps
Adopted Tableau to help clients make sense of and manage information
Customized data use according to each client's needs
Swapped presentations for creative workshops
Using Tableau to organize customer information
Developing a machine-learning model
Replaced data warehouse approach with 'Atonyx' data platform
Use of Tableau for real-time market intelligence and analytics
Agile approach to data reporting and analytics
Adoption of Tableau Desktop and Tableau Server in stages
Transformation of Tableau from a report analysis tool to a decision support platform
Introduction of Tableau in 2016
Installation of magnetic sensors every 200 meters in every lane
Implementation of Azure cloud-based data warehouse
Selection and connection of Tableau as the primary analytics platform
Use of Tableau Prep for data cleaning and testing
Adopting Tableau as the sole data analytics and visualization tool
Replacing existing monolithic system with a 100% cloud-based system on AWS
develop advanced dashboards using Tableau
integrate data from Google Analytics and Salesforce
Extensive assessment of leading analytics platforms
Implementation of Tableau in 2013
Development of a proprietary cloud-based platform
Migration of all company data to Snowflake and Fivetran
Trialled several analytics platforms and selected Tableau
adopting Tableau and data analytics
creating a dashboard combining different data sources into a single source of truth
Used Tableau to unify all disparate databases
Enabled self-service analytics for employees
detailed monitoring of key business metrics
beautiful and clear visualisations
encouraging self-service analytics
"centralised, streamlined dashboard"
Implemented Tableau as the visual analytics platform
"Combined financial transactions, customer, provider, treatment, and other data in Tableau dashboards"
Implemented two Tableau Server instances for internal analysts and ACOs
Adoption of Tableau as a data analytics platform
Utilizing Tableau’s drag-and-drop interface and ease of use
Adoption of Tableau as a one-stop-shop for workforce data
Automated daily refresh of workforce data
Optimized monthly reports with dashboards
Introduced Tableau across subsidiaries
Partnered with VISIDATA
Roll out of Tableau as a universal reporting tool
Assessment phase with evaluation of four different tools
Purchase of Tableau Server and 100 Tableau Creator licenses
Introductory training from a realization service provider
partnership with tech consulting company Cybertrend
initial stages of planning and Tableau implementation
training more 'Tableau Champions'
Invested in a Microsoft SQL data warehouse
Adopted Tableau for analytics and visualization
Partnered with InterWorks for IT service management
Centralized all data from the wider system
Proof of concept carried out across various tools
Selection of Tableau as the ideal solution
Standardization across the company post-acquisition
Construction of a new reporting data warehouse
Utilizing a single Tableau dashboard connected to the data warehouse
"Combining Tableau tools, applications, and dashboards"
Implementing a 16-core Tableau deployment
"Using additional Tableau tools like TabMon, TabJolt, and LogShark"
"Setting up disaster recovery, test, and utilities servers"
Integrating nine back-end systems into a single data warehouse
Building a data lake with Oak Consulting
Using Tableau for real-time reporting
Running a center of excellence to train and build a community of users
Combining data from different sources
Adapting existing templates for local markets
Central data platform
Intuitive Tableau visualizations
Deployed Tableau as an enterprise-wide visual analytics tool
Created real-time dashboards for monitoring
integrated Tableau’s hosted cloud analytics solution
hired an independent consultant to minimize costs and staffing impact
developed more than 180 dashboards
Data infrastructure overhaul
Adopting a data lake
Choosing Tableau for visualization and analytics
Centralizing data and analytics
Contacted Tableau to simplify data presentation
Implemented Tableau-powered BI solutions
Developed interactive dashboards
Used Tableau Cloud and Tableau workbooks
Integrating many data sources
Building beautiful dashboards from large datasets
integrating several tools with Wipro’s data discovery platform
using Tableau as the key visualization layer
consolidated Tableau dashboard
MySales Branch Analytics Reporting
centralizing data with Tableau
"creating official templates, color palettes, and fonts"
connecting live data to visualizations
Implementation of Tableau in 2017
Creation of a centralized operational dashboard called Pulse
"Integration with key applications like timesheet and attendance tracking, CRM, HR, customer pipeline, and surveys"
Collaboration with tech partner KeyData
Use of Tableau to make sense of user data across different regions
Transforming insights on consumer preferences into meaningful and effective content
Consolidated all online business data into a central data warehouse
Implemented Tableau for visualization and analytics needs
Encouraged self-service analytics among employees
Scaling from 11 licenses to more than 200 in less than two years
Establishing a strong internal support system for Tableau users
Evaluating 14 tools and selecting Tableau
Recommending the right BI solutions
Supporting clients in data analytics
Using Tableau to visualize data
Building a data-driven culture
Adoption of Tableau in 2014
Creation of uniform data sets on key national health target areas
Development of custom dashboards
Integration of HES Browser for self-service analytics
Creating visualizations and sharing key dashboards on Tableau Server
Ensuring back-end security measures to protect customer data
Connecting data through Tableau
Conducting business wide research to understand data needs
Establishing priorities and KPIs
Providing cross-divisional access to key data
Establishing Comair’s Data Stewards Programme
Integrating Tableau with existing databases and sources
System-integrated partners helped implement the system within three months
Started with the finance department
Scaled quickly across multiple departments
Adoption of Tableau Desktop by specialist group of analysts
Adoption of Tableau Server for controlled data publishing
Assignment of domain stewards for accountability and quality control
Chose Tableau to accommodate needs of analysts and data consumers
Maintained security of patient information
Use Tableau for identifying key metrics
Shift to Tableau Server for queries
Combine product and customer data for insights
Adopt Tableau for visual analytics
Use Tableau to create over 100 indicators
Share insights with the whole company
Online and offline training for new users
"Balancing empowerment, visibility, and governance"
Combining web-based data with offline data
Investing in employee training
Involving employees in the decision-making process
using Tableau Cloud on Amazon Web Services
automatic analysis of in-app behavior
Deployed Tableau for data visualization
Embedded Tableau into AS Watson’s internal platform (WatsonView)
Promoting a culture of self-service analytics
Quickly identified Tableau for its ease of use
Direct integration with data sources
Embedded 'super users' in each department for training
Developing 40 key performance indicators (KPIs)
Using Tableau to develop dashboards populated with data from Microsoft CRM system
Instituting strong data governance standards
Expanding their centralized data repository
Shift from IT-owned to business-owned self-service analytics
Adopting Tableau as a platform
Establishing a Center of Excellence (COE)
Training new developers and analysts
Creating certified self-service clusters
Using Tableau to connect directly to internal data sources
Adoption driven by the Managing Director with prior experience in Tableau
Training more staff to use Tableau
Setting up a comprehensive data warehouse based on Amazon Redshift
Using Tableau for data visualization
Creation of e-business team
Use of Tableau to blend data from different sources
Automating reporting processes
Self-service data visualizations
Centralizing the reporting
Adoption of Tableau for self-service analytics
Development of mini production areas
Socializing Tableau experience
Embedding dashboards in Southwest’s web portal
Implemented Tableau for self-service reporting
Connected various data sources to Tableau
Focused on user-friendliness
Adoption of Tableau for data consolidation and integration
Implementation of function-specific dashboards
Commencement of a data centralization project
Comprehensive training program for almost all employees
Implementation of the Tableau platform
Enterprise-wide transformation exercise
Implementation of 'Sysco Ecosystem for Enterprise Data' (SEED)
Adoption of Tableau on AWS
Adoption of Tableau across the organization
Collaborative relationship between business units and IT
Consolidating data from multiple sources
Exploratory analysis in Tableau
adopted Tableau for data visualization and sharing
create detailed data visualizations
connect all key data sources through Tableau
Expanding Tableau licenses
Transforming the COE model to be more transparent
Creating automated dashboards
Setting up secure data sources
Instituting an internal user group
introducing Tableau
data blending
custom Tableau workbooks
Building dashboards to visualize KPIs
Building dashboards for strategy management
Using Tableau for ad-hoc analysis
consolidate data sources
use of Tableau Server
Implemented Tableau Desktop in the marketing and communications department
Used Tableau Reader to distribute dashboard results
Acquired Tableau Server licenses for power users
"Adopting Tableau for fast, accurate analysis"
Connecting multiple data sources in one central location
Using interactive visualizations for data accessibility
Democratizing company data
Empowering employees with self-service analytics
Providing a business intelligence platform open to all employees
Hosting internal webinars and requesting feedback
Worked with Tableau partner Woodmark to refresh data architecture
Developed a set of Tableau dashboards
Adopted Tableau for agile analytics
Using Hadoop data lake for data storage
Optimized dashboards for mobile devices
Deployment of Tableau platform on a company-wide scale
Centralization of data sources into a data warehouse using ETL processes
Creation of KPI dashboards
Automatic refreshes and subscriptions in Tableau Server
Centralizing all incoming data in one location
Using Tableau to analyze both structured and unstructured data
Delivering insights directly to customers via web portals
Adoption of Tableau Desktop and Server
Enablement of real-time sales performance monitoring
Reduction of manual data extraction and blending
Adopted Tableau platform
Developed Chief Minister’s dashboard application
Partnering with Alight Analytics and using ChannelMix
Using Tableau Cloud for dashboard delivery
Integrating data from sentiment tracking tools like IQ Media and Brandwatch
adoption of Tableau and Tableau Server
partnership with PLANIT for expertise
utilizing dashboards for weekly public updates
building a data warehouse and portal service
"conducting training, guidance, and consulting services"
Adopted Tableau Desktop and Tableau Server
Created environment for diverse analyses and safely distribute dashboards
Planit provided training to sales and IT personnel
Integrated various data into assets
Adoption of Tableau for data analysis and collaboration
Utilizing Tableau Mobile for the sales team
Creating dashboards for business users and developers
Implementation of Tableau
Connecting to the data warehouse
Distributing Tableau dashboards to operational managers
Adopted Tableau Server
Centralized disparate data into Tableau Server
Created customer success dashboards
Embedded Tableau Server into internal analytics portal 'Merlin'
Adopting a subscription model
Starting small and scaling with demand
Leveraging existing technology investments
Establishing an analytics center of excellence
Evaluating various business intelligence solutions
Choosing Tableau based on previous successful implementations
Accelerating the process to introduce Tableau to other departments
Adoption of Tableau as analytics platform
Installation provided by local reseller Advectas
Acquired Tableau Desktop and Server licenses
Adoption of Tableau
Integration with Amazon Web Services and Hortonworks Hadoop Hive
Single sign-on integration with Tableau Server
Unified customer intelligence platform (LUCI Sky)
started as a pilot project within a key business area
implemented dashboards to track key performance indicators
Adoption of Alation’s Data Catalog alongside Tableau
Automated alert creation to ensure proper data loads
Uniform application of business rules at processing time
Adoption of Tableau in early 2016
Evaluation of ten competitors
Training and ongoing support for employees
Implementing Tableau for self-service BI
Allowing users to pull data from Hadoop into Tableau
embedding Tableau into SaaS solution
providing initial installation and training support through local reseller
using Tableau’s online training materials
Started with a few Tableau Desktop licenses
Migrated to Tableau Server
Connecting Tableau to Amazon Redshift
Started with four Tableau Desktop licenses in the statistics lab
Adopted Tableau Server to enhance data governance
Use of native data connectors
Collaborative process in report development
Adjust dashboards as priorities or needs change
Formed a new People Analytics team
Combined relevant data fields from multiple systems
Created 12 interactive dashboards across multiple categories
Embedding Tableau Server into the Zedi AccessTM platform
Adopting Tableau Desktop and Server internally
Blending data from MS Dynamics and Salesforce
Creating core dashboards with the finance team
Use of Tableau's mapping capabilities
Purchasing ten Tableau Desktop licenses
Developing a set of dashboards
Using Tableau’s native connectors
Automatic data refreshes in Tableau Server
Creation of mobile dashboards
Partnership between business intelligence team and IT
Customizing dashboards for different user roles
"Evaluation of Tableau, Birst, and Qlik"
Initial use of the free trial version of Tableau for 14 days
Providing consultants and analysts with Tableau Desktop licenses
Conducting single introduction sessions for using Tableau Desktop
Using free Tableau training videos
Planning training sessions with Tableau Trainers
Use of smartphones to record data
Implementation of Dimagi's CommCare app
Prototyping and testing Tableau dashboards
Creation of EC Marketing Lab
Choosing Tableau for data clarity and multifaceted analysis
Use of Tableau software
Training and support for frontline health workers
Creation of dashboards and real-time tracking
Entity extraction for mapping
Use of Tableau Desktop for interactive visual analytics
Blending data from disparate sources
Sharing visualizations via Tableau Reader
Using outsourced vendors for data warehousing and ETL
Setting permissions and customizing dashboards for different users
partnered with QI ao Cubo
deployed Tableau
Incorporating hands-on Tableau training
Use of Tableau cases in coursework
"evaluated Tableau, Birst, and Qlik"
chose Tableau for ease of use
provided Tableau Desktop licenses
training sessions with Tableau Trainers
use of free Tableau training videos
Started with desktop licenses and Excel
Moved to SQL Server for scaling
Gradual expansion to other areas of the company
Introduced Tableau within the Office of Strategic Planning and Institutional Research
"Created DataND, a campus-wide data portal embedding Tableau Server"
Adopted Snowflake Elastic Data Warehouse for fundraising office
implemented internal user groups
held five training sessions two weeks apart
Using Tableau on top of Amazon Redshift data warehouse
Creating dashboards with assistance from Biztory team
Scaled Tableau Server across the company
"Interactive approach, making people interact with data at the business level"
Strong partnership with IT to build data connections and automation
IT department spearheaded the search for BI solution
Guidance from Gartner
Trialing Tableau by CIO and CEO
"Deployment with the help of local Tableau partner, Solutive"
Adoption of Tableau and Alteryx
"Scaling from zero to 9,000 Tableau users"
Embedded Tableau Server into product
OEM partnership with Tableau
Single sign-on (SSO) integration
Deployment of Tableau
"Support from Tableau partner, The Information Lab"
Basic training and creation of initial dashboards
Automating standard reports and ad-hoc questions
Deployment of Tableau Server and Desktop
Training help for users
Use of CommCare informatics system
"Tableau provided software, training, and financial support"
InterWorks provided pro-bono product training
Tableau Visionaries helped create dashboards
adopted Tableau Desktop and Server
rebuilt information stack including HP Vertica and Informatica
prioritized self-service analytics for business users
relationship-building
starting conversations with teachers and administrators
data visualization workshops
data walks
"Provided a grant of software, training, and financial support"
Created interactive dashboards
Collaborated with Tableau Service Corps volunteer
Developed storytelling tools
Highlighting certain results
Changing chart types
"Providing software, training, and funding to PATH’s teams"
Using Tableau to see and understand data in real-time
Partner consultation
Introductory and advanced training courses
Promoting collaborative analytics
Placing Tableau champions within each department
Mobilized network of education and social-service providers
Launched Data Fellowship program
Provided pro bono training through partners InterWorks and DecisionViz
Purchased five Desktop pro licenses and a Server license
Integrated with SAP database
Created KPI dashboard
Migrating to Tableau
"Connecting to an SQL interface, Salesforce, and LucaNet"
Using Tableau Desktop and Tableau Server
Installation of sensors to collect data
Automating data refresh
Purchased Tableau Desktop and Tableau Server
Regularly hosts data workshops
Use of Microsoft SQL Server and Excel files
Training sessions for management
Implementing Tableau Cloud
Starting with a pilot phase before full-scale deployment
Avoiding big infrastructure spend by choosing a cloud-based solution
Utilization of Tableau Desktop for integrating and analyzing datasets.
Deployment of Tableau Cloud for scalable and secure reporting.
Customization of reports to meet operators' specific metrics.
Implemented Tableau as a front end to their metrics platform
Enabled users to access Tableau Server and build their own reports
Adoption of Tableau Desktop and Tableau Server
Leveraging R in Tableau
Connecting to data both on-premises and in the cloud
Publishing reports for each department over Tableau Server
Using Tableau Desktop for data analysis
Using Tableau Server for larger environments
Integrating existing data sources quickly
Localizing workbooks for global analysts
set user permissions and govern what clients see
performed a case study on different software
quickly set up feedback on collected data
Using Cloudera Hadoop distribution in Microsoft’s Azure cloud
Connecting Tableau directly to Hadoop environment
Starting with a few Tableau licenses and expanding usage
Involving more people in data visualization
Partnered with ReSolt for implementation
Started with the retail branch to visualize partner profitability
Expanded Tableau to four business areas with plans for all departments
use of Tableau dashboards in executive presentations
Using a data warehouse to store historical information
Creating dashboards for goal and indicator monitoring
Offering a dedicated summer course centered around Tableau
Using Tableau in winter lectures to produce interactive demographic data files
Project seminars utilizing Tableau for real-world data analysis
Embedding Tableau Cloud into hetras product
Using Tableau Desktop and Tableau Cloud
Using Tableau Desktop to generate and build reports
Deploying reports using Tableau Cloud
Blending data from Amazon Redshift and Google Analytics
Trialing Tableau Cloud
Connecting data directly to Amazon Redshift
Using Tableau Desktop and Tableau Server
Employing Google BigQuery with Tableau and Python
Use of Tableau Academic Programs to provide free licenses to students
Use of Tableau for Teaching to provide cost-free introduction to Tableau
Use of Tableau Desktop
Experimentation with Tableau Server
Began searching for a suitable visualization tool
Adopted Tableau after a few hours of usage
Use of Tableau combined with Alteryx
Deploying management summaries for executives
Providing training resources available online
"Combining data from marketing, sales, and finance into a single view"
"Connecting to existing data sources like Salesforce, Google Analytics, Marketo, Oracle, Excel"
Proof of concept
Using maps for regional analysis
Partnering with Tableau for quick dashboard creation
Use of Tableau Public to develop community engagement
Creation of influential visualizations
Use Tableau Desktop’s native connector to Google BigQuery
Share workbooks over Tableau Cloud
Ad-hoc analysis
Native connectors
investment in analytics
evaluation using Gartner Magic Quadrant
choosing Tableau for its features
embedding analytics into portal
subscription licensing model
Visuals produced quickly
Value demonstrated behind visuals
Industrialized solution for information system
Making data available to a wide range of users
Adopting Tableau for advanced visual analytics
Using Trifacta for data wrangling
Implementing Hortonworks Hadoop for better data handling
Removing silos across operating units
Establishing common data sets across all units
Implementing a bi-directional flow of data
Implemented Tableau in HR department first
Centralized data storage
Created dashboards for Advisory Board
Organized demos of data analysis solutions
"Implemented a user-friendly, quick process for all employees"
Identify behavioral patterns
Leverage data in the field
Bring data to the surface
Use of Tableau Desktop for data visualization
Sharing workbooks over Tableau Server
Scheduled updates for current data
Working with local Tableau partner Solutive
Adopting Tableau Desktop and Server
Standardizing reporting on Tableau
Regular meetings with senior managers to discuss data
Using Tableau dashboards for live data tracking and daily planning
Leveraging Tableau
Using Tableau’s online community and training resources
Integrating Tableau into existing work processes
Using Amazon RDS for data storage
Creating dimensional models for data integration
Using Tableau data extracts for report generation
Using Tableau Cloud to avoid server maintenance
Providing intuitive tools for end users
Use of Tableau Desktop for benchmarking and tracking student progress
Permissions for principals to add local datasets to Tableau Server
Providing a partially filled canvas for principals to enrich insights
Adopt Tableau as visual analytics software
Provide Tableau Desktop licenses to the Decision Support team and other departments
Deploy Tableau Server for automated dashboards
Information not found
Integration with open-source web mapping software
Leveraging Tableau's geo-mapping capabilities
Creating a plug-and-play solution
Trialed multiple visualization solutions
Selected Tableau after extensive research
Conducted initial two-week trial with real data
Requested an extension of the trial due to high demand
Combination of Dataiku’s Data Science Studio (DSS) and Tableau
Sharing completed visualizations through Tableau Server
Providing users with data and enabling them to perform self-service BI
Embed Tableau Server into PremierConnect Enterprise
Use of Tableau in OEM capacity
"Conversion of Excel spreadsheets to more usable, meaningful data"
Data re-engineering and integration into other vendors' applications
Providing easy-to-use data visualization tools
Using Tableau's data connectors to connect various data sources
Adopting Tableau Server for data security and accuracy
Testing all tools in the market
Choosing for ease and simplicity of use
Implementation of Tableau Server
Initial internal training by BI team
Establishment of a reporting guild
consulting services
initial proofs of concept (PoCs)
user-driven tool selection
close collaboration with Tableau team
Using Tableau Desktop and Tableau Server
Publishing data to Tableau Server
Encouraging 'power users' to build their own dashboards
Use of inventory dashboard to capture and track data points
Integration of Tableau visuals for better data understanding
Formed a big data analytics team
Conducted internal trials comparing different tools
Chose Tableau for its ease of use and visual appeal
Connected Tableau to internal databases and other data sources
Free trial of Tableau Cloud
Pilot program with 10 users
"Utilization of training aids such as free videos, walkthroughs, and live online training"
Participation in two-day 'fundamentals training' event
Centralized team managing data sources on Tableau Server
Blending data from various disparate sources
Support from Tableau staff
Creating reports and dashboards
Embedding Tableau into TUNE BI
Utilizing Tableau's data connectors
Created student profile dashboards using Microsoft SQL Server data warehouse
Shared dashboards through Tableau Server and embedded them into the Granite School District portal
Used existing Active Directory profiles to set permissions
Adoption of Tableau for data visualization and dashboards
Real-time data visualization
Quick engagement with leadership
Using Tableau Cloud for mobility
Creating dashboards instead of long reports
"connecting to existing databases like Oracle, Teradata, and MySQL"
democratization of data through ease of use
Transitioned from Excel and Cognos to Tableau
Two-track policy: presenting to clients and internal use
Hands-on method for teaching Tableau
Students construct storytelling with Tableau
Embedding Tableau Server
Using Tableau as the primary method for showing visualizations
"Established guidelines, best practices, and checklists"
Curated data sources in Tableau Server
Implemented a governance model
Implemented Tableau for data visualization
Adopted 'demand flow' supply chain methodology
Created automatic auditing views in Tableau
Implemented Tableau Desktop and Tableau Server globally
Published data sources to server for a single source of truth
Initial project to analyze hotel partnerships
Expanding Tableau use to other departments
Leveraging Microsoft SQL Server data mart
Visualization using Tableau
Internal training for Server and Desktop users
Creation of an internal user group
Engagement with Tableau consultants
Started small and thought big with slow rollout
Trained key analysts and nested them within each business division
Adopted an agile approach instead of a waterfall approach
Used live connection to SAP HANA
Using Tableau for data analysis
Online publication of interactive infographics
"Leveraged Amazon Web Services (AWS) to store, process, and access data at scale"
Adopted Tableau to uncover insights within big data
Adoption of Tableau Desktop and Tableau Server
Embedding dashboards into web-based BI products
Leveraging Leny's past experience with Tableau
Minimizing training due to intuitive interface
Adopting Tableau to teach data analytics
Conducting quick training sessions
Incorporating Tableau into the curriculum
Embedding Tableau dashboards into products
Training employees through learning by doing
selected Bodhtree Consultancy Ltd. as an implementation partner
implementation by joint team of eight from Bodhtree Consulting and Star Health
leveraged experience from previous solution
Evaluation of multiple solutions
Adopted Tableau
Using Splunk for data cleaning
Introduced Tableau in IT and business departments
Shared data organization-wide via Tableau Server
Interactive visualizations
A/B testing
Adopted Tableau based on Tableau Drive methodology
"Introduced in phases: current condition assessment, future structure organization, plot typing, and development base sorting"
Adoption of Tableau to unify data visualizations
Use of Tableau Server for accessible reporting
Implementing Tableau Desktop and Server
Providing Tableau Desktop licenses to main decision makers
Utilizing Tableau training resources
Utilizing Tableau Server and Tableau Desktop
Connecting Tableau to data warehouse and Excel spreadsheets
Displaying unified profiles of students and teachers
Using OEM Tableau Server
Embedding Tableau visualizations into cloud software products
Common data schema for all customers
Hiring a new CEO focused on identifying new revenue opportunities
Creating a Visualisation and Analytics (V&A) Division
Evaluating and integrating Tableau and Birst technologies
Investment in Tableau and Amazon Redshift
BI platform built on Amazon Redshift and Tableau
Integration of data from various systems
Setup of executive dashboards and reports
Using Tableau for data visualization to improve performance
Connecting Tableau to SAP BW and SAP HANA systems
Introduction of Tableau for unifying data
Use of Tableau Server for infrastructure
Customized dashboards for different school levels
Training sessions for principals
Creation of a dashboard providing a digestible overview
Nightly data refresh to keep data current
Implementation of row-level security
Collaboration with IT for governance and security
Trial of Tableau Desktop
Purchase of Tableau Desktop license with the Top Global University Project subsidy
Fundamentals training session for 15 members from different departments
Introduction of Tableau project with an original textbook and a Tableau champion
Using Tableau to showcase data to customers
Employing Story Points in presentations
Storing data as raw data and using a data warehouse for easy analysis
Using Tableau Cloud to collect files in one place
Promotion team using self-service BI to evaluate promotion activities
Deployed Tableau Desktop and Tableau Server
Collaborated with Finnish technology consultants Solutive
Created a three-person core team in the IT department and finance
Tested several data analytics and visualization tools
Selected Tableau for its ease of use and robust API integration
Aggregated and extracted commonly accessed data into a Teradata data warehouse
Using Cloudera Hadoop data lake
Integrating AtScale for query optimization
"Deploying Tableau Desktop, Tableau Server, and Tableau Cloud"
Use of Tableau’s native data connectors
Subset sampling for data exploration
Rapid hypothesis testing
User-friendly tool adoption
Training non-tech savvy end-users
Adoption of Tableau on top of AWS
Utilizing Tableau Desktop and Tableau Server
Self-service analytics with Tableau
Quick adaptation and training on the tool
Immediate implementation after initial trial
Adoption of Tableau Cloud
Enabling mobile access for sales teams
"Expanding usage from sales to marketing, finance, and supply chain"
Performed a benchmarking test on various tools
Trialed Tableau based on recommendations
Immediate installation without IT dependency
Automating Financial Planning and Analysis reporting process
Mapping point-of-sale data
Creating dashboards for remodel tracking
Combining point-of-sale and guest experience data
Implementing Tableau
Creating the Fast Analytics team
Automating reporting to save time and resources
Adopted Tableau Desktop and Tableau Server internally
Created an AaaS offering powered by Tableau Cloud
Implemented a permissions model by site and project
Storytelling with data
Using visualization to evoke emotion
Training business users on how to leverage Tableau
Conducting demonstrations
Implementing Tableau from the ground up
Tableau Server for real-time dashboards
Synchronize with Active Directory for permissions management
Self-service BI for agile decision-making
Using web data connectors
Integrating with Google Analytics
Implemented Tableau Desktop & Server
Created visualizations quickly
Adapted visualizations based on feedback
Transition from print to online
Use of Tableau Public to give everyone access
Adoption of Tableau recommended by Abhishek Bhardwaj
Collaboration with Data Semantics Pvt. Ltd for implementation
Incremental addition of Tableau Desktop licenses
Using Tableau as the preferred tool for research classes
Deploying Tableau into the heart of the company
Giving the tool to analytics users
Partnering with other departments
Rolling out visualizations built on Tableau Server
Encouraging data end users to become comfortable with Tableau Desktop
Using Tableau for data visualization
"Pulling data from Google BigQuery, PostgreSQL, JSON, and CSV files"
Using Flurry Analytics for data collection
Blending sales data with shelf layout specs
Using Tableau to visualize data and extract insights
Established a browser-based system on Tableau Server
Built a 'governor dashboard' using Tableau
Used Tableau to create interactive data visualizations
Incorporated touchscreens for intuitive data interaction
Tested with open-source BI solution (Pentaho)
Adopted Tableau and Amazon Redshift
Used Tableau partner USEReady for development
Migrated to a modern Tableau solution
Redefined and automated in-house data collection process
Set up a data warehouse
Proof of concept demonstrations
Training users to create their own reports
Implementation of a data warehouse with Amazon Redshift cloud infrastructure
Deployment of Tableau for data visualization and reporting
Gallery of Dashboards
Training users during rollout period
Monitoring user progress
Project area for dashboards
Actor roles for dashboard interaction
Moved from SQL Server to Hadoop platform coupled with Spark SQL
Implemented Tableau for data visualization
Engaged Actinvision for decision-making assistance
Rapid prototyping with clients using Tableau
Customization of deliverables through Tableau
Scaling the OEM system within Tableau
"Investing in devices that measure paddle power, twist, angle, and speed"
Visualizing data in Tableau Public
Iterative data analysis for improving techniques
Publish Tableau Cloud directly to clients
Facilitate interactivity with data
Proof of concept with Tableau
Technical consultation and training from Inekon Systems
Conducted proof of concepts
Tested Tableau on various source systems
Use of server-based solution
Customer involvement in product development
Browser-based offering to avoid extra procurement for end users
"Involve business, BI and analytics, and IT teams working together"
Use of best tools in the space
Use of Tableau to make reports interactive
Development of skills through personal blog
Evaluate several BI candidates
Choose based on user-friendliness and price
Demo with 30 people creating 130 reports
Encouraging self-service with the tool
Rolled out Tableau in roughly three months
Server and 30 licenses of Desktop
Partnership approach between business and IT
Drill down into data sets
Create reports and build predictive models
Connecting Tableau to the data warehouse
Using live dashboards for management-related questions
User groups for mutual learning
Community support for challenges
Using Tableau Public for visualizing basketball data
Embedding visualizations in blogs and platforms
Geographic analysis of the market
Building pricing models
Visualizing survey responses
Integrating with IBM text analytics
Use Tableau to forward general and specific reports
Enable departments to explore and analyze data independently
Tested Tableau over a two-week period
Setup and management of the server system handled by Tableau
providing Tableau to everyone
using Tableau intensively in marketing
expanding the solution to other applications
"use of behavioral modelling, personalization, and optimization techniques"
embedding decision engines and driving analytics
use of Tableau for ease of use and report customization
creation of a master version of market forecast report
custom drill-downs at each level
Use of Tableau for dynamic dashboards
Integration of multiple data sources
Thorough research and evaluation of Tableau
Knowledge sessions with management
Workshops for hands-on sessions
Company-wide adoption of Tableau
Developed a BI strategy tailored to their needs
Used proof of concepts to demonstrate capabilities to senior management
End-to-end replacement of their technology stack
Provided informal training to users
Connecting the planning process with Tableau
Developing dashboards and worksheets
Partnering with ReSolt
Adoption of Tableau
Using Tableau to visualize and analyze data from The Birkman Method
Communicating findings through Tableau visualizations to improve client relations
Driving decision-making power to managers
Using a tool that connects big data sets from different systems
Using Tableau for visualization
Utilizing Hadoop for data processing
Aggregating data with services like Amazon Redshift and Treasure Data
Selected Tableau after reviewing multiple BI tools
Downloaded and tested free trial version
Introduced Tableau to BI team and trained them
Implementation of Tableau Desktop and Tableau Server
Geo-referencing data to improve decision making
Partnering with Teknion Data Solutions
Building a data warehouse and master data management projects
linked multiple databases
used Tableau to read from non-integrated databases
Deploy an HP Vertica enterprise data warehouse (EDW)
"Use Tableau for faster, easier, and self-service data analysis"
Adoption of Google Cloud Platform
Implementation of Tableau for data visualization
Attended seminars held by independent consultancy Montage.
Created a pilot program with Tableau Desktop.
Integrated Tableau into the organization’s pre-existing environment with support from Montage.
Demonstrating Tableau's capabilities
Combining Tableau and Alteryx for comprehensive data stories
Adopted Tableau for faster and more efficient data analysis
Using Tableau for data visualization
Providing a single reporting solution for water quality across the organization
Enterprise-wide adoption of Tableau
Empowering teams to build dashboards quickly themselves
OEM partnership with Tableau
Integration of Tableau with their product
Building key metrics into Knowledge Link
Made reports systematic and automatically refresh on the server
Reshaped data to allow custom reports for different departments
embedded in business solution platform
develop new reports quicker
Adopted Tableau for daily data viewing
Processing CSV with Tableau Desktop
Connecting to AWS and Redshift
Use a unified format for all data processed by staff members
Creation of standardized reports for group evaluation
Adopt Tableau for data visualization
Use Tableau Server for sharing analyses
Use of Tableau Server for accessibility
Focusing on end users
Data review meetings with stakeholders
Delivering a wide range of BI systems
"Providing analytical services, implementation, training, and advanced analytics"
Integration with other applications and learning advanced tools like 'R' language
Creating visualizations that self-train and self-educate
Building intuitive dashboards to answer questions quickly
Educating users on data access
Building and empowering users to create dashboards
Engaging a Tableau Gold Partner (InterWorks) for hands-on experience
Building relationships with managers
Creating comprehensive dashboards with trend lines and averages
Automating data updates
adoption of Tableau
centralized reporting
secure dashboard sharing with Tableau Server
Hiring a Tableau consultant from Singapore
Training a team of 10 on specific use cases
Building custom dashboards and programs
"Using professional services for hardware, software, and knowledge setup"
collaboration across groups
teaching how to interpret data
Use Tableau for exploratory data analysis
Identify data gaps and make substantive changes
Use Tableau on the back end to establish connections to databases
using Tableau Cloud to avoid hosting on own server
creating consistent dashboards for recurring surveys
connecting and cleaning disparate data sources
Undergoing a full-blown digital transformation
Conducting extensive research to find a suitable analytics tool
Starting with a trial version to check practical applications
Implemented Tableau and brought in Slalom Consulting
Deployed Tableau Server with the help of Tableau Consulting’s Server Rapid Start
Discovered and adopted Tableau as a data analytics and visualization tool
Conducted training sessions for university staff
OEM partnership to embed and provide custom reporting
Integration of Tableau into LeanKit
Created an internal 'Analytics Task Force'
Collaborative approach with departments to sit down with their data
Integration at the Tableau Server level
Developers use Tableau Desktop to create visualizations
Use of Tableau dashboard to gather student profile information
"Sharing data with guidance department, attendance clerk, attendance officers, and parents"
Using Tableau Cloud
Working with a partner for back-end work
Leveraging real-time data
Identifying core behaviors and disciplines
Transparent information sharing
Drag and drop data
Data aggregation before report creation
Maintaining report templates
Built a decision support system on Tableau
Visualization-first approach for data presentation
Engaging with partners like Tableau
Integrating visualization tools
using Tableau for data visualization
combining social data with internal business data
Lean manufacturing principles
ISO9001 quality system
Single source of truth
Using Tableau for data visualization
Turning unstructured data into databases
Deploy Tableau Software as a data analytics solution
Engaged CYBERTREND for implementation
Evaluating several business intelligence solutions in the market
Choosing a solution that allows for fast implementation of data analytics
Ensuring the solution is easy for end users to learn and use
Analyzing data from a centralized data bank using Tableau
Partnered with Corporate Renaissance Group (CRG) for deployment
Use of Tableau 7 and later upgrade to Tableau 8.1
Integration of Tableau for synthesizing data from multiple formats
Change of server
Automation of reports
"Collaboration with Tableau partner, Head Innovations"
Pilot with executive team
Cascade down the organization
Use of Tableau for mapping player recruitment
Consultations with various staff members to design dashboards
Deployment through the enterprise
Empowerment of users with data
Using Tableau for immediate results
Connecting Tableau to various data sources
Used Tableau to analyze historical attendance data
Implemented Tableau Server for data access and sharing
Introduced dashboards for different departments
Connecting to a Microsoft SQL Server data warehouse using Tableau
Blending data from SQL Server and Excel
Creation of a new Strategic Planning Office
Training staff to transition to analyst roles
Investing in Tableau Server for publishing and sharing visualizations
Best of breed strategy
Joint roadmap and engineering level certification
Attended a training session by data visualisation guru Stephen Few
Used BI Scorecard process for evaluating business intelligence tools
Implemented Tableau Desktop for authoring data visualisations and Tableau Server for secure sharing
Using Tableau to visualize large volumes of data
Combining data from different sources
Leveraging Tableau Cloud to avoid constructing own server structure
Deploying Tableau for data visualization and analytics
Using Tableau Server for real-time report publication
Careful evaluation of visual analytics players
Choosing Tableau software
Integrating Tableau with MySQL and Excel data sources
Generating internal reports weekly and monthly using Tableau
Evaluation of various BI solutions since 2011
Chose Tableau in March after discovering its quick deployment and ease of use
Systematic deployment of Tableau across FET
Conducting more than 150 roadshows at various offices
Hands-on training and workshops
Evaluation of various providers
Started with Marketing department
Analyzing CRM data
Ordered first license immediately after seeing the tool
Downloaded the trial version and generated initial analyses
Optimization of products
Demonstrating Vertica behind Tableau
Share audience-specific insights with media and advertising partners
Publish dashboards to Tableau Server
Create consumer behavior dashboards
Integration of unstructured data from various sources.
Making the data more interactive and accessible through Tableau.
focus on user-centric design
Develop without being experts
Connect to data easily from various sources
Integration with existing data systems
Utilizing Tableau Desktop for quick analysis
Data federation and virtualization
Connecting different data silos
Referral partners
Reseller partners
Focus on visualization of KPIs and executable actions
Democratize information about customer demographics
Adoption of Tableau Desktop and Tableau Server
Creation of visual and actionable intelligence
Provision of interactive visualizations for detailed reports
Implementing Tableau for data visualization
Supporting customers with Tableau ecosystem
Leverage MarkLogic and Tableau together
Build applications to combine data from different systems
Kickstart clients with Tableau and guide them
Provide training for clients to self-sustain
Partnering with Tableau and attending Tableau Conference
Becoming a Tableau Gold Partner
Collaboration with consulting partner M2. technology & project consulting GmbH
Implementation of Tableau software
Allow users to discover it on their own
Professional training provided by Tableau
System integration with Tableau
Training administrative users and daily users
joint selection process with IT and user base
consulting from Tableau Silver Partner PATH
Combining social media trends with data analytics
Using online voting tools to understand customer preferences
Utilizing Tableau Desktop for data analysis
Utilize Tableau for quicker analysis
Train users to get the best out of the tool
Using tools that allow quick creation of dashboards and interactives
Adoption of Tableau for data visualization and reporting
Building own data sets for independent analysis
investigated a range of visualization tools
settled on Tableau
Blending data from multiple sources
Utilizing Tableau Server for dissemination
Creating a single database view
Publishing data source within Tableau
Consulted with Deloitte for recommendations
Worked with an external vendor to build SPOT
Integrated Tableau for reporting functionality
Choosing Tableau
Implementing term reviews every eight weeks
Expanding data analysis services to other schools
Implement Tableau Software
Use Cloudera Impala on a Hadoop data warehouse
Transitioning tasks from Excel to Tableau
Automating processes
Developing dashboards to replace static reports
Reorganizing physician schedules
Executive presentations
Behavior change conversations
Video presentations with dashboards
Bunch data together within Tableau
Break data down into small pieces for analysis
Utilizing Tableau for data visualization and exploration
Connecting to Google Analytics for web traffic and Internet usage trends
Using Tableau for data visualization
"Connecting various data sources (SQL Server, Survey Monkey, Salesforce, spreadsheets)"
Adopting agile development methodology
Collaborative dashboard development with customers
Import data into Tableau
Build interactive and visually appealing dashboards
collect data from various social media platforms
clean social media data before using it in Tableau
Thorough analysis of BI tools
Selection of Tableau for its high-quality visualizations and data discovery approach
Deployment of Tableau Server for easy access to insights
brought in Tableau
viewed instructional videos
obtained data from IT team in Excel format
visualized data in Tableau
Introduction to Tableau through partner Montage
Implementation of Tableau Desktop and Tableau Server
Embedding visualizations in SharePoint for easy access
Focused initial analysis on one hospital
Deep data analysis
Visualization using Tableau
"Site-by-site, hands-on educational workshops"
Initial deployment of Tableau Desktop licenses
Expansion to almost 100 Tableau Server users
Support from Tableau Professional Services for planning and installation
Connecting with InterData data warehouse
Using various data sources like PeopleSoft and flat files
Installed scalable data warehouse
Use of Tableau Server
"providing sales team with actionable, easy to understand data"
enabling sales reps to drill down to individual agency performance
Combining consumer behavior database with company's customer knowledge
Using Alteryx for data integration
Using Tableau for data visualization
Partnering with Tableau
Focusing on data insight for various industries
Pairing Attivio with Tableau
Bringing data together for visualization
Adopting Tableau for analytics
Ensuring everyone looks at the same data
Deployed Tableau with help from InterWorks
Training provided by InterWorks
Following best practices from the start
Downloaded a trial of Tableau
Purchased Tableau Desktop license
Engaged Tableau Consulting for assistance
Using Tableau to provide data access to all employees
Real-time data exploration with Tableau
Use of Tableau for data visualization
Enrolling in Tableau training sessions
Adopt Tableau for data visualization
Purchase professional licenses for direct database access
Blend data from multiple sources including Excel and Exact Target
"Downloaded a free, two-week trial of Tableau"
Invested in Tableau Server for publishing and sharing
Used Active Directory for authorization
"Downloaded a free, two-week trial of Tableau"
Invested in Tableau Server for publishing and sharing interactive data visualizations
Adopting an agile business intelligence tool
Started with proofs of concept using Microsoft Excel and SharePoint
"Evaluated multiple BI tools including Tableau, QlikView, and Jaspersoft"
Built the first BI platform using open source technologies
Revisited Tableau and decided to purchase licenses for broader deployment
Embedded Tableau dashboard into Salesforce for 360-degree customer view
Adoption of Tableau for data visualization and reporting
Blending data from different databases using Tableau
Using Tableau to put data from multiple sources into dashboards
Embedding Tableau solutions within existing systems
Service-oriented architecture for Tableau integration
Adoption of Tableau for interactive dashboards
Shift focus from data pushing to analysis
Using Tableau to connect and visualize data
"Blending data from different sources like SQL, AS400, and XML"
Use of Tableau Server for automation
Building mock-ups in Tableau
Use Tableau for powerful visualizations
Quick setup of analyses and visualizations
Connectivity with various databases
Standardized contact solution across all customer service centers
Adoption of Tableau for data visualization and analysis
Automating report production
Set up a dedicated committee to explore and try out the product
Studied online case studies and consolidated useful information into a knowledge pool
Connecting to multiple data sources
Using the extract API for data blending
Providing access to a broad number of people
One employee convinced their boss to buy a copy of Tableau
Hired personnel with a background in Tableau
Downloaded free trial
Proof of concept with IT around Server and other BI tools
Used Gartner Magic Quadrant to shortlist solutions
Chose Tableau for its agility and ease-of-use
Turned to RAV for support and implementation services
Using Tableau to slice and dice data
"Integrating multiple data sources (Google Analytics, Salesforce, CRM)"
Purchased Tableau Desktop licenses
Started blending data from various databases
Created a data queue for wholesale order processing
Integrated Tableau with Vertex SharePoint site
Established a data analysis unit
Chose Tableau Desktop and Server
Upgraded with each new release
Used Active Directory to control access
Selecting Tableau as the BI tool
Publishing work to Tableau Cloud
Embedding visualizations on the website
Presenting data interactively to prospective customers
Connecting Tableau live to Teradata
Using Tableau for data visualization
Implementing weather models to predict sales impact
"Using publicly-available, free data"
Applying skills and knowledge from day job to football analysis
Using Tableau and Tableau Public to create and share analyses
Automated tool for real-time data access
Visual data representation
Dedicated dashboards for each department
Grassroots effort
Collaboration between IT and business units
Building a supportive user community
Introduction of Tableau by a key director
Initial use in marketing department
Upgrade to Tableau Server for wider access
Standardized on Tableau in Life Sciences division
Installed a trial of Tableau technology
Using Tableau for real-time data analysis
Integration of multiple data sources
Partnering with Tableau
Daily collaboration with Tableau
formation of FINN.no’s BI team
standardizing on Tableau
integrating Tableau with Microsoft Dynamics CRM
self-service reporting
Started with one project for network statistics
Expanded to more than 10 projects covering various data types
development of a new business intelligence architecture
"decision support system, data integration, and reporting"
"collaboration with Altic, a systems integrator"
Built the data warehouse
Established data governance
Developed analytics layer
Conducted a three-month proof of concept
Using Tableau to create visual representations
"Integrating data sources from schools, colleges, and local authority systems"
Implemented Tableau as primary enterprise-wide analytics platform
Built smaller views with Tableau 7
Focused on expanding views and answering new business questions
Converted analysts into Tableau power users
Standardized on a Tableau real-time visualization analytics solution
Use of Actian Vectorwise analytic database for cost-effective performance
"Collecting data from application logs, Facebook insights, and analytics tags"
Enabled at-a-click access to Tableau dashboards within Salesforce
Published dashboards to the customer portal
Consulted Tableau Professional Services for deployment and integration
Standardized analytic efforts using Tableau
Established an analytics team
Aligned efforts with TAPFIN’s analytics using Tableau
Self-taught team through online resources and training
Adopting Tableau Cloud
Utilizing Google BigQuery
Adoption of Tableau Cloud
Creation of user IDs and login credentials for clients
Evaluated multiple BI solutions
Chose Tableau for its comprehensiveness and aesthetics
Integrated with existing infrastructure including CTMS and EDC systems
Using Tableau for data visualization
Hiring Michael Cristiani to build initial workbooks
implemented Tableau Server
measured job completion times
"Analyze, visualize and share information"
Reclaiming hours once lost to data preparation and report generation
BI update events
Breakfast briefings and webcasts
Partnering with Tableau
Set key performance indicators in place
Rapid prototyping in a two-week period
Aligning budget against strategic plans and project areas
Downloaded 14-day trial
Utilized Tableau Cloud
Hooking up Tableau to existing data
Creating a strategic workforce planning solution
Exported data out of the database and loaded it into Tableau for initial analysis
Used Tableau Desktop to hook directly to the database for real-time analysis
Adoption of Tableau for data visualization
Dynamic data updating
Crowdsourced information aggregation
Standardizing on Tableau for business intelligence
Transitioning from in-house report generation to using Tableau
Leveraging Tableau to enable customer self-reliance
Using Tableau's mapping functionality for targeted marketing
Merging with InterWorks
Achieving Tableau Silver Partner status
Embedding Tableau Server content into applications
Using Tableau Desktop for customer data analysis
Incorporating Tableau into product to allow customers to create their own content
BI Olympiad for competition
Using real business problems for data sets
Promoting Tableau among analysts and managers
Created an early warning system based on research
Turned research into a dashboard using Tableau
Use of Graphical User Interface (GUI)
Dashboards for exploratory analytics
Adopting Tableau Desktop for analysts
Using Tableau Server for broader company adoption
Embedding Tableau in email reports and presentations
Use of Tableau Server to engage business users and scale data dissemination
Development of engaging visuals and dashboards
Hiring a chief innovation officer experienced with Tableau
Recruiting champions to evangelize Tableau
Making it easy for interested employees to get involved
trial period with Walmart team
introduction by IT department
spread quickly throughout the U.S. business
Utilizing Tableau as a plug-and-play tool
Promoting self-service BI
Connect to disparate data sources
Utilize a fast visualization engine
Ensure flexibility for customer needs
Use Tableau for data aggregation and visualization
Integrate Tableau Server for 24/7 client access
Using Tableau's direct connector to Google Analytics
Blending Google Analytics data with Omniture and database data
Combining Tableau with Excel
Initial deployment via trial download
Encouraging staff to stay abreast of technology
Gradual deployment with a handful of users and Tableau Server
Attach Tableau to spreadsheets
Generate visualizations quickly
Rolled out to each department
Purchased Tableau Server
Show data in one small warehouse with Tableau
Online training for new users
Demos to senior levels to create interest
embed Tableau in SharePoints
embed on active desktops
embed in PowerPoint
publish content for mobile users
Implemented Tableau Server for data access
Used visual analytics to monitor and manage data
Provided training for iPad Tableau app usage
implemented Tableau Software’s business intelligence application
"provided browser-based, easy-to-use analytics"
Leveraging Tableau within the analyst community
Positioning Tableau as a solution for different data warehouses
Starting small with a couple of Desktop licenses
Gradual rollout to more users
Implementation of Tableau Server with a handful of users
Partnering with IT group for growth
Proving value with small projects
Layering Tableau on top of their database
"Offering in-house collaboration tools (Wikis, training sessions, documents, best practices)"
Creation of a corporate analysis department around Tableau
Working closely with managers and senior executives on different projects
Adoption of Tableau
Collaboration with vendors for problem-solving
Googling and discovering Tableau
Downloading free trial and testing
Purchasing licenses after committee decision
Learning programming skills for displaying data
Using Tableau to make data visually compelling without programming
"Collaborative process involving editors, designers, and data specialists"
"Worked with IT services consultant Interworks, Inc."
Deployed Tableau Desktop and Tableau Reader
collaboration between Teknion and data owners/users
methodical and systematic approach
clear understanding of profitable operations
Purchased Tableau Server and Desktop licenses
Integration with single sign-on mechanism
Direct publishing access for report developers
Minimal training apart from quarterly user group meetings
Generate progress reports
Create visualizations to understand issues
Use data for quality control and program evaluation
deployed Tableau’s visual data analysis software
combination of Tableau Desktop and Tableau Server
Used Tableau for data visualization
Distributed reports to different departments
Deployment of Tableau’s intuitive visual data analysis solution
deployed Tableau Desktop to key users without external resources
selected Tableau Server for online publishing and distribution
Deployed Tableau’s visual analysis applications
Enabled easy data distribution and sharing with customers
rapidly deployed BI solutions via SaaS model
turned to Tableau for data visualization
Identify a visual analysis and reporting solution
Adopt Tableau as the main analysis and reporting solution
Use of Tableau to blend analysis of structured and unstructured data
Integration of Tableau with In-Spire tool
leveraging Tableau Desktop
deploying the solution quickly
Adoption of Tableau for data visualization and analysis
Started with a trial version
Encouraged team to stay abreast of technology
Deployment across organization gradually
Selecting Tableau Standard for the entire consulting staff
Connecting to client data in Excel and Access on-site and in real-time
Selected Tableau Professional
"Used visual analysis to find important trends, relationships, and outliers"
Driver-Based Planning
Digital innovation (moving away from Excel and Access)
Enhancement of organizational mindset and communication
POC (Proof of Concept) solution
Clear target image and precise project goals
Leveraging expertise and experience of external colleagues
Unified solution for CBP
Benchmarking 'Excel-based' CBP against latest technologies
Selection of Board due to its robustness and flexibility
Integration of FP&A and BI solutions
Selection of an appropriate system
Exploring the Business Intelligence market
Carrying out small pilot projects
Proof-of-concept examinations
Rating matrix comparisons
Adoption of Board Intelligent Planning Platform
Integration with Microsoft Office products for familiarity
Implementation of a tool for integrated restaurant planning
Agile project approach by Board partner Better Decisions Group (bdg)
Identified and compared three possible tools from the market
"Selected Board for its user-friendliness, flexibility, cost-effectiveness, and programming-free toolkit"
Choosing Board Intelligent Planning Platform
"Agile, phased approach to working with data"
Active testing and performance assessments
Selecting an extendible platform with cloud compliance
Implementing a nine-step workflow for purchasing planning and analysis
Using historical data for procurement planning
Phased approach to implementation
Proof of Concept using company’s own data
Exploration of several EPM options
Driver-based planning
Adopting an Agile approach
Introducing a lighter and more flexible Enterprise Performance Management solution
Implementing the Board Intelligent Planning Platform
"Replace legacy systems with new, modern technology"
Implement Board across the entire organization
Assess each team's needs and identify areas for maximum benefit
Tailor-made approach for each team
Adopting Board’s Intelligent Planning Platform
Utilizing Board Community forums and learning resources
Implementing a home-grown sales performance management (SPM) solution
Use of Board for Sales and Expenses Reporting since 1994
Gradual extension of Board for Cost Planning and Sales & Operations Planning
Integration of Crop and Seeds sales teams worldwide
Evaluated standardized planning solutions on the market
Selected Board Intelligent Planning Platform for its planning and analytic features
Implemented a unified approach to planning and analysis
Start small think big
Sell the tool
Focus on enthusiastic people
Avoid getting lost in history or hearsay
Implementing the Board Intelligent Planning Platform
Standardizing and integrating the budgeting process
Selection of a best-in-class cloud platform
Integrated processes and data across the organization
Automated financial consolidation process
Phased implementation based on an agile approach
Extensive Proof of Concept (PoC) involving multiple stakeholders
Phased approach beginning with a sales reporting system
Implementation of a unified platform integrating multiple legacy systems
Automation across financial reporting and analytics
Increased business partnering
Linking systems for a single point of truth
Dashboarding and reporting capabilities
Selecting Board Intelligent Planning Platform
Training internal project team with Board consultants
Conducting end-user training
Intensive selection process
Proof of concept with shortlisted vendors
Coaching model for project implementation
Initial implementation for FP&A
"Progressive expansion to other departments (sales & marketing, HR, logistics, warehouse management, managerial area)"
"Replaced rigid, inconsistent, manual processes with a single platform"
Integration of 20 data sources
Automated business report delivery
Implementing the Board Intelligent Planning platform
Collaborating with Simpson Associates for project implementation
Digital transformation journey
Implementing Enterprise Performance Management solution with Board Intelligent Planning Platform
Integration with SAP ERP and CRM
Transition from S&OE to S&OP
Establishing a mature S&OP process
Combining forecasts from across the organization
Using Board platform for intelligent planning
Implement Intelligent Planning platform
Delegate substantial part of planning to sales managers
Use Board’s platform features for comprehensive information baseline for financial data analysis and reporting
Implemented a single work environment shared between all departments
Adopted a more agile and efficient system of workflow management
Integrated multiple applications and platforms into a macro-system
Strategic Planning
Merchandise Planning
Seasonal Budgeting
Assortment Planning and Buying
Retail Planning
In-Season Management
Collaboration with PwC
Implementation of Board Intelligent Planning platform
Collaboration with Board Italia and Bios Management
Gradual extension of the platform to cover all reporting needs
Using Board Intelligent Planning Platform across key business functions
Generating ad hoc reports
Implementation of Board Intelligent Planning Platform
"Integration of financial planning, sales performance management, and capacity planning"
Automating reporting and planning processes
Complete replacement of spreadsheet-based system with Board’s cloud-based Intelligent Planning Platform
Migration of supply chain operations onto the Board solution
"Building reports, scorecards, and KPIs on Board platform"
selected Board Intelligent Planning platform
ensured platform was user-friendly for accountants
technology partner Mecklemore tailored the solution
Partnering with CFO Solutions as EPM advisor
Using the Board Intelligent Platform to automate and consolidate financial reporting
Implementing Board Financial Consolidation (BFC)
Exploration of available solutions on the market
Choosing Board over SAP BW due to flexibility and lower costs
Developing tailored applications using Board
Adoption of Board Intelligent Planning Platform after extensive search
Week-long training session with a training partner
Creating a unified database for general ledger data
"Working with Board Partner, Axians, to select and implement the platform"
Involving key stakeholders throughout the project
Implementing Board Intelligent Planning Platform
Using historical sales data for predictive modeling
Centralizing demand and capacity data
Implemented Board across control and analysis area
Created a single point of truth fed directly by company’s ERP
Built applications for cost analysis and intercompany transaction analysis
Set up a Business Intelligence Competence Center (BICC)
Carried out multiple proofs-of-concept (POCs)
Europe-wide call for tenders for a BI and planning tool
Develop a central reporting system
Enlist services of linkFISH Consulting
Establish uniform standards
adopt a Cloud-based platform
select a toolkit approach for flexibility
Implemented new pricing and terms system for Germany
Harmonized structures for greater transparency
"Integrated system across Germany, Austria, and Switzerland"
Joint proposition for financial planning transformation with KPMG
Access to one single source of the truth via Board’s Intelligent Planning platform
Adoption of Board Intelligent Planning Platform
"Integrating annual, three-year, and ten-year plans"
Cross-functional collaboration
in-depth assessment and selection of financial planning and analytics software solutions
"partnering with Board Japan’s partner Nippon Rad Co., Ltd."
"Reorganizing workflows to integrate planning, management control, and budget process"
Adopting advanced digital tools for analytics and performance management
Using Board Intelligent platform for information analysis
Thorough research and review of several budgeting and business intelligence solutions
Selected Board to replace nearly 200 spreadsheets
Partnered with Corporate Renaissance Group for implementation
Evaluating new software with KPMG Germany
"Using market overviews by analysts such as BARC, Forrester, and Gartner"
Implementing Board’s Intelligent Planning Platform
Selected Board's Intelligent Planning Platform
Provided initial support and training via Board Professional Services
Mobile adaptation for field service use
Search for an easy-to-use enterprise decision management platform
"Evaluate multiple platforms (IBM Cognos, Tableau, Board International, Qlik)"
Select Board International as the best platform
Integrated platform for digitization
Visualization of the assortment at every step
Utilization of analytics
Carefully defining necessary reports
Analyzing systems for best report creation
Implementing in two logical layers: technical IT basis and business applications
Implemented an integrated planning solution
Selected Board decision-making platform
Worked with Board partner celver AG
Automatisierung der Prozesse durch KI
Integrierte Business- bzw. Retail-Planung
Strukturierte Verzahnung des Gesamtprozesses
Migrating and standardizing supply chain planning processes
Adopting an agile and incremental approach with user stories
Integrating third-party engines into a modern planning solution
"Focus on key characteristics for selection criteria such as ease of installation, implementation and maintenance costs, and self-service capabilities"
Consideration of Board due to positive past experiences
"Phased implementation addressing Sales and Marketing requirements first, followed by Finance, Administration, and Logistics"
Initial focus on Sales and Marketing team requirements
"Development of applications for Finance, Administration, and Logistics departments within Board"
Adopting specific consolidation software
Integrating consolidation with existing Board operational reporting
Researching potential business intelligence tools
"Assessing Board, Tableau, and Power BI"
"Choosing Board for core reporting, analysis and budgeting capabilities"
start with a smaller idea and add new functionality over time
phase-by-phase development plan
"combine Business Intelligence, planning, and predictive analytics in a single interface"
Design and implement a new business planning model
Use of a multi-dimensional platform (Board)
Coordination of different actors in the process
Establishment of a new FP&A team
Consultation with NTT Data
Training internal team with specific skillsets
Using a 'model company' to establish templates
Used Gartner for analyzing available options
Detailed comparison of the market
Invited several vendors to present their solutions
Judged speed and governance of information
Direct partnership with the provider rather than consultancies
design and implement a new business planning model
consolidated approach with SDG consultants and main process owners
analysis of process and integration of model into company context
Pilot with Merret BI powered by Board
Integration of multiple data sources
Replacement of existing BI systems with Board
Finance transformation project
Driver-Based Forecasting and Financial Consolidation
"Budgeting, Planning, and Reporting"
Conduct a detailed analysis
Categorize the full range of reports
Run workshops with various departments
Design a multidimensional data model
Implement a central data warehouse (DWH) on an MS SQL server platform
"Using Board to unify Business Intelligence, Planning, and Predictive Analytics"
Custom demonstrations with HOA's data during evaluation process
Continuous open dialogue during evaluation
Deploying the Board platform
Integration into multiple systems
Eliminating complex spreadsheets
Partnership with the supplier
Tight project schedule
Integration with existing systems (Rimo and ERP)
detailed selection process involving IT and business departments
setting up a global and integrated financial planning system
Evaluated both Board and IBM’s Cognos solution
Chose Board for its core offerings and capabilities
Invited various providers and consulting companies to PoCs
Implemented Merchandise Financial Planning solution
Followed with Assortment Planning solution
No-programming approach
Automatic data imports
Intuitive screen setup for users
Proof-of-concept comparison of several software vendors
Engagement with SPL Tele as an experienced partner
Data standardization and transfer from various sources
Evaluated multiple tools
"Chose Board for its seamless integration of analysis, planning, reporting, and simulation"
"Unificazione delle funzioni di Business Intelligence, pianificazione e analisi"
Collaborazione con GOOD NUMBERS
Implementazione di cruscotti direzionali e indicatori aggregati
Simulazione degli ordini di vendita
Researching potential business intelligence tools
"Assessing various BI tools (Board, Tableau, Power BI)"
Choosing Board for its core capabilities
researching a complement to ERP system
leveraging Board's flexibility in data entry and user self-sufficiency
utilizing an all-in-one platform
Review of the market
Choosing the Board Intelligent Planning Platform
"Gradual introduction, development, optimization, and expansion of applications"
In-house training courses
Implementation of a Board module developed by CONTRACT Controlling
Involvement of internal controller in the project from the beginning
"definir y comunicar los Objetivos de Mejora, Planes de Acción y Presupuestos"
"conjugar Consultoría de Procesos de Negocio, BI y CPM en una única solución"
desarrollo de una Prueba de Concepto (PoC)
detailed analysis of reports from departments
categorization of reports
design of a multidimensional data model
implementation of a central data warehouse
nightly data updates with SSIS packages
Proof of Concept and analysis workshop
Data consolidation in a new Data Warehouse (Microsoft SQL Server)
Used Gartner for analyzing available options
Conducted a detailed comparison of the market
Invited several vendors to present their solutions
Chose direct provider engagement over consultancies
Focused initially on Maintenance for implementation
pulling information from a variety of data sources including Microsoft SQL Server 2005 and Microsoft Great Plains
developing a full suite of KPIs with the Board team
Explored a range of BI products
Selected Board for its flexibility
Phased implementation
Implemented customized and agile processes
Continuous planning to reflect ongoing transformation
Unified data management within a single platform
Implementation of two separate and independent environments: Sales & Finance
One unique dataset sourcing from the S&OP process to ensure one version of the truth
Redefining the process for forecasting
Use of Board software for commercial data since 2003
Expansion to Industry division for analyzing industrial performance
Adoption of Board software for Finance function
Chose a flexible BI solution (Board)
Implemented in phases with careful project reviews
Integrated sales forecast data with ERP
Partnered with a Board partner for advice and assistance
Implemented in phases focusing on simplicity
Used cloud solutions to avoid in-house IT expertise
Ensured effective handover of knowledge to in-house staff
Deployed Board as its new reporting tool
Ran Board on top of data warehouse and linked to other internal data sources
Replicated existing reports while improving them from a data visualization and remote access perspective
Einsatz des All-in-One Toolkit Board
Projektumsetzung federführend über den Berliner IT-Dienstleister 2K Business Intelligence Solutions
Integrated approach to business planning
Leverage scenario planning capabilities
use Board’s Intelligent Planning Platform
"integrate strategy, finance, and operations"
"connecting strategic, operational, and financial plans with available resources"
using Board’s Intelligent Planning Platform
Develop and support Sales & Operations Planning process
"Integrate strategy, finance, and operations"
Implementing agile and customized processes
"Creating user-friendly tools for simulations, sales planning, and analysis"
Selected Board Intelligent Planning Platform
Leveraged code-free interface for developing applications independently
Adoption of Board's intelligent planning platform
Seamless integration with Office 365
Fast implementation time with an agile and modular approach
identified suitable platform (Board Intelligent Platform)
replaced manual processes
Introduction of a professional analytics and business intelligence platform
Selection of Board after attending a budget workshop and product research
Closely working with Board Partners consultants
Consulting Gartner for recommendations
Conducting Proof-of-Concept evaluations with multiple vendors
Driven by the finance department and funded by the IT department
Parallel running for a month before full production
"Integration of budgeting, planning, and forecasting with reporting, analysis, and dashboards"
Implementation of a single integrated approach to Business Analytics and Corporate Performance Management
"Use of Board's financial budgeting, planning and reporting solution"
Engaging a leading technology solution provider
"Implementing Board for integrated budgeting, forecasting, planning, and analytics"
Thorough evaluation of the market
Selection based on ease of use and unified nature of the toolkit
"Integrating budgeting, planning, and forecasting with reporting, analysis, and dash-boarding"
acquisition of the Board platform
"building dynamic, intuitive reports"
eliminating room for manual error
Conducted a search for CPM tools
"Selected Board for its flexible planning and modeling capabilities, drill-down features, and overall ease of use"
Selected Board for its well-crafted design and intuitive interface
Automate calculations and output reports
Run macro-economic what-if scenarios using Board
Extensive search of BI and CPM vendors
"Deployment in phases: starting with reporting, then expanding to performance management and predictive analytics"
Rolling out Board as a single platform for business planning and reporting
Streamlining planning process
Using Board for financial consolidation and reporting
Implementing Board for performance management and reporting needs
Coaching approach to learn how to use and design Board applications
Flexible implementation and autonomous development
Einführung einer cloudbasierten Version der Board-Plattform
Know-how-Transfer und Coaching der Mitarbeiter
unified decision-making platform
integrated workflows with approval and rejection processes
interactive dashboards
automatic consolidation of different currencies
formal software selection process
international roll-out across all showrooms and retail partners
training of around 80 employees worldwide
Detailed market survey
Proof of concept by implementation partner MCG Management Consulting
Implementierung der Board-Lösung in der Cloud
"Standardisierung und Harmonisierung von Strukturen, Daten und Prozessen"
Schaffung einer übergreifenden Sicht auf wesentliche Leistungsindikatoren
Self-service implementation
Coaching process with departmental key users
Conducting extensive Proof of Concept process
Intensive and creative workshop with Board
Creating a central datasource for reporting and planning
Agile application set up
End-to-end consultancy approach from design through implementation
Adopting a serverless architecture
Partnering with cloud consultancy Altostratus
"Using Google BigQuery, Pub/Sub, Dataflow, and Google Maps Platform APIs"
Redefining and restructuring core infrastructure
Adopting Google Cloud's architecture and SRE principles
Using Protocol Buffers to define architecture
Collaborating with Rezco team
Migrating to Google Cloud
Partnering with Linkbynet for support and training
Migrating data environment to the cloud
Choosing Google Cloud for its big data and analytics solutions
Providing training to professionals
Using Google Cloud's App Engine for initial deployment
Migrating to Google Kubernetes Engine and BigQuery as volumes increased
One-click cross-sell project
A/B testing with propensity models
"Integration with cloud tools like BigQuery, Cloud Functions, Pub/Sub, and Cloud Scheduler"
Using Google Cloud for video storage
Leveraging Compute Engine for video processing
Employing TensorFlow for machine learning tasks
Utilizing BigQuery for app and service analysis
Use of Google Cloud for infrastructure
Integration of blockchain for security
Development of API and microservices platform
Migrating to Google Cloud
Using BigQuery for autoscaling
Collaborating with Rackspace Technology
Migrating information from physical servers to the cloud
Switching to a squad-based organizational model
Partnering with Google Cloud
Building a centralized data lake in the cloud
Adapting big data infrastructure to the cloud
Migrated systems to Google Cloud
Used Cloud Run and Cloud Build for deployments
Leveraged Cloud SQL and other Google Cloud tools
Built Genomes in the Cloud on Google Cloud Platform
Adopted Google Cloud Storage
"Used Cloud Life Sciences, GATK, and Picard on Google Compute Engine"
Using managed services
Deploying Google Kubernetes Engine for autoscaling
Utilizing BigQuery for data analysis
Automating security tasks
Cloud-native services
Establishing a Data Lake
Leveraging Google Cloud services
Migration to Iaas - Compute Engine
Marketing automation and micro-segmentation with BigQuery and Dataproc
Migration to Google Cloud in 2019
Use of Google Marketing Platform and partnership with Trakken
Standardization of processes and building expertise through training sessions
Leveraging Google Cloud’s high scalability and global network
Optimizing cloud architectural design
Using CloudMile for technical support and troubleshooting
Adopting Google Kubernetes Engine for container management
Utilizing IoT Core for real-time device status monitoring
Deploying social networking platform in Google Cloud
Using microservices in GKE
Utilizing virtual machines in Compute Engine
Employing Cloud Storage for data backup
Leveraging BigQuery for near real-time data analytics
Rebuilding data infrastructure
Conducting a trial and evaluating solutions
Working with consulting firms like Montreal Analytics and Hashpath
Cloud-based solution
Leveraging integrations between tools
Ensuring security and data governance
Quick ramp time with minimal headcount investment
Supporting data self-service
Adopting Looker for BI and data visualizations
Using Looker for embedded analytics and workflow integrations
Leveraging Looker’s modern architecture and cloud deployment options
development of a single cross-company data platform
collaboration with Artefact
move from on-premises to cloud infrastructure
"using BigQuery, Cloud Storage, BigQuery ML, Vertex AI"
focus on data analytics and AI
Centralized data in BigQuery
Use Looker’s platform and integrations
"Built custom, data-driven workflows"
"Leveraged Looker’s modeling layer, LookML"
Run applications and data in Google Cloud
Utilize Google Maps Platform for optimized routes
Leverage Google Cloud Professional Services for key projects
Partnered with Looker for advanced data experiences
Used both out-of-the-box and custom data solutions
Worked with systems integrator DAS42 for implementation and training
Reached out to Google Cloud for support
Used Firestore for database management
Used BigQuery for business intelligence and data modeling
Used BigQuery and Vertex AI for machine learning algorithms
Adopted Google Cloud solutions
Used BigQuery for data analytics
Optimized use of Google Maps Platform for geolocation
Reviewed and improved use of Google Workspace
Automated Quality Management review system for sales calls
Developing NLP solution to review Cantonese-English contact center conversations
Using Google Cloud resources and BigQuery for data analytics
Engaged experts from Analytics8 for data strategy design and implementation
Built a cloud data warehouse in Snowflake
Leveraged Fivetran for fully managed data pipelines
Used dbt for ELT processes
Using Google Cloud solutions like Compute Engine and Cloud Dataflow
Analytics support from Quantiphi
Adopting AI and machine learning for image analysis
Adoption of Google Cloud and Google Workspace resources
Inclusion in Black Founders Fund for mentorship and support
Use of App Engine for app creation and scaling
Use of Cloud SQL for essential support
Using Looker for data-rich experiences
Maintaining data integrity and consistency
Breaking down data silos
Creating and maintaining data consistency as the company scaled
Pooling data from siloed sources
Updating infrastructure
"Using Google Cloud tools like App Engine, Cloud Spanner, and Pub/Sub"
Developing natively into Google Cloud serverless architecture
Lift-and-shift migration to Google Cloud
Building a data lake
Utilizing Google Cloud managed services
Training product owners to use BigQuery SQL
Migration to Google Cloud
Collaboration with Google Cloud partner Datatonic
Collaborated closely with business to address their analytical needs.
Phased migration to the cloud.
Leveraged Google Cloud's smart analytics platform including Looker and BigQuery.
Using Google Cloud for infrastructure
Collaboration with SADA for technical assistance
Microservice-based architecture and proprietary message bus
"Utilized Looker's modeling layer, LookML, for data governance and metric definitions"
Leveraged Looker's native connection with AWS Redshift for a single source of truth
Embedded analytics into external products for winery and merchant partners
Migrated application to Google Cloud
Used Google Kubernetes Engine (GKE) for containerized microservices
Using Google Cloud products for innovation
Google App Engine for scalable web presence
Google BigQuery for data analysis
Google Kubernetes Engine for horizontal scaling
Google Cloud Datastore for stateful configuration
Google Cloud Pub/Sub for decoupling systems
Google Cloud Dataflow for data processing
Collaborated with Google Cloud
Initial test with Looker
Partnership with Commit
Moved infrastructure to Google Cloud
Utilized Google Cloud Bigtable as NoSQL database service
Employed Google Compute Engine and Cloud Dataproc
Deployment of Google Cloud solutions
Utilization of BigQuery for data handling
Integration with Looker Studio for data visualization
Scaling with App Engine and Cloud Run
Moving to Google Cloud Platform
Using Kubernetes Engine
Employing BigQuery for data warehousing
Using Compute Engine for infrastructure-as-a-service
Utilizing Cloud SQL for relational databases
Using Google Cloud's full technology stack
"Collaboration with Onigroup for design, assessment, and integration guidance"
"Used Google Cloud products to create scalable, responsive customer analytics"
Migrated from a monolithic infrastructure to scalable microservices with help from Accenture
Deployed SAP S/4HANA and BW/4HANA on Google Cloud infrastructure
Partnering with Pluto7 and Google Cloud
Using machine learning and AI
Evaluating six months of manufacturing data
Conducting a month-long Makeathon
Built data pipeline with Google Cloud
Teamed up with Google Cloud Partner SFEIR
Migrated data in two phases
Migrated to Google Cloud
Adopted microservices-based architecture
Used managed services and automations
Migration to Google Cloud
Building a data lake with BigQuery
Using Cloud Data Fusion for data ingestion
Training sessions with Google Cloud experts
Adoption of Google Cloud tools
"Partnership with Minka, an open banking platform"
Migrating to Google Cloud and BigQuery
Leveraging Dataflow and Cloud Composer for data pipeline requirements
Hosting data platform applications on Google Kubernetes Engine
Use of Google Cloud Platform
Integration with Google Maps and Street View cars
Migration to Google Cloud solutions
Deep dive on tools for technological and financial feasibility
Access to Google’s engineers for feedback and support
Building an internal development platform
Choosing Google Cloud for data analysis tools
Using cloud-based development tools
Collaborated with Google Cloud and CGI
Developed replatforming strategy
Adopted platform strategy for partnerships and collaborations
Used BigQuery for real-time and predictive analytics
Using Google Cloud ecosystem
Developing Heimdall compliance tool
Releasing RAM as an open source project
adoption of a site reliability engineering (SRE) model
use of Google Cloud operations suite
collaboration with Cloud Consulting Services
Migrated platform to Google Cloud
Used implementation partner OP-Rate
Built a parallel architecture and moved services load by load
Optimized dataflow pipeline during migration
Adopted BigQuery as serverless data warehouse
Combined BigQuery with AutoML and Vertex AI
Used Google Cloud tools like Vertex AI Workbench
Use of managed services extensively
"Use of Google Cloud's AI Platform, BigQuery, Cloud SQL, and Pub/Sub"
Migrated to Google Cloud
Use of Google Cloud Spanner for geographic expansion
Google Kubernetes Engine for process automation
Optimizing use of Docker containerization
Partnership with Google Cloud
Gradual migration to Google Cloud
Use of Google Workspace for productivity and collaboration
Centralizing platform's technology on Google Cloud
Partnered with Servinformación for training
Built a chatbot for accessible user interaction
Used Google Cloud tools for data organization and processing
Using Google Cloud services such as Kubernetes Engine and Compute Engine
Moving from spreadsheets and server-based solutions to cloud-based solutions
Implementing rolling update strategy for deployments
deploy Google Cloud services
use AI-powered chatbots
leverage BigQuery for data analysis
"Building a scalable, online platform using Google Cloud"
Utilizing deep learning and image processing capabilities
use of Google Kubernetes Engine
cloud migration to Google Cloud
implementation of site reliability engineering (SRE)
use of BigQuery for data warehouse creation
Migration to Google Cloud
Adopting infrastructure-as-code methodologies
Developing a web-based SaaS solution
Using Google Cloud for infrastructure
Implementing machine learning algorithms
Leveraging Google Kubernetes Engine
Utilizing Google Cloud Technical Account Management
Move to microservices architecture
Use of Google Cloud and Google Kubernetes Engine
Implement CI/CD pipeline with Jenkins and Terraform
Deploy Google Workspace for internal communications
Migrated data pipeline to Google Cloud
Leveraged Firebase for data collection
Used BigQuery for data analytics
Leveraged Google Cloud data and machine learning capabilities
Collaborated with Accenture for retail data science expertise
Migrated to BigQuery for data warehousing
Utilized Google Analytics 360
use of Google Cloud services
development of own electronic signature software
integration with Google Workspace
Engaged Google Cloud Professional Services
Identified relevant use cases and datasets
Using Google Kubernetes Engine for scaling and control
Using Compute Engine for research and prototyping
"Migrating to Google Cloud services like BigQuery, Pub/Sub, and Dataflow"
Using Google Cloud and Google Workspace from the start
"Onboarding all platforms, services, and staff onto Google products"
Automated scaling to accommodate growth fluctuations
Building in a serverless Google Cloud environment
Using App Engine as the backbone of operations
Focusing on software development over server management
Migrated to Google Cloud
Used open source technology
Moved to containerized solutions
Implemented Kubernetes Engine
Use of Google Cloud
Utilizing Google Workspace tools
"Adopting Speech to Text APIs, Google Cloud-integrated storage, BigQuery, DLP API, Looker Studio"
Using BigQuery for real-time data access
Deploying BigQuery as a scalable data lake
Moved to Google Cloud
Ran proof of concepts
Collaborated with Google Premier Partner Searce
leveraging Google Cloud
using Google Kubernetes Engine
retraining neural networks with TensorFlow
Building on Google Cloud Platform
Using distributed sensor network and high-speed web crawler
use of Google Anthos for enhanced containerization
migration of mainframe applications to cloud
adoption of hybrid cloud infrastructure
"collaboration with partners Tietoevry, Accenture, and Heirloom"
Adopting an API-first strategy
Utilizing Apigee for API management
Moved to Google Workspace and Google Cloud
Utilized Google Kubernetes Engine for transition
"Implemented Google Cloud's monitoring, diagnostic, and analytics tools"
Partnership with Google Cloud
Collaboration with Rackspace and ML6
"Adoption of Google Cloud solutions like BigQuery, Kubernetes Engine, and AI Platform Notebooks"
Use of Google Cloud's serverless architecture
Collaboration with Rackspace and Google Cloud Consulting Services
"Created a data lake for clean, accurate data"
Implemented Looker for data governance and self-service access
"Used Looker’s semantic modeling layer, LookML"
Transforming workforce collaboration with Google Workspace
Migrating core banking systems to Google Cloud
Upgrading to SAP S/4HANA
Using Google Cloud Professional Services for implementation
Leveraging SAP-certified Compute Engine VM configurations
Migrating SAP-based banking applications to Google Cloud
Building a Data Exposure Enablement Platform (DEEP)
Choosing BigQuery as a serverless data warehouse and data lake
"Using Google Cloud to host Mammosphere for security, cost-effectiveness, and managed services"
Utilizing Google Cloud APIs and managed services for enhanced analytics capabilities
Deploying Cloud IoT Core and BigQuery
Using Google Maps Platform for navigation
Employing Google Cloud machine learning solutions
Transition to Google Cloud
Two-phase modernization plan
Use of Google Kubernetes Engine for microservices
Migration of middleware and core banking elements to cloud
Implementation of real-time data streaming
Using Google Cloud-managed services
Acquiring Early Birds for AI-driven personalization
Using Google Cloud for scalability and broad range of technologies
Full migration to Google Kubernetes Engine (GKE)
Leveraging Google Cloud big data technologies like Cloud Dataflow and BigQuery
Collaboration with Google Cloud Partner DoiT International
"Utilizing Google Cloud's managed services like App Engine, BigQuery, Looker Studio, Cloud Datalab, Apigee API Platform, and Cloud Endpoints"
Leveraging Google's third-party auditors for infrastructure certification
Adopting Google Cloud
Using Big Data and AI technologies
Implementing hybrid environment
Developing IoT and centralized analytics systems
Migrating from a legacy data warehouse platform to Google Cloud
Utilizing BigQuery for data exploration and proof-of-concept
Migrate and centralize data in the cloud
Deploy Looker
Modernize tech stack
Provide self-service access to trusted metrics
Developed MyStore Sales app
Used Google Cloud for data warehousing and analytics
Worked with implementation partner Noovle
Migrating to Google Cloud
Using BigQuery as data warehouse
Moving to a new-generation cloud platform
Giving corporate Gmail addresses to all frontline workers
Leveraging Google Workspace for collaboration
Incorporated Google Cloud into hybrid cloud architecture
Adopted lift and shift method for cloud migration
Leveraged autoscaling feature of GKE
Modernizing infrastructure
Adopting Google Cloud technology
Using data-driven insights
Collaboration with Google Cloud Technical Account Management team
Partnering with Wabion for analytics platform setup
Collate and analyze data for insights
Use Google Cloud for scalable and secure analytics
Migrating data to Google Cloud with assistance
Using Google BigQuery as the main analytics engine
Integrating various Google tools seamlessly
Migrating to Google Cloud
Creating an internal data team
Federated data governance
Support from IPNET Growth Partner
Implementing FinOps model
Migrating infrastructure to Google Cloud
Using Looker Studio for financial accountability
Using BigQuery for data extraction
Scrum model for project management
Deployed GKE on VMware (GDC Virtual)
Integrated GKE on VMware with VMware vCenter to provision Kubernetes clusters
Internal containerization
Adoption of Google Workspace
"Collaboration with Devoteam G Cloud for design, deployment, and training"
Migrating to Google Cloud Platform
Engaging Google Cloud Premier Partner and Specialization Partner (Pythian)
Conducting proofs of concept with shortlisted cloud vendors
Using Google Professional Services Organization's expertise
Migration to Google Cloud
Collaboration with Google Cloud since 2012
Adopting Google Workspace for productivity
Investing in AI and ML capabilities
Implemented BigQuery for analytics power
Set up fast with minimal upfront financial investment
Used BigQuery for automated queries and easy data storage setup
Switched from Amazon Web Services to Google Cloud
Used Google App Engine for autoscaling and Cloud Storage for data storage
Digitized shopping experience
Built a marketing data lake
Adopted a multi-site approach
Grouped countries by language type
A/B tested new design changes
Partnered with DataArt for consulting
Migrated to Google Cloud
Implemented Compute Engine
Used BigQuery for data collection
Implemented an AI and Internet of Things (IoT) project
Used hundreds of tags from wind turbines to capture data
Used AI and machine learning algorithms to predict component failures
Using Google Cloud Platform managed services
Implementing Google BigQuery for analytics
Building daily one-way automatic synchronization with Google Cloud Storage
Using auto-scaling Compute Engine instances and Container Registry
Built a data pipeline in the cloud
Worked with Google Premium Partner SFEIR from proof of concept to implementation
"Used Google Cloud Platform products like BigQuery, Cloud Dataflow, and Cloud Storage"
Using Google Cloud for scalability
"Leveraging Google Cloud tools like App Engine, Firebase, and BigQuery"
Migration to Google Cloud
Adoption of Google Kubernetes Engine
Using BigQuery for data analysis
Utilizing Cloud Bigtable for database infrastructure
Cloud SQL for database management
Service-oriented architecture
Using BigQuery for real-time data analysis
"Leveraging Google Cloud products like Firebase, Cloud Functions, and Cloud SQL"
Migrated data platform to Google Cloud
Used BigQuery for analytics
Implemented Cloud Pub/Sub for messaging
Used Cloud Dataflow for data transformation
Built an analytics platform using Looker and Google BigQuery
"Moved data warehouse to Google BigQuery for speed, scalability, and cost efficiency"
Teaming up with Crystalloids
Building a new architecture on Google Cloud
Integrating all relevant applications
Creating a central point of truth for data
Using Google Cloud to power transformation
"Building ML algorithm with BigQuery, Cloud Functions, Google Kubernetes Engine, and TensorFlow"
Cross-functional team collaboration
Migrated to Google Cloud
Utilized Google Cloud Premier partners KPMG and Aliz
Centralized data on BigQuery
Use Google BigQuery for data collection and analysis
Combine data from multiple sources using OWOX BI Pipeline
Automate reporting with Google Apps Script in Google Sheets
Adoption of Google Cloud
Use of App Engine
Deployment of BigQuery
Migrated AI software platform to Google Cloud
Redesigned platform architecture to microservices design
Leveraged 28 Google Cloud services
Moved to Google Cloud
Used AI-powered solutions
Implemented fully managed cloud services
Performed lift-and-shift migration of virtual machines
Built an ML platform using Google Cloud managed services
Adoption of Fivetran managed data pipeline
Utilizing BigQuery for data storage and analysis
Integration with Google Cloud
Migrated Zi Power Ads AI to Google Cloud
Used architecture consultancy services from iKala
Amended planned Google Cloud architecture to reduce internet traffic costs
Adopting managed cloud services early
Using Google Cloud's infrastructure
Evolving infrastructure with microservices and Kubernetes
Incorporating automation with Cloud Functions and Cloud Run
Unify dispersed IT landscape
Use Google Cloud-managed services
Bring data together from different systems
Deployment of Brightfield hardware for real-time data collection
Use of Google Cloud infrastructure for scalability
On-edge processing and cloud-based analytics
Using Apigee to manage APIs
Deploying as a managed service
Utilizing Google Kubernetes Engine and BigQuery for high availability and performance
Connecting on-premise data warehouse with BigQuery
"Maintaining highest standards of data integrity, security, and privacy"
Partnering with CDP for engineering lead
Cloud-native approach
Managed microservices
Fully managed backend with Firebase
Using Google Cloud for data warehouse and franchise platform
Implementing Google Workspace for data circulation
Built a cloud-based data warehouse with OWOX BI and BigQuery
Collated unmapped datasets into a single platform
Outsource operational work
Host as little as possible
Migrate data processing infrastructure to Google Cloud
Use Google Cloud tools to automate and optimize internal processes
Migrating to Google Cloud and Google Workspace
Quick migration process for SAP
Deploying Google Cloud Cortex Framework
Using BigQuery and Looker for data analytics
Adopting Google Cloud and BigQuery
Integrating Quantum Metric for real-time customer interactions
Using data-driven insights to enhance customer experiences
Switching from homegrown data analytics solution to Looker
Integration with other tools like Segment and Braze
Creating custom data experiences for different departments
Migrating applications and workloads to Google Cloud
Using Kubernetes to reduce maintenance requirements
Employing Google Cloud products for compute and data workloads
Migrating SAP applications to Google Cloud
Using Google Cloud's advanced cloud infrastructure
Implementing BeyondCorp Enterprise for zero trust network protections
Leveraging Google Cloud solutions like BigQuery for data analytics
Move to Google Cloud VMware Engine
Use of FinOps techniques
Close down on-premises data centers
Seamless migration with VMware HCX
Building their own commerce enablement engine on Google Cloud
"Using Google Cloud services such as Anthos, Cloud SQL, BigQuery, GKE, and Vertex AI"
Engaging SADA for end-to-end process for listing on Google Cloud Marketplace
Participating in SADA’s SaaS Alliance Program
Migration to Google Cloud
Use of O3 Enterprise ZeeroMED suite
Integration of RIS and PACS platform
Migrating infrastructure to Google Cloud
Adopting SAP S/4HANA with architecture fully based on Google Cloud
Implementing a hybrid environment focusing on a cloud-first strategy
Building on Firebase and Google Cloud
Deploying Cloud Functions for Firebase
Using Cloud Pub/Sub for data pipelines
increased reliance on Google Cloud and Firebase
use of Google BigQuery for data analysis
use of Google Cloud APIs and Firebase for app development
Adopting cloud technology as its operational foundation
Active role in debates and significant changes
"Using Google Cloud's tools like Kubernetes Engine, Cloud Spanner, and BigQuery"
Migrating from Cloud SQL to Cloud Spanner
Use of Google Kubernetes Engine for container orchestration
Adoption of managed services for flexibility and agility
"Utilization of Google Cloud services such as App Engine, Stackdriver, BigQuery, Looker Studio, and Cloud SQL"
Use of Google Cloud
Containerization with Google Kubernetes Engine
Utilization of TensorFlow and NVIDIA Tesla V100 GPUs
Implemented Google Compute Engine for real-time processing
Created a multi-cloud infrastructure with built-in redundancy using parallel architectures
Utilized Google Stackdriver for storage and Google Cloud Load Balancing
Using Google Cloud BigQuery as a central repository for data
Integrating multiple data sources into BigQuery
Using Looker for data visualization and analysis
Employing Firebase for performance optimization and customer interaction analysis
using Google Cloud managed services
standardized solutions orchestrated on Google Kubernetes Engine
real-time data streaming and querying on BigQuery
Digitizing the grid
Using Google Cloud for data management and optimization
API-based solutions developed with Apigee
microservices-based platform
Developing a hybrid model with on-premise and cloud
Long-term technical partnership with CDP/Mero Group
Adoption of Google Cloud and managed microservices approach
Utilization of Firebase for a fully managed backend
Using Google Cloud and Google Workspace
Partnering with Cloudwürdig for implementation
Building an ETL pipeline for data processing
Switching to Google Workspace for communication
Teaming up with OWOX BI for BI data analytics
Using Google Cloud and BigQuery for data integration
OWOX BI Pipeline for data collection and processing
Migrated data processing infrastructure to Google Cloud
Integrated new data science tools with the data pipeline
Use Pub/Sub for scalable and responsive event-driven architecture
"Automate processes with Google Cloud tools like Cloud Vision API, App Engine, and Cloud Functions"
Google Workspace for collaborative tools
SAP migration to Google Cloud
BigQuery for data processing
Google Cloud Cortex Framework for enriched data solutions
Adopting Google Cloud
Using BigQuery for data foundation
Partnering with Quantum Metric
Switched from homegrown data analytics solution to Looker
Integrated Looker with other tools like Zendesk and Braze
Used Looker for geospatial functionality and real-time analytics
Migrating applications and workloads to Google Cloud
Using Google Kubernetes Engine (GKE) for managing and scaling clusters
migrate mission-critical SAP applications to Google Cloud
use Google Cloud solutions for IT provisioning
collaborate with Google Cloud partner NTT
Move applications to Google Cloud
Use Google Cloud VMware Engine
Build a recommendation engine
Manage APIs with Apigee
Use BigQuery for data analysis
Implement FinOps techniques
Building own commerce enablement engine on Google Cloud
Utilizing Google Cloud Marketplace Integration Services
Collaborating with SADA and Google Cloud
Migration to Google Cloud
Use of O3 Enterprise ZeeroMED suite
Google Cloud migration
Hybrid environment focusing on cloud-first strategy
Utilization of Google Workspace and advanced data analytics tools
Chose Firebase for its simplicity and intuitive dashboards
Deployed Cloud Functions for Firebase as a serverless framework
Used Cloud Pub/Sub and BigQuery for data pipelines and predictive analytics
combination of multinational cloud services and Google Cloud services
use of Google BigQuery and Cloud APIs
increased reliance on Google Cloud and Firebase
use of Google Cloud services in multiple regions
adopting cloud technology
using Google Cloud’s team expertise
Using managed services
Adopting Google Kubernetes Engine for container orchestration
Moving to Google Cloud services for reliability and performance
Use of Google Kubernetes Engine for container orchestration
Internal resources for deployment
Multi-cloud infrastructure
Using microservices with built-in redundancy
Implementing Google Cloud tools progressively
Adoption of Google Cloud and BigQuery
Use of BigQuery Data Transfer Service for daily imports
Partitioning and clustering for query optimization
Utilization of Looker for data visualization
Breaking code base into microservices
Using Kubernetes for container orchestration
Consulting MediaAgility for best practices
Using Google Cloud and its resources
Employing preemptible virtual machine instances through Compute Engine
Migration to Google Cloud Platform
Establishing a robust data warehouse and data pipeline
Utilizing BigQuery for fast query speeds
"Employing various Google Cloud services (e.g., Cloud Pub/Sub, Cloud Dataprep)"
Rearchitected for the cloud
Shifted data centers to the cloud in four-month sprints
Right-sized system requirements
Adoption of Google Kubernetes Engine (GKE)
Use of BigQuery and Google Workspace
Migration to Jakarta Cloud Region
Deployment of Cloud Vision API for OCR and fraud detection
Use of BigQuery and Looker Studio for data dashboards
Setting up ETL pipeline with CTS
Regular meetings to tweak dashboards based on recommendations
Leveraging Google Cloud-managed services for autoscaling
Using Google Cloud to scale quickly without incurring too much cost
Seamlessly migrating workloads to Google Cloud without downtime
Setting up CI/CD pipelines to automate deployments on Google Kubernetes Engine
Collaboration with Google and SpringML
Use of TensorFlow models
Analyzing video footage for detection
Integration of AI and ML into city operations
Using Google BigQuery to create a unified customer database
Implementing OWOX BI Pipeline for real-time data integration
Migrating to Google Kubernetes Engine (GKE)
Working with Devoteam (Google Cloud Managed Services Partner)
Using Google BigQuery for data storage and processing
Using Pub/Sub for data ingestion
Becoming a Google for Education Build Partner
Integrating with Google Workspace and Google Classroom
Migrating to Google Cloud
Adopted Google Cloud and BigQuery for data analytics
Integrated Security Command Center for unified security management
Leveraging Google Cloud services
Using Google Pipelines API and Compute Engine
Employing open source software
Migration to Google Cloud
Using Cloud Monitoring
Containerization with Google Kubernetes Engine
Optimization of code for GPUs on Google Cloud
Utilization of Google Cloud’s C2 HPC instances with 2nd Generation Intel Xeon Processors
Use of Google Cloud Startup Program
Adoption of Kubernetes Engine for scalability and automation
Partnership with Google Cloud and Chromebook
Adoption of a fully managed service
Integration of Speech-to-Text for faster video editing
Partnering with Google Cloud
Using Pub/Sub real-time messaging service
Reimagining data connectivity and delivery model
Consolidating global data in a data lake on Google BigQuery.
Building reusable data-migration patterns and leveraging automation.
Developing the CNA Model Factory with Google Vertex AI.
Implementing proper data governance and metadata practices.
Iterative 'fail fast' development model
Analyzing user patterns with BigQuery
Optimizing GPU usage
Use of BigQuery for data warehousing
Utilizing Compute Engine for running validation services
Future expansion with Cloud Bigtable and Google Kubernetes Engine
"Use of Compute Engine, Cloud SQL, and Google Kubernetes Engine"
Adoption of microservices approach
Use of Cloud IAM
Partnering with Nuvem Mestra
Using Google Workspace for Education tools
Teacher training on Google Workspace for Education
Migrated to Google Cloud and Cloud Spanner
Utilized Google Kubernetes Engine for containerization
Employed BigQuery for data analysis
Migrated from on-premises technologies to Google Cloud Platform and Tableau
Utilized Google BigQuery for faster data processing
Implemented probabilistic record linkage algorithms
Migrated systems to Google Cloud
Partnership with Teraflow.ai
Proof of concept to assess cloud providers
Adoption of Google Kubernetes Engine (GKE)
Using Terraform for automation
Utilizing managed services and persistent volumes
Built an analytics data architecture with Looker and Google Cloud
Created COVID-19 monitoring dashboards within a day
Infused COVID-19 information throughout existing clinical dashboards
Rapid iteration on dashboards to keep pace with the pandemic
Partnering with Google Cloud for infrastructure
Using App Engine for scalable application
Leveraging Firestore for web-based interface
Integrating multiple provider systems
Migration to Google Cloud
Partnership with 66degrees for managed services
Pivot to SaaS
Engaged cloud managed services provider Riley
Used Buildkite CI/CD platform
Containerized application environment with Kubernetes
Used Cloud SQL for relational database
Used BigQuery for data analysis
Trial and hack-week with BigQuery and Google Analytics Premium
Support from Google’s service partner Datatonic
Integration of Cloud Dataflow
Use of Google Cloud and BigQuery to store and analyze data
Integration with PV systems through inverters
Utilized Google Cloud solutions
"Leveraged Kin + Carta for customer experience, user research, and app development"
Adopted TensorFlow for computer vision algorithms
Migrated data to BigQuery with the help of Google Cloud partner WebEye
Connected BigQuery with Google Analytics and Firebase
Used Looker for report generation
Partnering with Sakura Sky
Implementing Google Cloud
Deploying StratosMedia digital content management solution
Adopted Google Cloud for big data and analytics capabilities
Utilized Google Cloud for Startups program for technical documentation and training
Deployed credit scoring module on Cloud Functions
Built CI/CD pipelines on Cloud Run
Selection of specific Google Cloud services
Focus on generating quality code
Well-designed infrastructure
Migration to Google Cloud
Adoption of Google Kubernetes Engine
Use of Google Cloud's managed services
Migration to Google Cloud
Adoption of Google Kubernetes Engine
Implementation of BigQuery
Adoption of Google Workspace
Using IoT solutions
Asset Tracking
Predictive Maintenance
Remote Monitoring
Switch to Google Cloud Storage
Use Google Cloud managed services
Behavior analysis with Google BigQuery and Firebase
Adopting the data mesh approach using BigQuery
Creating a global catalog for data
Using a common infrastructure (Data Hub) with BigQuery
Collaboration with Google Cloud partner Aliz
Migrated SAP to Compute Engine on Google Cloud
Implemented two application servers in two different zones
Enabled disaster recovery in Taiwan via asynchronous database replication
Using Google Cloud with Intel Xeon Scalable processors
Leveraging Compute Engine for scalability
Utilizing historical satellite imagery for back-testing models
Using Google Cloud to scale at speed
Using Compute Engine for rapid VM spin-up
Using Cloud Storage for machine and file efficiency
Utilizing Google Cloud's multizone architecture
Adopting a multi-cloud strategy
Using Google Cloud VMware Engine
Implementing a hybrid-cloud strategy
Creating a continuous integration and continuous delivery (CI/CD) pipeline
Use of Google Cloud from the very beginning
"Utilization of managed services, open source software, and BigQuery"
Adoption of event-driven programming and serverless technologies
Consulted Google Cloud Premier Partner Intelligence Partner
"Implemented Google Cloud solutions including Google Workspace, BigQuery, Compute Engine, and App Engine"
Developed a hybrid cloud smart analytics solution
Created weekly dashboard reports and scorecards
Use of Google App Engine for scalable infrastructure
Adoption of Google Cloud Platform for core business operations
Integration with Google Workspace
Adopt modern technologies
Migrate on-prem data to BigQuery
Use Google Kubernetes Engine
Employ data engineering and machine learning
Collaboration with Google Cloud team
Implementation of Recommendations AI
Use of multiple Google Cloud solutions
Leveraged blockchain and AI
Tagged every fruit with AI image analysis
Created unique blockchain records
Utilized Google Cloud solutions
use Google Cloud for security features and scalability
collaborate with DoiT International for implementation
Migrating to Google Cloud with Go Reply
Building a containerized architecture using Google Kubernetes Engine
Implementing Terraform
Using BigQuery for real-time analysis
Employing Apigee for API management
Leveraging AI/ML technologies on Google Cloud
Utilizing Firebase for user behavior understanding
Migration to Google Cloud
Using Google Kubernetes Engine and App Engine
Microservices architecture
Google Cloud Platform
Google Kubernetes Engine
Container-based solution
"Use Google Analytics Premium, Google Tag Manager, and BigQuery to integrate digital data sources and CRM data"
Deploying Google Tag Manager across apps and websites
Connecting CRM data with digital analytics data using BigQuery
Leveraged highly scalable infrastructure and security solutions for hybrid cloud of Google Cloud
Commissioned by the Taitung County government to develop an innovative app
Using Google Cloud tools
Orchestrating ETLs with Cloud Composer
Using Dataflow for data ingestion
Employing BigQuery as a data warehouse
Automating setups with Cloud Datastore
Partnered with HVAR
Mapped all data sources and sets
Used Looker combined with BigQuery
Provided training courses
Partnership with Quantiphi and Google Cloud
Developed a Knowledge Graph prototype in 10 weeks
Moving to Google Cloud
Using managed services
Configuring custom machine types
Using preemptible virtual machines
Partnered with Google Cloud and Pluto7
Built a FHIR-based CDSIC and EHR record ingestion and harmonization process
Adopted Google Cloud healthcare-specific solutions
Use of Google Cloud services
"Outsourcing installation, administration, and maintenance tasks"
Overhauling decision architecture
Using managed services such as Dataproc and Dataflow
Migrated VM structure to microservices
Implemented production database replication architecture
Used real-time data ingestion tools
Migrated to Google Cloud
Used Anthos for building and deploying cloud-native apps
Employed BigQuery for data analytics
Used Google Kubernetes Engine for microservices
using Google Cloud infrastructure
employing Firebase and Cloud Functions
installing pre-configured sensors
developing as a service products
Leveraged managed services to minimize maintenance burden
Partnered with IT consultancy Computas for strategic support
Teamed up with Google Cloud Partner Aliz
Adopted Google Cloud's mature AI capabilities and ease of use
Using Google Kubernetes Engine (GKE)
Building a microservices architecture
Using BigQuery for analytics
Maintaining a lean team
Moved from on-premises to Google Cloud Platform
Utilized App Engine for scalable API access
"Employed Cloud Pub/Sub, Cloud Dataflow, Cloud Bigtable, and BigQuery for data processing and analytics"
Use predictive analytics
Leverage high-resolution geospatial data and machine learning
Use of Atlas AI’s Aperture platform
Adopting Google Cloud solutions
"Using Compute Engine, Networking Products, Cloud Storage, and BigQuery"
Centralizing infrastructure management
Adopt microservices architecture
Use Google Kubernetes Engine
Leverage Google Cloud's open source principles
"Use Cloud Bigtable, Cloud SQL, and Dataflow for data processing"
Implement Firebase for app development
Establishing BigQuery as its data warehouse
"Using Google Kubernetes Engine to automatically deploy, scale, and manage clusters"
Leveraging Compute Engine for machine learning workloads
Integrating Google Cloud environment with multiple data sources
Utilizing Google Cloud solutions
Standardizing processes with Google Cloud
Migrating to Google Cloud
Using Google Kubernetes Engine for scalability
Using Cloud SQL for managed databases
Partnering with Google Cloud
Collaborating with Bip
Using BigQuery and Vertex AI
Implementing Bip xTech Cognitive Suite
Worked with Google Cloud Partner Informatica
Migrated data analytics and warehousing systems onto Google Cloud
Used Equinix ECX Fabric to build a hybrid architecture
real-time integration with external platforms using Pub/Sub
analyzing shipping data with BigQuery
building a no-code platform using Cloud Run
scaling on App Engine
Migration to the cloud
Use of collaboration tools like Google Workspace
Leveraging machine learning and AI
Collaborated with Cirruseo
Built applications on App Engine
Integrated with Google Workspace using single sign-on
Used Google Cloud
Utilized Google Kubernetes Engine for rapid development
Implemented Pub/Sub for fast reporting and optimization
Adapting architecture for maximum efficiency
Using Google Cloud's product ecosystem
Switching from monolithic setup to microservices
Partnered with Google Cloud
Implemented Chronicle SOAR
Migrating to Google Cloud
Partnering with IPNET for technical support
Adopting BigQuery as central data warehouse
Combining Vertex AI with BigQuery
Deploying Looker from Google Cloud
Connecting Faire's cloud database to Looker
Using Looker's semantic model LookML
Identifying datasets with labels easily understood by business users
Working with Looker's Professional Services
Consolidating data ecosystem on Google Cloud
Creating a data lake for the platform
"Implementing advanced analytics, data engineering, and data science capabilities"
"Selecting Looker for semantic layer, integration with GitLab, and stored engine capabilities"
Using BigQuery for storing databases
"Deploying, managing, and scaling website applications using Google Kubernetes Engine"
Utilizing Cloud Storage for unstructured data
Employing Cloud Run for easier code deployment
Utilize Google Cloud Platform's BigQuery
Implement Raw Data solution by fifty-five
Partnering with Nohup for implementation and management
Using Google Cloud services for scalability and performance
Aggregating online and offline data for comprehensive analysis
Containerization
Automated deployment
Logging
Cross-cloud strategy for migration
"Using Google Cloud services like Kubernetes, Compute Engine, Cloud SQL, Cloud Build"
Using Google BigQuery for data processing
Deploying microservices architecture on Google Kubernetes Engine
Migrated to Google Cloud
Utilized Google Kubernetes Engine
Implemented Cloud Dataflow for data processing
Used Cloud Machine Learning Engine for AI model training
Migration to Google Cloud
Use of Google Kubernetes Engine
Leveraging Fastly for global edge cloud platform
Building a data lake
Migrating to Google Workspace
Using BigQuery on Google Cloud
"Integration with Dataflow, Cloud Composer, and Compute Engine"
Migrating to Google Cloud
"Using Compute Engine, Cloud Storage, Cloud Pub/Sub, and Cloud Dataflow"
Switching to BigQuery and Cloud Datalab for data science models
Using Google Analytics 360 to manage and track content
Driving revenue and innovative advertising solutions with Google Ad Manager
Migration to Google Cloud
Using AutoML Natural Language and Cloud AI building blocks
Partnering with Google Cloud
"Utilizing auto-scaling solutions, encrypted database services, and straightforward pricing"
Developing an app for district-wide communication and safety
"Offer more agile, affordable, and future-proofed services"
Use Google Cloud to accelerate client implementations
Evaluate and present Google Cloud Platform as the right option
Migrated from a monolithic to a microservices environment
"Collaborated with Google Cloud partner, Somos Nuvem"
Used extensive documentation and support from Google Cloud
Using Google Cloud to analyze datasets and develop algorithms.
Participation in Data Solutions for Change program by Google.
Used Google Cloud products
Shifted from Kubernetes to Firebase and Cloud Functions
Implemented Cloud Composer for workflow orchestration
Using Google Maps Platform and Google Cloud solutions
Creating dashboards using Google BigQuery
Synchronizing demand profile using Sheets
Building platform on Google Cloud
Moving to microservices
Using Google Kubernetes Engine (GKE)
Using BigQuery for analytics
Using Google Workspace for collaboration
Market Prioritization 2x2 Framework
Google Cloud data infrastructure
Google Analytics Measurement Protocol
Customizing content and creatives for non-English markets
Migrated games to Google Cloud
Used VMs on Compute Engine
Employed Cloud Load Balancing and Cloud CDN
Utilized Cloud Armor for DDoS protection
Implemented Cloud Monitoring
Worked with MeshCloud for system architecture
Migrating to Google Cloud
"Building on Google Cloud data analytics services like BigQuery, Cloud Dataflow, and Cloud Dataproc"
Used Google Cloud's global network and Translation AI
Adopted Cloud Load Balancing to reduce latency
Migrated data to BigQuery for near real-time analytics
Implemented Looker for data visualization and insights
Utilized Firebase for efficient A/B testing and ad performance tracking
Leveraged Vertex AI for ad conversion predictions
Use of Google Cloud
Hosting game solutions on Google Cloud infrastructure
"Leveraging Compute Engine, BigQuery, Cloud Storage, and Cloud Logging"
Switching to Google Cloud
Migrating to a Kubernetes-based architecture
Centralizing control over infrastructure
Consolidation of monitoring activities into Google Cloud's operations suite
Use of cloud-native tools
Real-time metrics for monitoring
Use of curated dashboards
Migrated to Google Kubernetes Engine (GKE) on Google Cloud
Implemented autoscale feature in Google Kubernetes Engine
Used Firebase for mobile apps and web app hosting
Developed data analytics project with BigQuery
Moved to a microservices ecosystem
Rebuilt front end with GraphQL and React
Used Google Cloud learning resources
Migrated server infrastructure to Google Cloud
Leveraged Google BigQuery for data storage and analysis
Used Google Kubernetes Engine for infrastructure agility and scale
Utilized TensorFlow for machine learning models
Investing in Google Cloud technology
Focusing entirely on Google Cloud products
"Implemented Cloud SQL, Cloud Storage, App Engine"
Used Google Cloud's intuitive tools for infrastructure setup
Use of RISE with SAP
Migration to Google Cloud infrastructure
Closing data centers
Use of multicloud setup
Moving hosted services to Google Cloud
Using Rackspace Technology for managed services
Using satellite data to track the movement of fishing boats
Machine learning to automate pattern detection
Cloud computing for data processing and analysis
Disrupting the tools employees used every day
Migrating the workforce to Google Workspace
Pilot-testing Google Workspace with IT team members and select business teams
Rolling out Google Workspace to the broader workforce in a big-bang approach
Competition between Google and another provider
Hackathon programs
Enterprise workshops
Training and workshops via Globe University campuses
Migrating data warehouse to Google Cloud
"Using Google Cloud managed services like Pub/Sub, Cloud Scheduler, Dataflow"
Migrate work to the cloud
Build product around Google Cloud from the start
Switching to Google Cloud
Using BigQuery for real-time insights
Leveraging Cloud Functions for faster time to market
"Implementing Cloud Run, Pub/Sub, Cloud Logging, and Cloud Scheduler"
Phased migration approach
"Using Google Cloud services such as GKE, Compute Engine, Container Registry, Cloud Pub/Sub, Cloud Storage, Cloud Dataflow, and BigQuery"
Using BeyondCorp principles
Evaluated multiple data solutions
Built a core data model based on key performance indicators
Integrated insights across all teams and data sources
Developed a data infrastructure with Looker on top
Migrated to Google Cloud
Use of Kubernetes container clusters
Centralized data pipeline using Cloud Composer
Serverless real-time data streaming with Pub/Sub and Dataflow
Migrated to Google Cloud
Implemented microservices architecture
Used Google Kubernetes Engine and Cloud SQL
Partnered with Rackspace Technology
Consolidate functions into a single cloud-based web application
Use of detailed analysis to determine necessary features
Identification of suitable Google Cloud services
Migrated to Google Cloud
Adopted Google Workspace
Worked with implementation partner Sastrify
Complement physical infrastructure with Google Cloud
Use cloud to secure non-critical operations
Identify opportunities for cloud infrastructure in quantitative development
Leverage TensorFlow and Cloud Bigtable for machine learning
Use of public cloud service
Engagement with Searce for optimization
Migrated website infrastructure to Google Cloud
Used managed services and autoscaling
Containerized infrastructure using Docker
Integration of programmatic data into BigQuery
Building custom dashboards in Looker
Upskilling internally using Google Cloud documentation and user forums
Implementing Google Workspace for real-time collaboration
Using serverless architecture to avoid managing infrastructure
"Leveraging Firebase, Filestore, Cloud Functions, and other Google Cloud tools"
Using Google Kubernetes Engine for scalability
Utilizing BigQuery for data analysis and decision-making
digitization of archives
partnering with Google News Initiative and Assetway
deployment of Assetway Media Center platform
manual adjustments for indexation
Migration to Google Cloud
Partnership with IPNET Growth Partner
Creation of four data layers
Migrated to Google Cloud
Used Vertex AI for ML model training
Set up VMs in Compute Engine
Leveraged BigQuery and Dataflow
Migrated to Google Cloud in stages
Used Google Kubernetes Engine for scalability
Utilized BigQuery and Looker for data analytics
Migration to Google Cloud
Use of Firebase for mobile backend
BigQuery for analytics data storage
Cloud Run for API deployment
Using historical customer data to tailor key touchpoints
"Integrating Google Cloud services such as BigQuery, Firebase, Cloud Functions, and Pub/Sub"
A/B testing with Recommendations AI and manual systems
Adoption of Google Cloud
Switching to Platform as a Service (PaaS)
Stress testing prototypes
Interconnected platforms for smooth transition
Migrated to a microservices architecture
Used managed solutions on Google Cloud
Brought DevOps in house
Embedding Looker in the platform
Creating a unified semantic data layer
Using Looker to build dashboards for Cloud Cost Management
Building aggregate tables for large datasets
Leveraged Looker data application platform to launch Harver Insights
"Looker's Git-versioned modeling layer, LookML, for defining metrics"
Transition from legacy system
Use of Google Cloud's analytics solutions
Automated machine learning resources
Build a centralized data portal (NRP)
Work with technology services provider SADA and Google Cloud
Use proven technologies like Google Cloud solutions
"Adopting a dynamic, near-term-focused technology strategy"
Moving from virtual machines to containerization through Kubernetes
"Porting products, services, and data to Google Cloud regions"
Migrating SAP S/4HANA landscape and call center environments to Compute Engine
Using Google Cloud tools like BigQuery and Google Workspace
Implementing custom-built solutions on App Engine
Proof of concept for performance and pricing with Google Cloud
Use of TensorFlow and Google Cloud
Collaboration with Google Cloud Partner CloudMile
Development of a mobile application
Seamless migration to Google Cloud
Proof of concept with CloudCover
Using managed services
Training complex models on TensorFlow
Developed a proof of concept for a visual inspection system
Leveraged AI at manufacturing sites
Used Visual Inspection AI by Google Cloud
Using Google Kubernetes Engine (GKE)
Partnering with Devoteam G Cloud
Using BigQuery and AutoML for data insights
Deploying machine learning models for crew optimization
Digitizing work orders with Vision AI
Overhaul existing revenue attribution models
Partner with OWOX to create end-to-end analytics system
Use Google BigQuery for high-powered analysis
Automate advertising bids
Adoption of Google Kubernetes Engine (GKE)
Containerization and managed clusters
Training with Devoteam and Cirruseo
Switch to a DevOps mentality
Migration to Google Cloud with support from Kartaca
Reworking infrastructure to use containerization and microservices
Running pre-production environments on Google Cloud before full deployment
Embrace cloud technologies for scalability and speed
Daily 15-minute standups with visual output
Migrated to Google Cloud
Utilized BigQuery and Looker Studio for data analytics
Implemented Google Workspace for internal and external collaborations
Switch to Google Cloud
Use of App Engine for fast app development
Firebase Authentication for user authentication
"Developing a single, scalable modern data stack"
Breaking architecture into manageable blocks
Leveraging native Google Cloud integrations
Using managed services and self-hosted tooling
Creating a privacy-centric customer data platform (CDP)
Migration to Google Cloud
Use of Apache Beam and Dataflow
participation in Google for Startups Accelerator
implementing microservices for cost savings and scalability
adopting various Google Cloud solutions
Adopting Google Kubernetes Engine for orchestration.
Using a microservices architecture on Kubernetes.
Continuous development and deployment.
Partnering with Google Cloud
Iterative growth and improvement
"Using Google Kubernetes Engine, Compute Engine, Cloud Functions, Certificate Authority Service"
Support from Google Cloud Premier Partner (Devoteam)
Leveraging Google Cloud managed services
Migrating to Google Kubernetes Engine (GKE)
Using detailed documentation to complete infrastructure migration
Implementing Google Workspace for collaboration
Use of Google Cloud AI machine learning APIs
Integration of multiple data streams
Flexible pricing model
Use big data and machine learning
Collaborate with ecommerce platforms
Utilize Google Cloud for data processing and scalability
Run sophisticated ML models for risk assessment
"Developed Turbo, an insurance management system on Google Cloud"
Adopted a microservices architecture on Google Kubernetes Engine
Real-time data monitoring and bulk data transfer with Looker Studio
Implemented Google Cloud
Used Google Workspace and Chromebooks
"Adopted Cloud Functions, Pub/Sub, and other Google Cloud tools"
migrating to Google Cloud
autoscaling instance groups
phased migration approach
Leveraging Google Cloud services
Using Google Kubernetes Engine and Compute Engine for scalability
Employing Cloud IAM for access management
Opting for a multi-cloud architecture
Phased migration approach
Performing proof of concept with multiple providers
Migrating to Google Cloud with minimal disruption
Using Google Cloud Partner Devoteam for support
migration to Google Cloud
deployment of Rhymes ERP solution
use of container technology like Docker and Kubernetes
using BigQuery for centralized data warehouse
using Compute Engine for model training
using Looker Studio for report generation
using Vision AI for image analysis and transcription
Migrating infrastructure to Google Cloud
Using Terraform’s infrastructure-as-a-code approach
Leveraging Terraform to provision containers in Kubernetes clusters
Migration to Media CDN on Google Cloud
Switching CDN providers
Integrating Google Cloud Armor and BigQuery
Migrating applications to Google Cloud
Using Cloud Vision AI to safeguard the online community
Adopting serverless products like Google Cloud
Maintaining backwards compatibility during migration
data-led insights for prediction and optimization
using AI to turn data into prescriptive and predictive solutions
"use of Google Cloud services like Vertex AI, Cloud Storage, BigQuery, and Document AI"
Partnering with Nana Bianca
Using Google Cloud
Replacing previous infrastructure with Kubernetes clusters managed with Google Kubernetes Engine
Setting up a queueing system with Google Cloud Pub/Sub
Using Google Cloud Storage and Google BigQuery
Using a serverless development and analytics environment
Leveraging Google Cloud Platform including App Engine and BigQuery
Utilizing machine learning models for data analysis
Google Kubernetes Engine for high load microservices
Use of Google Cloud technology
Integration of BigQuery with familiar tools
Development of user-defined functions (UDFs) for statistical tests
Developing new tools with Google Cloud
Using Google Kubernetes Engine for auto-scaling
Teaming up with Google Cloud partner SFEIR
Migrated on-premises Netezza to Snowflake on Google Cloud
Used Google Cloud’s serverless technology
Adopted a phased approach to migration
"Implemented product recommender systems, optimized pricing, and built a loyalty program"
Migrating to Google Cloud
Setting up VMs on Google Kubernetes Engine
"Using Cloud SQL, Dataproc, and MongoDB Atlas"
Partnering with Persistent for migration
Partnered with Google Cloud
Leveraged ML and analytics tools
Built an AI model for predictions
Utilized a cloud-based infrastructure
Using Google Kubernetes Engine Autopilot
Using BigQuery
Using Cloud Load Balancing
Using Confidential Computing solution
Leveraging Google Cloud for end-to-end development
Using Vertex AI and BigQuery for accurate AI model development
Utilizing extensive teaching materials and documentation for training
Partnering with SpringML
Building a data lake strategy
Adopting Google Cloud solutions
Adoption of Google Cloud solutions
Partnership with Devoteam G Cloud
Implementation of BigQuery for data analytics
Use of Natural Language AI for sentiment analysis
Compute Engine for infrastructure tasks
Security Command Center for threat detection
Built a custom viewer analytics solution with managed services on Google Cloud
"Used Cloud Pub/Sub, Cloud Dataflow, and BigQuery for data processing and storage"
Built CDP on Google Cloud
"Leveraged Google Ads, Google Analytics 4, and Campaign Manager 360"
Used first-party data for campaigns
Using Google Cloud and BigQuery for big data analytics
Deploying a multi-cloud architecture
Integration with Google Maps Platform and APIs
Use of Google Optimize for A/B testing
Integration with Google Analytics and BigQuery
Collaborating with Riley for project execution
Migrating data warehouse to Google Cloud
Using BigQuery for data storage and analysis
Implementing infrastructure as code
Using Google Cloud managed services
Containerized infrastructure with Kubernetes
Utilizing Compute Engine and Google Kubernetes Engine
Funding and support from Google Cloud program for startups
Consolidate technical architecture
Use Google Kubernetes Engine for containerized apps
Leverage BigQuery and Dataflow
Cloud-based digital reinvention
"Adoption of Google Cloud solutions (App Engine, Firebase, BigQuery)"
Migrated to Google Cloud
Worked closely with Google Cloud Premier Partner Netpremacy
Integrated BigQuery and Google Kubernetes Engine
Built a one-stop platform called JKOS
Run JKOS in a public cloud environment
Leverage third-party software for data analytics
Employ virtual machines on Compute Engine
Use Cloud Load Balancing and Cloud CDN
Moved to Google Cloud Platform
Consolidated onto Google Compute Engine
Launched business on Google Workspace
Implemented Cloud Bigtable for data storage
Used Cloud Memorystore for Redis instances
Migrated normalization and deduplication software to Google Kubernetes Engine
Deployed multi-regional resources with Terraform
Migrating to Google Kubernetes Engine for container management
Using BigQuery for data processing
Parallel deployment on old and new platforms
Customizing infrastructure configurations
Migrating to Google Cloud in stages
Using Kubernetes and Google Kubernetes Engine for better scalability
Incorporating Cloud Functions and Cloud Run for automation
Adopt Google Cloud for scalability and innovation
Use Firebase A/B Testing to segment users and test features
Employ Google Cloud products and services for infrastructure management
Test drive with Google Cloud
Phased migration to Google Cloud
Leveraging Kubernetes for orchestration and deployment
Proactive support and collaboration with Google Cloud team
Lift-and-shift approach to replicate on-premises workloads to Google Cloud
Use of Compute Engine and App Engine for efficient resource management
Adopting Apigee API Management Platform for better API design and management
"Integrating Google Analytics, Google Ads, and YouTube for marketing"
Leveraging machine learning and automation
Partnering with Pluto7 and Google Cloud
Implementing lean process improvements
Chose Google App Engine for hosting and application development
Focused on user experience and content improvements
Collaboration with Google Cloud and University College London
Utilizing Google Kubernetes Engine and BigQuery
Migrating data from physical servers to Cloud SQL
Using BigQuery for real-time insights
Utilizing Looker Studio for data visualization
Partnering with GoPomelo
Google Cloud migration
Automated VM upgrades
High availability connectivity
Use of BigQuery and Looker Studio for data insights
Migrating to Google Cloud
Utilizing Google Compute Engine for scalability
Implementing live migration and data redundancy features
Using Fivetran connectors
Adopting Google BigQuery
Leveraging Google Cloud
Micro-services-based architecture
"Use of Google Cloud tools (e.g., BigQuery, Cloud Composer, Cloud Dataflow, Vertex AI)"
Collaboration with Google Cloud Partner Revolgy
Partnered with Searce for pilot projects and migration
Test run on new cloud provider with metrics outlined
Migrated entire infrastructure to Google Cloud
Use of Google Cloud Machine Learning Engine
Deployment on Google Compute Engine
Move to Google Kubernetes Engine
Evaluation and selection of Google Cloud Platform
Pilot project with Google Compute Engine
migrate applications to Google App Engine
use BigQuery for data analytics
use auto scaling features of Google App Engine
use instance groups in Google Compute Engine
Using Google Kubernetes Engine
Implementing Istio and Cloud Logging for monitoring
Adopting a rolling release model
Utilizing Google Cloud for infrastructure
Leveraging Firebase for mobile app development
Using BigQuery for storing session logs and gaining insights
Employing Looker Studio for analytic visualizations
Migrating to Cloud SQL for data storage
Infrastructure on demand with Google Compute Engine
Detailed tracking with Stackdriver
Collaboration with Google Partner ICOA
Moved to Google Cloud for scalability and performance
"Adopted Google Kubernetes Engine for managed, scalable containerized environment"
migrated core workforce to Google Workspace
integrated BigQuery with Sheets
phased migration to Google Cloud
adoption of Google Kubernetes Engine
Proof of concept with Google Cloud
Migration to Google Cloud
Utilizing Google Cloud services like Cloud Storage and Compute Engine
Partnered with Fearless Technology Group
Adopted Google Cloud
Implemented Kubernetes for containerized applications
Used Google Kubernetes Engine for streamlined upgrades
Implementing Looker
Building a consistent data architecture
Leveraging modern technology stack
Migrated data to Google Cloud
Used Looker for business intelligence
Integrated data across departments
Chose Google Cloud Platform for cost efficiency and features
"Used Google App Engine, Google Compute Engine, and Google Kubernetes Engine"
Leveraged Google Cloud APIs and Datastore for efficiency
Switching DNS to a new platform
Using Google Cloud Kubernetes clusters
Partnering with Claranet
Deploying the entire platform live instantly
Moving applications to Google Cloud
Implementing Google Kubernetes Engine
Detailed access permissions with Google Workspace and Google Cloud SQL
Monitoring stack with Google Operations suite
Migrating to Google Cloud
Implementing AI-based recommendation model
Using Looker for reporting
"Deploying BigQuery, Cloud Storage, Cloud Composer, and Compute Engine"
"Leveraging Dataproc, Cloud Bigtable, and BigQuery"
Running entire operations on Google Cloud
Migrating with the help of Searce
Using Google Kubernetes Engine for scalability
Decision to use Google Cloud
Implementation of BigQuery and Dataflow
Development of containerized infrastructure on Google Kubernetes Engine
Use of Vertex AI for scaling machine learning models
Moving from an internet data center to a cloud-based solution
Adopting BigQuery and Holistics for data processing and visualization
Conducting a comprehensive market analysis of systems
Leveraging blockchain technology
Using Google Kubernetes Engine
Implementing Virtual Private Cloud
Deploying Shielded VMs
Build SAP systems on Google Cloud
Use BigQuery for data processing
Link data globally using Cloud Interconnect and IP/MPLS
Implement serverless data pipelines with Cloud Functions and Workflows
Utilized Google Cloud's autoscaling features
Leveraged Google Cloud’s flexible pricing
Adopted Google Compute Engine for server infrastructure
migrating to Google Kubernetes Engine
using Google Cloud Build for automation
working with Rackspace for expertise and planning
integrating Cloud Pub/Sub and Cloud Functions for monitoring and updates
Adoption of Google Cloud solutions
Use of BigQuery for analytic purposes
Integration of Datastore for real-time data
Implementation of App Engine for API processing
Use of Pub/Sub for campaign delivery
Adoption of Looker for dashboards
Migrated from an on-premises setup to Google Cloud
Used Compute Engine for proof of concept
Implemented Google Kubernetes Engine
migrating to Google Cloud
adopting serverless managed services
collaborative approach with data scientists and engineers
Built platform on Google Cloud
Focus on market fit
Use serverless architecture with App Engine
Migrated to microservices with Google Kubernetes Engine (GKE)
Moved databases and applications to Google Cloud
Replaced existing messaging solution with Cloud Pub/Sub
Moved database to Cloud Bigtable
Enabled open access to customer data warehouses via BigQuery
Use of Google Cloud for dynamic scaling
Leverage Kubernetes for test execution
Focus on serverless products
integrated Dataflow for data processing
used Cloud Bigtable for data storage
employed Cloud SQL for main domain database
used IoT Core for device management
utilized Google Kubernetes Engine and Cloud Run for frequent requests
Multicloud strategy
Automating the data center and reducing provisioning times
Co-innovation strategy with Google Cloud
Transition from legacy data visualization platform to Looker
Internal training of teams
"Utilized Google Cloud to build a scalable, unified data warehouse"
Ingested data from Google Analytics and other datastreams using third party data processing tools
Used Cloud Functions to run data pipeline processes
Employed Looker Studio and Looker for data analysis and reporting
Integration with Google Cloud
Use of Cloud Bigtable
Adopting Google Kubernetes Engine
Leveraging Vertex AI
Switched to public cloud
Porting development environment to Google Cloud
Use of smaller Compute Engine VMs with automatic load-balancing
Migrating to Google Cloud
Implementing autoscaling with Google Kubernetes Engine
Using Google BigQuery and Cloud Pub/Sub for real-time data
migration to Google Cloud
use of BigQuery
adoption of Anthos
integration with machine learning and analytics tools
Migrated SQL and Kubernetes environments to Google Cloud
"Worked with SADA, a Google Cloud Premier Partner"
Evaluated cloud platforms
Containerizing application with Docker
Using Google Kubernetes Engine for orchestration
Integration with Google Cloud and Datadog
Moved to Google Cloud
Utilized Google machine learning APIs
Implemented Cloud Vision API for image analysis
Used Cloud Data Loss Prevention API for content detection
Modernizing infrastructure with Google Cloud
Pooling data from siloed sources
Using tools like BigQuery and Looker for analytics
Migrated 120 staff to Google Workspace within a week
Spent two weeks on testing before migration
built a hybrid system using Google Cloud
connected MARBLEX's data with the game ecosystem
Implemented Google Cloud speech recognition
Migrated stock availability calls to central contact center
Opened a new voice channel hosted on Google Cloud
Leveraging Google Cloud portfolio
Building proprietary solutions on Google Cloud
Switching to Google Cloud
"Divide customer life cycle into three stages (pre-purchase, purchase, service)"
Developing a smart chatbot using Google Dialogflow
Regular brainstorming between teams
Migrating SAP environments to Google Cloud
Partnering with Capgemini
Implementing SAP S/4HANA
Reuse of services across several touchpoints
Modular architecture
Exposure of services to multiple consumers within and outside of the organization
Moved from another cloud provider to Google Cloud
Used Kubernetes Engine for container management
Utilized Google Cloud managed services
Migrated to Google Cloud
Consolidated multiple servers and virtual machines
Implemented secure cloud development environment
Migrated to Google Cloud
Consolidated platforms to AI-ready infrastructure
"Used Google Cloud services like Compute Engine, Cloud Load Balancing, Cloud SQL"
Implemented CI/CD pipeline
Used Flutter for front-end development
deploy Google Cloud
use Google Kubernetes Engine (GKE)
utilize BigQuery for data analysis
implement Firebase for app development and split testing
Migrated from virtual environment to Google Cloud
Use of managed services including Cloud Storage and Compute Engine
Automatic offload of data into more affordable storage tiers
use of Google Cloud's managed services
integration with open-source products like Apache Beam and TensorFlow
utilizing GitHub integration on Cloud Build for continuous integration and deployment
Deep dive into Google Cloud’s solutions
Partner support from Avenue Code
Load testing before migration
implemented microservices architecture
adopted Google Kubernetes Engine
added Cloud Spanner for relational database solutions
Implementation of Looker after trial stages
Setting an end date for previous BI tool
Transitioning to Looker as the common communication channel
Using Cloud AutoML and Cloud Natural Language
Creating a controlled vocabulary of standardized terms
Leveraging Google Cloud for near real-time streaming
Combining public datasets on BigQuery with proprietary algorithms
Worked with Google Cloud and BigQuery
Partnered with SoftServe
Conducted a design-thinking workshop
Migration to Google Cloud
Building a data lake and analytics solution
Collaboration with Google Cloud Partner freiheit.com technologies GmbH
Creation of more than 100 data analytics workbenches
Adoption of Google Kubernetes Engine and Cloud CDN
Migration to Google Cloud from a rival provider
Participation in Google Cloud for Startups Program
Use of preemptible instances and Google Kubernetes Engine
Move workload storage into a cloud-native architecture
Collaborate with technology partner Max Kelsen
meetings
online training
visits to the company headquarters
Migration to Google Cloud
Use of Avenue Code's expertise
Optimizing database architecture
Creating apps in Dataflow
Synchronization between on-prem database and Google Cloud
Opted for cloud-based hosting
Changed cloud provider to Google Cloud
Adopted serverless services
Adopt Google Cloud services
Implement BigQuery-based data architecture
Use Cloud Pub/Sub for event consolidation
Connect mobile apps to App Engine
Use Looker Studio for visualized reports
"Utilization of Google Cloud solutions (e.g., Cloud Run, Cloud SQL, Google Cloud Armor)"
Adoption of App Engine and Google Kubernetes Engine
Deploying news platform in a public cloud
Close collaboration with Google Cloud and iKala Cloud
Using serverless architecture and managed tools
Digitalization of company structure
Creation of a data platform
Use of Looker for data visualization
Partnership with Aeris Communications
Migration to Google Cloud
Use of Aeris Mobility Suite
Optimizing legacy systems
Investing in cloud-based analytical solutions
Using BigQuery for data processing
Using Compute Engine to power insights application
Integrating Tableau with BigQuery
Migrated from legacy BI platform to Looker
Built custom portal for each of the 30 Club teams
Used Looker Blocks for quick implementation
Leveraged LookML for semantic layer and data governance
Migrated services to Google Cloud
Utilized Google Kubernetes Engine (GKE)
Employed Cloud Load Balancing
Used Cloud CDN and Media CDN
Utilizing Google Cloud for unlimited autoscaling
Listing on Google Cloud Marketplace for global expansion
Developing AI decision engines with Google Cloud tools
Investing in human-centric technology
Using Google Cloud and tools like App Engine and Cloud Run
Using a multicloud data strategy
Integrating internal and external datasets
Choosing best-of-breed integratable software
Deploying Looker for self-service analytics
Real-time ETL into Amazon Redshift
Built ML-powered advertising services on Google Cloud
Leverage operational machine learning with Google Cloud services
Migrated workloads to Google Cloud
Worked with Google Cloud partner Searce for migration and architecture
Centralized global platform
Consistent taxonomy
Migrated to Google Cloud
Used Google Kubernetes Engine and BigQuery
Collaborated with Datatonic
Develop a new data platform with Google Cloud
Set out a clear data strategy
Implement a privacy by design approach
Build an end-to-end streaming data pipeline
Centralized data solution on BigQuery
Using Cloud Functions and Pub/Sub for data integration
Operational dashboard with Looker Studio
Adoption of a hybrid cloud using Google Cloud Platform
Using Compute Engine for faster provisioning and better performance
Utilizing Google Cloud for secure data storage and processing
Implementing Cloud Bigtable for scalability and performance
Migrated to Google Cloud
Utilized Google Kubernetes Engine for rapid scaling
Adopted Dataflow for parallel data processing
partnered with Google Cloud
integrated Dialogflow for virtual assistant
ran workshops to define goals
worked closely with everis for implementation
Move systems to the cloud
Use Google Cloud and Fortinet solutions
"Migrate data into BigQuery and use Dataprep, Dataflow, and Data Fusion"
Adoption of microservices-oriented architecture
Use of Google Kubernetes Engine (GKE)
Real-time data collection with open source tools and Google BigQuery
Collaboration and communication between teams
Detailed plans to avoid issues during execution
Early testing
Built machine learning models and language models
Used Google Cloud tools such as Firebase and BigQuery
Migrated to Vertex AI from Open AI
Google Cloud credits for experimentation
fine-tuning infrastructure
working closely with Google Cloud engineering team
using BigQuery for centralizing data
"Adoption of Google Cloud solutions like BigQuery, Cloud Run, and Cloud Composer"
Utilization of AI predictive modeling and recommendations strategy
Migrated to Google Cloud
Leveraged Google Kubernetes Engine for microservices architecture
Implemented BigQuery for data warehousing
Used YugaByte DB for cloud-native data infrastructure
Adoption of Google Cloud and Firebase
"Use of BigQuery, Looker Studio, and Firebase for data and analytics"
Collaborative approach with communities and research partners
Collaborating with Google Genomics
Using Broad Institute’s GATK Best Practices pipeline
Migrating to Google Cloud
Implementing a consolidated cloud architecture
"Adopting BigQuery, Google Kubernetes Engine, and Operations"
Migrating Risk platforms to the cloud
"Training 1,100 employees on Google Cloud Platform"
Using serverless infrastructure
"Leveraging tools like BigQuery, Compute Engine, Google Kubernetes Engine, Cloud SQL, Cloud Storage, and Dataproc"
Unified customer data on BigQuery
Used Google Cloud Consulting for migration
Running datasets and algorithms on Cloud Storage and Cloud SQL
Using Google Kubernetes Engine to simplify DevOps processes
Using CI/CD pipelines on Google Kubernetes Engine
Simplifying on-demand cluster creation with Dataproc
Utilizing Cloud Logging for real-time log management
Run SAP on Google Cloud
Partner with Managecore
Use of TensorFlow for ML models
Deployment on Google Cloud
Custom-configured GPUs on Compute Engine
Migrated VMs to Google Cloud
Autoscaled node pools with Google Kubernetes Engine
Used committed-use discounts on Google Cloud
Using Google Cloud App Engine for system management
Employing BigQuery for marketing analysis
Deploying continuous software updates up to 10 times a day
Using Google Kubernetes Engine
Real-time data streaming on BigQuery
Google Cloud operations suite
Replace on-premises tools with cloud-based platform
Use Google Workspace for real-time collaboration
Leverage state-of-the-art technologies and processes
Ensure smooth rollout with technical support from Online Partner
Provide IT training for non-IT employees
Use of Google Genomics for data analysis
Integration with Google Cloud Platform
Integration with Google Cloud
Use of Looker for data analysis and insights
"Implementation of Google Cloud solutions like Kubernetes Engine, Dataflow, and BigQuery"
Built and hosted bus-tracking system in the cloud
Chose Google Cloud Platform for minimal day-to-day administration
Used Google App Engine for processing telemetry data
Used Google Cloud Datastore for trend analysis
Migration to Google Cloud
High Performance Computing
Adopting Google Kubernetes Engine (GKE)
Migration from monolithic application to microservices-based structure
Using GKE Horizontal Pod Autoscaler and Ingress for load balancing
"Adoption of Google Cloud, Analytics 360, and BigQuery"
Advanced audience segmentation
"Using Google's scalable, reliable infrastructure"
Pay-as-you-go pricing
switched to Google BigQuery
completed a proof of concept in two weeks
migrated all SQL to Google BigQuery in another two weeks
Cloud-first strategy
Infrastructure as code
BeyondCorp security model
Use of open source platform and provider-agnostic tools
Switched to Google Cloud
Developed a containerized infrastructure using Google Kubernetes Engine with Docker
Setup BigQuery to work with Cassandra database
Used Google managed services such as Cloud Pub/Sub and Cloud SQL
Deploy Google Workspace
Build a data lake on BigQuery
Automate onboarding processes
Use Google Cloud Managed Services for migration
migration to Google Cloud Platform
"use of Kubernetes Engine, Google Compute Engine, Google Cloud Pub/Sub, Google Cloud Bigtable, Google Stackdriver"
Implemented advanced contextual monitoring solution
Powered by Quantexa’s CDI platform
Deployed on Google Cloud
Adoption of Google Cloud tools
Incorporation of cloud technology
Using Google Cloud for scalability and performance
Running Docker containers on Container Engine with Kubernetes
Continuous integration and deployment with Jenkins
Adopting Google Kubernetes Engine for microservices
Using Cloud SQL for real-time data
Implementing infrastructure as a code with Terraform
Establishing DevOps best practices with Google Cloud Partner Servian
"created centralized, scalable data warehouse on BigQuery"
used Looker for actionable insights
intelligent connections between Google Cloud tools
Using Google Kubernetes Engine for continuous integration/continuous delivery (CICD)
Automatically scaling and self-healing infrastructure
Using Google Cloud Platform tools like Stackdriver for monitoring and visibility
Use of Google Kubernetes Engine for autoscaling
Adoption of microservices-focused approach
Integration with major analytics platforms
Worked with Google Cloud Partner iKala to identify the most appropriate cloud service
"Used Google Cloud services such as Compute Engine, Cloud Dataproc, BigQuery, TensorFlow, and Cloud Machine Learning Engine"
Moved entire advertising exchange and audience platforms to Google Cloud
Engaged Google Cloud Professional Services
Built a scalable and automated architecture using Google Cloud-managed services
Deployed into seven Google Cloud regions for increased global reach
Redevelop the platform into microservices
Use of GKE managed environment for microservices
Utilize Bigtable for high-performance data processing
Implement Pub/Sub for real-time exchanges
Rely on Cloud Storage for secure data management
Migrate data analytics processes to Google Cloud
Use Teradata VantageCloud with Accenture's expertise
"Collaborate closely with Google Cloud, Teradata, and Accenture"
Rebuild infrastructure with Google Cloud
Use Google Cloud Bigtable for high-performance data processing
Utilize Google Kubernetes Engine for automating infrastructure management
Migrating to BigQuery
Using Google Cloud data security and governance features
Adopted Google’s ZYNC Render
Integrated ZYNC via native plugin with 3D software
Using Google Kubernetes Engine (GKE)
Breaking data storage into stages and using BigQuery
Utilizing Google Workspace for operations
Swapping 80% of on-demand nodes for preemptible VMs
Leveraging Google Cloud stack
Using Google Kubernetes Engine
Migrating to Google Cloud
Automating the ETL process
Utilizing BigQuery and Cloud Dataflow for data processing
Recreating environment from the ground up using Google Cloud products
Adopt Google Cloud services
Work with Searce for migration and implementation
Migrated to GKE for autoscaling
Used BigQuery for data analysis
Integrated Looker Studio for data visualization
Worked with Helix Technologies
Implemented Google Cloud solutions
Used Cloud Load Balancing
Leveraged Google Kubernetes Engine
Used Operations Suite (Cloud Logging and Cloud Monitoring)
Used Google Kubernetes Engine for scalable infrastructure
Moved to service-based architecture
Utilized managed services of Google Cloud
Scanned networks and managed customizable questionnaires
Partnered with Google Cloud
Ran extensive performance and functionality tests
Smooth and transparent migration to minimize customer impact
Adopting Google Cloud's high-performance GPUs and scalable computing products
Utilizing Cloud Storage for cost-effective and secure data storage
Started with internal pilot projects
Deployed internal licensing and customer management databases on App Engine and Compute Engine
Committed PaperCut Views to the platform
"Using Google Workspace for Education, Google Classroom, and Chromebooks"
Creating custom solutions for teaching and learning
Utilizing Google Cloud for integration and compute power
Adoption of Google Kubernetes Engine
Utilization of Cloud Load Balancing
Use of BigQuery for data analytics
Moving from multicloud to integrated Google Cloud architecture
Using Kubernetes-driven architecture
Google Kubernetes Engine as infrastructure backbone
Switching from monolithic applications to microservices
Using Google Kubernetes Engine
Using App Engine for custom microservice applications
Building a new data warehouse on BigQuery
Migrating to Google Cloud
Using Compute Engine and GKE
Adopting BigQuery
Leveraging Google Workspace
Reassessing growth strategy in the cloud
Migrating to Google Cloud
Migrating core applications to Google Cloud
Building a new cloud-native omnichannel platform
Creating an excellent engineering organization
Migrating SAP applications to Google Cloud
Choosing Managecore as a migration and managed services partner
Leveraging Google Cloud for flexibility and management tools
"Using BigQuery, Cloud Datastore, and Cloud Storage for backend support"
Deploying code and managing workloads via App Engine
Leveraging BigQuery for data storage and processing
Implementing Looker for data analysis and reporting
Support from Looker Professional Services
Pinpoints cost-efficiencies with granular Google Cloud Platform pricing
Explore innovative new approaches with Google Cloud teams
Manage a scalable and elastic data warehouse
"Using BigQuery, Dataprep, Dataflow, and Cloud Composer"
Running a detailed education session with Google Cloud Singapore
Ingesting application audit logs and implementing change data capture
"Using Google Kubernetes Engine, Cloud Storage, and Stackdriver"
microservices architecture
containerization
use of Kubernetes
use of Google Cloud solutions
deploying TiDB in a cloud environment
leveraging Google Cloud for deployment
Common way to integrate applications into back office
Use of APIs and Apigee platform
Shared flows for API design
Switch to Google Cloud for development work
"Use Compute Engine for internal environments, development, and testing"
Moved to Google Cloud
Engaged a partner for migration
"Utilized Google Cloud services (e.g., Kubernetes Engine, Compute Engine)"
Adoption of Google Kubernetes Engine
Migration to a public cloud container architecture
Utilization of microservices and containers
Partnering with MediaAgility and using Google Maps Platform and Google Cloud services
Digitizing maps and creating trade zones based on estimated travel times
Using advanced analytics in the BigQuery data warehouse
Moved satellite data processing to Google Cloud Platform
Used Google Cloud Storage for image archive
Worked with Google Cloud consultants and Technical Account Manager
Full migration to Google Cloud
Utilizing Google Cloud Pub/Sub and BigQuery
Building alerts with Google Cloud Logging and Monitoring
Migrated to Google Compute Engine on Google Cloud
Utilized Google Cloud Storage for data encryption
Utilized eye-tracking panel data
Employed Vertex AI machine learning models
Used BigQuery and Cloud Run
Building a data platform with Looker
Integration with Google Cloud
Modifications to SQL codebase
Deploying virtual instances with Google Compute Engine
Comprehensive logging and monitoring with Stackdriver
Using cloned machines with load balancer
Migration from an incumbent cloud service to Google Cloud Platform
"Use of various Google Cloud services like BigQuery, Cloud Storage, and Cloud Composer"
Migration to Google Cloud
Use of Google Kubernetes Engine (GKE)
Implementation of Recommendations AI
"Utilization of various Google Cloud products (Compute Engine, Cloud Storage, BigQuery, Pub/Sub)"
Using Google Cloud to scale solutions
Utilizing Cloud SDK and Cloud Storage to handle data
Adopting AI and ML models for diagnosis and triage
Using Google Cloud for scalability
Using Kubernetes clusters hosted by Google Kubernetes Engine
Using Google Cloud's Compute Engine for services requiring less scalability
Migrating to Google Cloud
Inclusion in Google for Startups Accelerator (GFSA)
Use of various Google Cloud solutions
Migrated to Google Cloud
Used built-in logging and online resources
Switch to cloud-based infrastructure
Engaged Siatik for proof of concept
Redesigned core architecture to microservices using Google Kubernetes Engine
Used tools like TensorFlow and Cloud ML Engine
Migrating to Looker on Google Cloud
"Adopting Cloud Build, Cloud Run, and Google Cloud's operations suite"
Creating a pipeline for continuous improvement
Migrating to Google Cloud
Using serverless technologies
Leveraging project-based infrastructure
Autoscaling with Google Kubernetes Engine
Using Cloud Load Balancing for lower latency
Using BigQuery for efficient data analytics
Migrating to Google Cloud
Using Google Kubernetes Engine
Adopting BigQuery for data normalization
Leveraging DataStax Astra DB for database management
"Utilizing BigQuery, Dataflow, Dataproc, BigLake, and Dataplex for data processing and governance"
"Built a scalable, high-performance analytics platform with BigQuery"
Used Looker Studio for faster report delivery
Built an internal content API with Cloud Endpoints and App Engine
Enhanced analytics capabilities with machine learning elements of Google Cloud
Move from on-premises data center to cloud
Adopt BigQuery and Cloud Bigtable
Use Kubernetes for deploying and scaling applications
Moved infrastructure to Google Cloud
Used managed services and Kubernetes for autoscaling
Adopted Google Cloud
"Utilized Cloud CDN, Cloud Functions, BigQuery, and Looker Studio"
Participated in Google Cloud for Startups program
Replacing on-premises servers and software with BigQuery
Utilizing Google Cloud for data analysis
Migration to Google Cloud
80/20 rule for cloud migration
Participation in Google Sand Hill Program
Deepened integration with Google Cloud
Shifted from ETL to ELT
Used Cloud AutoML for pricing automation
Set up a centralized data warehouse
Adopted Google Cloud
Built data pipeline with higher efficiency and scalability
Migrated to Google Cloud
Deployed GPUs on Compute Engine
Used BigQuery and Looker Studio
Running on Google Cloud
Using Compute Engine
Using BigQuery for data analysis
Migration to Google Cloud
Partnering with Endocode
Implementation of Google Kubernetes Engine
Moved to Google Cloud Platform
Used Google BigQuery for data analysis
Centralizing and unifying information management and governance
Migrating and consolidating data to Google Cloud
Establishing data integrity measures
Using BigQuery as the foundation for data management
Using App Engine for managed environment
Traffic-switching on App Engine
Using Google Cloud operations suite for monitoring
Moved to Google Kubernetes Engine
Used autoscaling in GKE
Implemented Cloud SQL for automation
Deployed microservices
Consolidation onto Google Cloud
Use of Google Kubernetes Engine
Deployment of Google Workspace for collaboration
Migrated entire infrastructure onto Google Cloud
Embraced Google Cloud's high availability with autoscaling
Switch to Google Cloud Platform and Google Kubernetes Engine
Use of containers for continuous integration pipeline
Deploy software faster using fewer resources
Store and retrieve user-generated data using Kubernetes Engine
loading Google Analytics 360 data into BigQuery
consolidating data to a single location
"implementing a platform for data acquisition, cataloging, and storage"
Use of Google Cloud for managed infrastructure
Two-zone backup
CI/CD pipeline based on Terraform and Ansible
Use of Google Cloud Armor for security
Hosting on Google Cloud
Using Google Kubernetes Engine
Adopting a microservices architecture
"Using Google Cloud Storage, Google Cloud Pub/Sub, and Google Maps APIs"
Moving to Google Cloud Platform
Leveraging Google Maps Platform APIs
Partnering with Searce for API determination and implementation
Use of BigQuery for query-based billing
Configuration of a technical stack centered on SQL
Adoption of Cloud Data Fusion for low-code ETL processing
Use of Google Cloud TPUs for image processing
Integration of local and cloud operations with Google Kubernetes Engine
Utilization of deep learning models and TensorFlow
adopting cloud technology
"using Google Cloud tools like Cloud Storage, BigQuery, Looker Studio, and Apigee"
implementing telemedicine
Leveraging Google Cloud managed services
Building close to Google Analytics APIs
Centralized to Google Cloud and Google Earth Engine
Leveraging Google Cloud machine learning operations tools like Vertex AI and BigQuery
Using Google Earth Engine for near-real-time satellite dataset access
Adoption of Google Cloud services
Utilization of BigQuery for data management
Google Kubernetes Engine for scalability
Cloud Composer for task automation
Deploying SAP on Google Cloud
Partnering with oXya for expert advice and managed services
Google Cloud Professional Services SAP Healthcheck
Adopting a domain-driven design approach
Separating source code for distinct domains
"Using Google Cloud-managed services such as GKE, Cloud SQL, BigQuery, Cloud Bigtable, and Dataproc"
Adopted Google Workspace to reduce maintenance burden
Used Google Cloud Functions to automate administrative tasks
Partnered with Google Cloud Partner Siatik for optimal setup
Implemented Google BigQuery for data processing
Migration of content to the cloud
Use of Google Cloud solutions
Integration with multiple data sources
Building large data dashboards quickly
Utilizing Looker for insights
Use of fully managed services like Google Cloud to avoid infrastructure management
Starting with services that provided least friction from ideation to deployment
Full integration with Google Ads ecosystem and Google Tag Manager
Use of Google Cloud services
Adoption of Google Kubernetes Engine for containerized applications
Integration with Google Drive for digital asset management
Use of Firebase for push notifications
Migration to BigQuery for enhanced analytics capabilities
Migrating SAP suite to Google Cloud
Using NTT DATA for migration
Implementing Cloud Data Fusion and BigQuery
Utilizing Google Confluent Cloud for real-time data streaming
Auto-scaling Kubernetes clusters
event-driven architecture using Cloud Run and Cloud Functions
"integration of Google Cloud services like Pub/Sub, Cloud Tasks, Firestore, and BigQuery"
Using Google Maps Platform from day one
Combining Google Maps data with proprietary algorithms
Creating optimized landing pages for organic and SEO traffic
Using static map pages for faster load times
Utilizing Place Autocomplete for better search accuracy
Migration to Google Cloud
Use of containerized microservices
Adoption of Google Kubernetes Engine
Implemented Google Marketing Platform
Used BigQuery for cloud data warehouse
Employed Display & Video 360
Built custom pipelines for data ingestion
Migrating applications to Google Cloud
Using Google App Engine for a fully managed environment
Re-architecting applications to optimize performance
"Utilizing tools like Google BigQuery, Google Cloud Dataflow, and Google Cloud Pub/Sub"
Adopt a state-of-the-art disaster recovery (DR) site in the cloud
"Collaborate with a trusted Google partner, PointStar"
Utilize Google Cloud VMware Engine and Google Cloud Backup and DR Service
Deploying Google Cloud
Using Google Kubernetes Engine for microservices
Using BigQuery for data analytics
Implementing GKE Multi Cluster Ingress
Using Google Analytics 4 for user behavior insights
Use of Google Cloud products
Rapid prototyping with Cloud Functions
Building a data science platform with open source tools
Using Google Cloud for scalable infrastructure
Implementing Google Kubernetes Engine
Using Cloud CDN and Cloud Load Balancing
Migrated to Google Cloud
Upgraded communication and collaboration tools to Google Workspace
Used Injenia for training and support
partnering with Atos
using Google Cloud
integrating Vertex AI
adopting Google Workspace
migrating all on-premises infrastructure and applications to Google Cloud
upgrading legacy SAP ECC to S/4HANA
leveraging DXC Technology as Managed Service Provider
Deploying AI image generation ML models in Google Cloud
Utilizing GPU spot instances with NVIDIA L4 GPUs
Automated recommendation system for AI models and configurations
Real-time data analytics for product optimization
Staged migration to Google Cloud
"Utilization of Google Cloud products like App Engine, Datastore, BigQuery, and Google Kubernetes Engine"
Migrated from legacy cloud to Google Cloud
Leveraged Google Kubernetes Engine for autoscaling
Used BigQuery for real-time data analytics
Deployed Cloud CDN to speed up content delivery
Utilized Memorystore for Redis to enhance performance
Implemented Google Workspace tools for collaboration
migration to Google Cloud
use of BigQuery and other Google Cloud tools
close collaboration with Google Cloud team
Adopt Google Cloud infrastructure
Use Live Stream API and Transcoder API
Leverage Media CDN
Employ Google Kubernetes Engine (GKE)
Utilize BigQuery for data analytics
Implement Video Stitcher API
Deploying Anthos and VMware on Google Cloud
Building repeatable solutions
Leveraging entire portfolio of Google Cloud solutions
Break down barriers between previously isolated parts of the organization
Support a community of 1000 employees to learn more about Google Cloud
Provide extensive training on Google Cloud
Encourage developers to consider the cost aspect of their work
Leverage Google Cloud's automation for meeting regulatory obligations
Formed SEBx as an innovation studio
Chose Google Cloud for infrastructure
Integrated Vault by Thought Machine
using cloud tech
combining ML and human intelligence
using Google Cloud products
Migrated infrastructure to Google Cloud's Compute Engine
Implemented Cloud Pub/Sub and Cloud Load Balancing
Collaborated with implementation partner Global IT
Adopting Google Cloud services
Using BigQuery for data integration
Leveraging Cloud Machine Learning Engine
Using Google Earth Engine for geospatial data
Migrated to Google Cloud
Created a data lake
Interconnected data sources
Deployed machine learning models
Integrating software solutions onto a centralized platform
Leveraging IoT sensors for data collection
Using Google Cloud for infrastructure and analytics
Comprehensive migration to Google Cloud architecture
Use of Google Kubernetes Engine (GKE) for autoscaling
Use of BigQuery for data analytics
Use of Google Kubernetes Engine and BigQuery
Participation in Google Next conference
Gradual migration to Google Cloud
Implementing Google Cloud products
Using Talend as an ETL tool
Integrating BigQuery and Cloud Storage
Integration of telehealth solution from Twilio
Partnership with Google Cloud
Building a data warehouse with BigQuery
Working with Google Cloud partners Accenture and OChK
Migrated to Google Cloud for scalability and cost efficiency
Partnership with Kasna for seamless transition and optimization
Migrated to Google Cloud
Used machine learning models
Deployed and scaled app on Google Kubernetes Engine
Used a combination of managed data analytics and database services
Incorporate Google Cloud into multi-cloud architecture
Deploy products on multiple cloud platforms
Use open-source tools for product development
built microservices on Compute Engine and GKE
added Cloud Functions and Pub/Sub for backend management
uses Cloud SQL and Cloud Storage for temporary storage and backup
Migrating services to Google Cloud
Using Kubernetes containers and Cloud SQL
Leveraging pre-configuration features
Adopting Looker for data insights
Implementing day-zero reporting
Building Analytics Hub
Migrating tech infrastructure to Google Cloud
Utilizing Google Kubernetes Engine (GKE) and Compute Engine
Adopting Cloud Run for scalable microservices
Using Google Cloud infrastructure for scalability and stability
Transitioning to Firebase Realtime Database and BigQuery
Moved online services and data analytics workloads to Google Cloud
Adopted scalable infrastructure using Google Kubernetes Engine (GKE) and Cloud SQL
Leveraged Cloud Run for app services development and deployment
Implemented BigQuery for faster data processing and in-depth analytics
Migrated to Google Cloud
Used Google Kubernetes Engine
Implemented Cloud Logging
Running OTT systems in Google Cloud
Leveraging containers in Google Kubernetes Engine (GKE)
Using custom VMs in Compute Engine
Utilizing Media CDN for content delivery
Adopt a containerized solution
Hand infrastructure over to an experienced cloud provider
Use Kubernetes Engine for continuous integration and development
use of Google Cloud
partnership with Searce
use of BigQuery for analytics
Developed a SmartCloud platform with an event-driven architecture
"Used Google Cloud services like Dataflow, BigQuery, Pub/Sub, Cloud Functions, and IoT Core"
Migrating from on-premises to Google Cloud
Implementing a proof of concept (PoC) environment for testing
Automate and orchestrate processes
Use of scalable and reliable Google Cloud solutions
"Design a strategy based on availability, use and integration, and democratization"
unify data sources
use Google Cloud and its products
engage Google Cloud partner Jellyfish
use SCRUM methodology
Use of Google App Engine for scalability
Development of second screen app to enhance interactivity
"Selection of Google Cloud, BigQuery, and Looker"
Establishment of a collaborative engineering workflow
Focus on a modern analytics platform with robust embedded analytics
Automated image tagging using AI and machine learning
"Utilized Google Cloud Platform including services like Kubernetes Engine, BigQuery, and Cloud SQL"
Moved from legacy infrastructure to Google Cloud
Used Firebase for mobile app development
Collaborated with CTS for workshops and guidance
Google Cloud migration
collaboration with Revolgy
participation in Google Launchpad Accelerator
Migrating to Google Cloud
Working with Rackspace
Leveraging Google Cloud and MongoDB's flexible document model and dynamic indexing
Using Cloud Machine Learning Engine and TensorFlow for training and development
Using Kubernetes Engine for autoscale functionalities
Utilize Google Cloud Vision AI for automated tagging
"Use Looker Studio for tracking usage, statistics, and reporting"
Migration to Google Cloud
Adoption of Google Kubernetes Engine for auto-scaling
Use of Google Cloud Functions for rapid feature release
Deployment of Cloud Scheduler for automation
Adopted Google Cloud's solutions
"Partnered with Zenta, a Google partner, for IT support"
Using Flywheel on Google Cloud Platform
Business Associate Agreement with Google
Using BigQuery for data analysis
Integrating BigQuery with other Google Cloud analytics tools like Firebase and Firebase Crashlytics
Transitioning from existing infrastructure
Developing and executing a plan with data analytics
Using Google Cloud services like BigQuery
Migration to Google Cloud
Using BigQuery as the primary data lake solution
Creating a centralized data hub
Using a hub-and-spoke model for data integration
Working with Google Cloud for secure and scalable infrastructure
Migrating to Google Cloud
Using Google Kubernetes Engine for scalability
Employing BigQuery for data analytics
Move to Google Cloud
Implement continuous integration and continuous deployment using Google Kubernetes Engine
Rebuild front end of website and mobile app using Docker containers
Migrating to Google Cloud Platform
Using Google Kubernetes Engine
Utilizing Google BigQuery for data analytics
Building a data lake on Google Cloud
Use of AI and ML models
"Extract, transform, and load (ETL) data using Dataflow"
Using Pub/Sub for messaging between IoT gateways and Google Cloud
Moved to Google Cloud
Implemented Google Kubernetes Engine
Adopted microservices architecture
Utilized BigQuery and Looker Studio for analytics
Use of Google Cloud for deployment and operations
Implementation of Apache Spark through Dataproc
Utilization of Google Kubernetes Engine and Pub/Sub
Modular microservice setup
Migrated systems to the cloud
Used Google Cloud Bare Metal Solution powered by Intel
Leveraged existing Oracle licenses
Migrating to Google Cloud
Using Google Kubernetes Engine (GKE)
Implementing BigQuery
"Use of Google Cloud solutions like BigQuery, Compute Engine, Firestore"
Microservice architecture with Google Kubernetes Engine
Optimizing testing models with Google Cloud
Utilizing Google Cloud's multi-zone infrastructure spin up capabilities through Terraform
"Opened SUBARU Lab at Shibuya, Tokyo"
Chose Google Cloud for its managed services and high-performance hardware
Adopting cloud-based solutions
Partnering with CI&T
Using Enterprise Agile development methodology
"planning, scoping, and migration to Google Cloud"
"use of BigQuery, Cloud Functions, Cloud Storage"
Adopting Cloud Healthcare API
Implementing HL7-FHIR standard
Using Apigee for data integration
"Using Pub/Sub, Cloud Functions, and Cloud Tasks for data transfer"
"Evaluated cloud, machine learning, and AI solutions"
Partnered with Deloitte Consulting
Consolidated data across multiple business units
Automating infrastructure management with Google Kubernetes Engine
Establishing a real-time analytics pipeline
Using BigQuery ML models for predicting ROS and ROI
create Digital Tech department
migrate to Google Cloud
use Cloud Functions and Cloud Run
develop internal Data Science Academy
Collaboration with Google Maps Platform
Partnership with Maplicate
Merged data lake and data warehouse into one single platform
Implemented a cloud-based modernization strategy
Used Google Cloud's services
Support from Google Cloud Professional Services
Certifications in Google Cloud
Migration tests and pilots
Planning and performing tests
Switching to Google Cloud
Working with Devoteam G Cloud for implementation
Using Terraform for provisioning
Adopt cloud technology
Real-time data streaming
Use of machine learning models
Implement predictive maintenance
Migration to Google Cloud
"Utilization of Google Cloud products like Compute Engine, Dataproc, Cloud Bigtable, Kubernetes Engine, Cloud SQL"
move to cloud
use Google Kubernetes Engine
data-based A/B testing
real-time insights
"use of BigQuery, Looker, Pub/Sub"
Migrated to Google Cloud infrastructure
Used Google Cloud's managed services
Collaborated with Google Cloud partner Devoteam and ML6
Migrated to Google Cloud
Participated in Google Cloud Technical Account Management (TAM) program
Worked with Google Cloud partner freiheit.com
Adoption of Google Cloud infrastructure in 2019
Building a central analytics platform for big data processing
Use of Vertex AI and BigQuery for machine learning and advanced analytics
"Development of DEMON, an online demand forecasting service"
Migrating database to BigQuery
Using managed services like Cloud Dataflow and Cloud Storage
Used Google Analytics 360 integrated with Google BigQuery for big data analysis
Reallocated underperforming ad spend
Automated data imports with Google BigQuery Data Transfer Service
"Integration of machine learning model learning, service, and deployment environment with Vertex AI"
TPU-based Coral development board for data processing
Utilizing Google Cloud's ecosystem for seamless workflow
Digital transformation process
New architecture to expand scalability
Migration to the cloud
Training and planning with Nubosoft
migrating entire environment to Google Cloud
deploying Google Kubernetes Engine
using Auth0 for sign-on validation
utilizing BigQuery for real-time insights
employing Cloud SQL and Cloud Storage for data management
"Utilize Google Cloud services including BigQuery, Compute Engine, Cloud Pub/Sub, Cloud Dataflow, and Cloud Dataproc"
Implement a scalable data pipeline
Cloud-first strategy
Building an MVP from scratch in Google Cloud
Collaboration with Google Cloud and aeros
Partnering with Google Cloud
Establishing foundational processes and technologies
Building the Global Reference Node
Implementing Google Cloud services
Using Google Analytics and Ad Manager
Leveraging local Google partners for implementation
Partnering with Accenture and Google Cloud
Using Google Cloud's Partner Success Services
Overhauling data management approach
Creating data pipelines
Implementing infrastructure-as-a-code practices
Zero-downtime migration
Staged migration from data to infrastructure
Project management tasks and access management
Security workshops and training
Migrated to Google Cloud
Used Google Kubernetes Engine
Stored data in Cloud Spanner
Consolidation of data processing infrastructure into Google Cloud
Use of cloud-native technologies like Kubernetes and Istio
Adoption of a DevOps methodology
Built and launched the website in six weeks
Worked closely with Google Cloud and SpringML
Using Google Cloud Functions and Pub/Sub
Developing UnFaaSener framework
Migrated to Google Cloud
Redesigned environment with Movti's support
Incorporated new solutions into developers' daily routines
Using Google Cloud for infrastructure and data
Deploying BigQuery analytics data warehouse
Leveraging managed Google Cloud services
Migrated to Google Cloud
Implemented Google Kubernetes Engine
Used infrastructure as code
Collaborated with Google Cloud engineers
Used Google Kubernetes Engine for autoscaling
Employed load and performance testing
Used Istio for managing microservices
Leveraged BigQuery for data analytics
Used Cloud CDN for live video streaming
Migrated entire tech stack to Google Cloud
Used Compute Engine and Cloud Armor for security and stability
Adopted Google Workspace for remote work
Built data lake with BigQuery
API-first approach
Integration with ecosystem partners
Use of Apigee API Management
Building an AI operation pipeline using Vertex AI
Leveraging Google Cloud's machine learning platform
Modularizing processes for efficient digital transformation
Using Google Cloud products
Integrating third-party applications
Optimizing for growth from day one
migration to Google Cloud
use of Google Cloud Functions and Cloud Storage
partnership with Nublify
adoption of Google Kubernetes Engine
creation of DAS platform
Consulted with Google Cloud Partner Wise IT
Implemented a microservices-based infrastructure orchestrated with Google Kubernetes Engine
Switching to a multi-cloud infrastructure
Using Google Kubernetes Engine
"Utilizing Google Cloud big data tools like Pub/Sub, Dataflow, and BigQuery"
"Adopting a cloud data analytics platform using BigQuery, Cloud Dataprep, and Looker Studio"
Conducting proofs of concept to prove the solution's effectiveness
Promoting grassroots engagement through hackathons
Rebuild existing BigQuery
Deploy Looker
Develop data-driven approach
Implement a new target operating model
testing cloud providers
using Google Cloud for batch processing and streaming data jobs
integration of Looker with Google BigQuery
Migrating to Google Cloud
Using Google Kubernetes Engine (GKE)
"Utilizing BigQuery, Cloud Pub/Sub, and Cloud Dataflow"
Leveraging Google Cloud support for Windows workloads
Using Google Cloud Platform
Consolidating data onto a single platform
Implementing microservices-based architecture
Adoption of multi-cloud infrastructure
Microservice-based architecture
Use of managed solutions from Google Cloud
Migrating to Google Cloud
Deploying Google Kubernetes Engine
Using BigQuery for data consolidation and prediction
Partnering with MediaAgility for consulting
Collaboration with Google Cloud
Use of BigQuery for data storage and analysis
Utilization of Google Kubernetes Engine for microservices architecture
Partnership with DoiT International for implementation
Adopted Google Cloud solutions
Developed applications using App Engine
Used BigQuery and Firebase for data management
Migrated to Google Cloud Platform
Engaged Google Cloud Professional Services
Implemented Google Cloud solutions
Developed scalable in-house sentiment and topic model
Used Google Cloud VertexAI for training data and testing models
Use Google Marketing Platform and Google Cloud to power a data platform
Consolidate data into one place using CRMint
Move to Google Cloud
Adopt multitenant environment
Use Google Kubernetes Engine for containerization
Incorporate Google Cloud Professional Services for migration
Migrated ERP and critical workloads to Compute Engine
Modernized mobile applications and built a connected car platform
Used Cloud Logging and Cloud Monitoring for IT operations
Trained IT staff on Google Cloud
Using BigQuery and other Google Cloud solutions
Partnering with Analytics Pros
adoption of Google Cloud products such as BigQuery and Looker
integration with Firebase for development purposes
adoption of Google Kubernetes Engine (GKE)
Full-stack migration to Google Cloud
Using robust and easy-to-understand documentation
migrating systems to Google Cloud
using BigQuery for ML-based data warehousing
"leveraging Cloud Composer, Cloud Storage, Cloud Functions, Data Catalog, Cloud Data Loss Prevention, Google Kubernetes Engine, and Cloud Run"
Embedding analytics into their product with Looker
Using Snowflake’s cloud data warehouse
Utilizing Matillion for ETL
Collaborating with 4 Mile Analytics for expertise
Utilization of Google Cloud for secure data handling
Deployment of microservices in Kubernetes clusters
Use of Google Firestore as a central data repository
Selected Looker after evaluating several alternative providers
Embedded Looker analytics in its product to power its Insights platform
Containerization
Using Google Kubernetes Engine
Kubeflow for deployment
Migrated solutions to Google Cloud
"Used serverless, containerized environment run on Google Cloud Run"
Migrated data to Google Cloud managed data services such as BigQuery and Bigtable
Leveraged Google Maps Platform for geolocation services
Building a unified data platform with BigQuery
Collaboration with fifty-five for a comprehensive marketing data strategy
Using Google Kubernetes Engine and Dataproc for data processing
Used Google Cloud services such as Dialogflow CX and Vertex AI
Shifted from on-premises architecture to the cloud
Built a comprehensive data pipeline
Partnered with Google Cloud and DELVE
Migrated and standardized decades of data
De-siloed data to create and manage donor personas
Applying for research credits
Utilizing eight Cloud GPUs simultaneously
Using Google Cloud
Deploying microservices infrastructure with Google Kubernetes Engine
Using Cloud Storage as a data lake
Running Spark clusters with Dataproc
Training ML models with TensorFlow on AI Hub
Use of Google Kubernetes Engine and Google Cloud global network
Deploying dedicated Kubernetes clusters
Leveraging TensorFlow for AI modeling
"Used Google Cloud services like App Engine, Compute Engine, Cloud Storage, Datastore, Cloud SQL"
Adopted Kubernetes for rapid deployment
Analyzing products available in the market
Testing the tools that best fit its needs
Creating an exact replica of the previous network
migrated to Google Cloud
adopted BigQuery
shifted from ETL to ELT processes
used Vertex AI
Replaced on-premise data warehouse with Google Cloud's BigQuery
Implemented GPS-equipped tracking collars
Built a data platform with Google Cloud
Developed a data mesh
"Using tools like BigQuery, Looker, Dialogflow, and Vertex AI"
Built custom model monitoring tool Heimdall
Using artificial intelligence and real-time data analysis at scale
Collaborating with regional transit authorities
Utilizing Google Cloud Research Innovator program resources
Using Cloud Billing Reports from Google Cloud console
Billing Export to BigQuery and Looker Studio
Creating custom cost dashboards
Unified and innovative workflow
Consolidating separate databases into one unified and serverless data lake on BigQuery
Using Google Cloud and ServiceNow to establish an intelligent digital workflow
"Leveraging Google Cloud services such as GKE, Cloud SQL, and BigQuery"
Partnering with Google Cloud Partner MatrixC for infrastructure setup
Switched to Cloud Translation API
Implemented automated translation for product descriptions
Migrated to Google Cloud
Used Google Kubernetes Engine
Automated deployment of new services
Utilized Google Cloud's operations suite for logging and monitoring
Migrating data and processing to Google Cloud and BigQuery
Building Potens for automating data query workflows and managing BigQuery data
Partnering with Slalom for migration
Using Google Cloud VMware Engine
Google Cloud's Rapid Assessment & Migration Program (RAMP)
Investing in web presence and ordering platforms
Working with technology partners such as Google Cloud
Using a data lake in Google Cloud
Collaborating with BOTfriends to create a voicebot
Utilizing Google Cloud's scalable infrastructure
Migrating microservices to Google Kubernetes Engine (GKE)
Using machine-learning algorithms for content recommendations
Employing preemptible instances for cost control
Benchmarking performance on major public clouds
Lift-and-shift migration to Google Cloud
Migrated to Google Cloud
Use of Google Kubernetes Engine
Integration of technology stack with existing tools and workflows
Regular check-ins with Google Cloud team
Moved workloads to the cloud
Opted for a multi-cloud architecture
Built platform on Google Cloud
Microservices architecture
CI/CD pipeline for rapid development
Collaboration with Google Cloud engineers
Benchmarking Google Cloud against competitors
Migrating to Google Cloud on a case-by-case basis
Creating a single location for all data
Running machine learning models
Moving to Google Cloud Storage
Using Google Kubernetes Engine
Using managed services
Full-stack migration to Google Kubernetes Engine
Lift-and-modernize migration
Live incremental migration
Started with Google Cloud platform
Implemented systems to record and manage point-of-sale transactions
Deployed cloud services for infrastructure and smart analytics
Used Google Kubernetes Engine for application scaling and management
Leveraged Looker’s data application platform
Connected Looker to Waterline
Built data model and embedded dashboards
Set up permissions configurations for customers
Incorporating Google Cloud into IT infrastructure
Using Google Cloud APIs to set up gaming content distribution
Employing multi-cloud architecture
Built a hybrid cloud
Collaborated closely with Google Cloud teams
Integrated public and private cloud
Using Google Cloud management resources
Leveraging Google Kubernetes Engine and Container Registry
"Using BigQuery, Cloud SQL, Datalab, and Dataproc"
Integrating Looker for analytics needs
Built a system on Google Kubernetes Engine (GKE)
"Employed Cloud AutoML, Cloud TPU, and Cloud Machine Learning Engine"
Used Cloud Pub/Sub for data streaming
Managed and monitored processing with Stackdriver
Move applications to Google Cloud Platform
Use Google BigQuery for data analysis
Build a real-time loan scoring engine
Moved production services to Google Cloud Platform
Utilized Google Kubernetes Engine for microservices
Implemented Google Cloud Dataflow for data processing
use of Google Cloud
ready-to-go data pipeline
integration of BigQuery
Use Google Analytics 360 to gather data
Integrate Google Analytics 360 with Google BigQuery
Work with Analytics Pros to keep data quality high
Lift-and-shift migration of virtual machines
"Using BigQuery, Looker Studio, and Cloud Datalab for system performance analysis"
Moving to a microservices-based application architecture
Using GKE for container management and Cloud Pub/Sub for message queueing
"Revamp technology platform using Google Cloud, including Google BigQuery and Looker"
"Build a trusted, flexible, and customizable data and BI platform"
Embed Looker into Polaris Insights Explorer and Benchmark Explorer
full migration to the cloud
incorporated tools such as Google Kubernetes Engine and Cloud SQL
"Utilizing Google Cloud and its various services (GKE, Cloud Run, Pub/Sub, etc.)"
Winning credit from Google Cloud as part of the Google for Startups program
Integrating Vertex AI for machine learning and process optimization
Adopting an API-first approach with Apigee
Deployed games on Google Cloud
Migrated to Google Kubernetes Engine
Adopted Pub/Sub for scalability
Used Anthos for global network management
Move to Google Cloud
Use of Premium Tier Network
Deployment of game servers in local regions
Use of Cloud Load Balancing
Use of Virtual Private Cloud (VPC)
100%-cloud solution
Overlay on-premises and hybrid systems
Selected Deloitte as managing vendor
Implemented BigQuery to handle large volumes of data
Used Looker Studio dashboards for data democratization
Migration to Google Cloud
Utilizing Google Kubernetes Engine (GKE) and Firebase
Integrating Google Cloud with Google Workspace and Salesforce.
Using Google Cloud VPN for innovative solutions.
Moving its website to Google Cloud.
Migrating to Google Cloud
Using Kubernetes for containerized infrastructure
Implementing Google Cloud IAP and KMS for security
Migrated from previous provider to Google Cloud.
Employed BigQuery to reduce operation load by 80%.
Used Dataflow to stream first-party data into BigQuery.
Utilized Cloud Data Fusion for data integration.
Leveraged Pub/Sub as a queuing engine.
Using AutoML to identify species
Training machine learning models with archival images
Using camera trap images to train AutoML models
Use of Google Cloud Platform
Elastic and automatic scaling
Real-time analytics
Integration with Google Classroom
Use of BigQuery for data analytics
early adoption of Microsoft Copilot for Microsoft 365
custom-built plugin for Copilot in Microsoft Teams
tailored training for each department
"Migration of 12,000 users to Dynamics 365"
Working with Microsoft FastTrack for optimal user adoption and implementation
Incorporating Microsoft Teams and other applications within Dynamics 365
Moving operations to the cloud
Implementation support from Argano
Three-stage approach for implementation
Transition to Microsoft Azure
Use of Databricks Lakehouse architecture
Integration with Azure Data Lake Storage
Automation with Azure Functions
"Evaluation of several products on parameters like scale, performance, integration, and security"
Deployment of Azure Data Lake
Instructor-led training sessions
Integration with Office 365 and Power BI
Leveraging Microsoft Power Apps for data acquisition and reporting
Utilizing Microsoft Power Automate for process automation
Building low-code or no-code applications
Migrated to Microsoft Azure with FastTrack support
Used Microsoft solutions such as Azure SQL Database and Power BI
"Developed powerful, visually appealing dashboards"
fit-gap analysis
sprints for development
user training sessions
support from top management
Migration to Microsoft Azure
"Leveraging Microsoft Defender, Microsoft Sentinel, and Power BI"
Developing a Managed Detection and Response solution using AI and machine learning
Greenfield SAP S/4HANA-based platform implementation
Partnering with Microsoft Azure
Using out-of-the-box functionality
Migrated to Microsoft Azure
Worked with Ingram Micro
Utilized Microsoft DCO program
Development of a standardized DMS for real estate industry
Utilization of Microsoft Azure as technology foundation
Automated management and classification of documents with Azure Cognitive Services
migrate from dual datacentres to Microsoft Azure
leverage native Azure services
focus on value-add activities
Adopting a unified platform with Microsoft technologies
Leveraging digital champions to promote technology
Implementing intuitive and democratized tools for employees
Migrated workloads to Microsoft Azure VMware Solution
Phased transformation approach
Culture change to a team of teams approach
Empowered current employees to drive the cloud-first modernized workplace
Commitment to make the most of Microsoft as an end-to-end technology partner
Partnered with SquareOne for robotic process automation
Implemented Power Automate and Azure Cognitive Services for automating processes
Built workflows using OCR technology
Showcased solutions to management to get buy-in
Expanded RPA across different departments
Adopted a 'one best way' strategy
Centralized data and analytics under one team
Deployed Microsoft Power BI
Standardized the look and feel of dashboards
Created a global analytics ecosystem named INSIGHTS
Deployed Microsoft Dynamics 365 Sales and Customer Insights
"Integrated demographic, behavioral, and psychographic data into customer profiles"
Transition from traditional call center to AI-driven voicebot
Implementation of AI voicebot using Feedyou Platform and Microsoft Azure AI
Iterative approach using Power BI to analyze and improve voicebot performance
Architectural redesign
"Development of Wavelength, an enterprise low-code application platform (LCAP)"
Utilization of the Azure Stack and Azure Integration Services
instilling a digital culture
partnering with Microsoft
integrating various technologies into a cohesive system
implementing Microsoft 365 E5
using Power BI for analytics
implementing Dynamics 365 Finance and Operations
leveraging Microsoft Power Platform
Use of Power BI to monitor customer base in regions
Automation of recurring reports with Power BI templates
Integration of Power BI with Microsoft Teams
Enrolled into the Microsoft for Startups program
"Leveraged Microsoft's networks, support, and guidance"
Utilized Microsoft Azure for infrastructure and AI capabilities
Published app on Azure Marketplace
Set up a centralized Data & Analytics team
Adopted Microsoft 365 tools
Cloud-first approach with Microsoft Azure
Introduced Power BI
procurement and implementation of Legal Interact's Contract Corridor
customizing the solution to workflows
weekly training sessions for staff
integration with Microsoft Teams
Implementing Dynamics 365 Finance
Using Microsoft 365 E5 for productivity and security
Embracing Power BI for business insights
Leveraging Azure for cloud infrastructure
Adopted Microsoft Fabric Real-Time Analytics (RTA)
Implemented Fabric event streams to ingest data
Used KQL Database for real-time data analysis
Seamless integration with existing Microsoft Power BI
Migrating from the existing Hadoop platform to Platform as a Service (PaaS) powered by Azure HDInsight
Gradual as-is migration with minimal code change
"Utilizing tools such as Azure Data Factory, Azure Data Lake Gen2, Azure HDInsight, and Power BI"
Collaboration with Microsoft
Use of Power BI dashboard for efficient operation
adoption of Microsoft Azure
use of Microsoft Power BI
migration to cloud-centric approach
use of Azure Synapse Analytics and Azure Data Lake
development of mobile applications with Power Apps
integration of IAM and Microsoft security tools
Collaborated with Microsoft
Implemented Azure Well-Architected Framework
Deployed Microsoft Power BI
Adoption of Microsoft Intelligent Data Platform
Collaborative deployment with Adastra and Microsoft
Prototyping Azure-native solutions
adopting the Microsoft Cloud for Nonprofit
building out solutions for volunteer and grant management
designing the foundation of the nonprofit’s data strategy and analytics
Partnered with ETG Global Consultancy
Formed a team of 100 expert users from different departments
Thorough analysis and careful planning
Partnered with OneDynamics
Gradual rollout of Dynamics 365 Sales
Involved employees in enhancing functionality
"Built Omnidata, a centralized analytics platform"
Participated in an AI program by the UN and Microsoft
Used Microsoft Azure OpenAI Service and Azure AI Search
Provided training in analytics and machine learning
Adopting Microsoft Power Platform
Developing internal digital skills
Creating a center of excellence for Power Platform
Implementation of Microsoft Azure Synapse Data Warehouse and Azure Machine Learning Studio
Real-time synchronization of information sources
Creation of a data warehouse in the cloud
Transition to Microsoft Dynamics 365 Sales and Customer Service
Develop a common data model
Minimal customization to stay close to default capabilities of Dynamics 365
Partnering with Alula Technologies
Using Alula Life Policy Administration System built on Microsoft Azure
Setting up digital portals
Automating processes like customer onboarding and regulatory compliance
Early adoption of new technologies
Efficient rollout using Active Directory groups and Microsoft Configuration Manager
Use of Microsoft Data Migration Assistant to avoid database compatibility issues
"Use of Microsoft Dynamics 365 Finance, Dynamics 365 Project Operations, and Azure solutions"
Creating digital twins for every stone
Using ledger technologies to record immutable data points
Incorporating AI tools like Copilot
Microsoft Power Platform strategy workshop
Roadmap for seamless integration of AI-powered transformative technology
Transition to Microsoft Azure
Redevelop applications to be cloud-native
Standardize workflows with Microsoft 365 E5
Use of Power BI for reporting and analytics
Shift to Microsoft Azure
Implementation of Dynamics 365 Finance and Operations
Use of Microsoft Teams and Microsoft 365
Local support from Microsoft partners
Installed CloudMonitor into Azure tenancy
Used reporting capabilities for a holistic view
Incorporated lessons into client consultations
Adopted Microsoft Teams and Microsoft Power BI
Introduced cloud-based Modern Workplace client and M365 App suite
Created dedicated expert groups to unlock collective intelligence
Migrating workloads from on-premises datacenters to Microsoft Azure public cloud platform
Implementing Azure cost management measures
Using Power BI for monitoring and optimization
Digital transformation programme
Adoption of Microsoft Power Platform for no-code and low-code development
Portal approach using Microsoft Azure technologies
Worked with Microsoft to create a digital twin
Developed Power BI dashboards to bring diverse datasets together
Alliance with Microsoft
Deploy Microsoft Fabric
Unify data supply chain components
Migrate EY Canvas to Microsoft Azure
Integrate EY Helix analytics suite
Step-by-step approach to deployment
Combining complicated data platform components to create a bespoke solution
Integrating Power BI into future analytics solution
Migrating to Microsoft Office 365
Building departmental portals using Microsoft Power Apps and SharePoint Online
Centralizing real-time analytics dashboards using Power BI
chose Microsoft Fabric on the Azure platform
joined the private preview of Microsoft Fabric unified analytics platform
Migrated SAP environments to Microsoft Azure
Implemented Azure Virtual Desktop
Used Azure Backup to protect workloads
Integrated Azure Monitor and Microsoft Sentinel
Used Azure Express Route for secure connections
Adopting Microsoft Teams and Surface Hub devices
Implementing the Connected Shop Floor (CSF) program
"Ensuring familiarity with technology already in use (smartphones, connected devices)"
Using Power Platform in-house
"Leveraging Power Apps, Power Automate, and Dataverse"
Building AI models with AI Builder
Creating a centralized portal with Power Pages
Deploy as many Microsoft resources as possible
Use Azure ExpressRoute and Azure NetApp Files
Full migration of SAP systems to Azure
Use of Microsoft Power BI as primary technology interface
Integration of Azure Cognitive Services
Adoption of Azure OpenAI Service for NLP capabilities
Participation in Microsoft for Startups program
low-code approach
use of Microsoft Power Apps
embedding app into Microsoft Teams
integration with Microsoft Entra ID and Azure Active Directory
Moved data to Azure SQL Database
Upgraded to Azure SQL Hyperscale service tier
Concurrent running of existing and new databases during migration
"Built Presidio Project Portal on top of Microsoft Project for the web with Power Apps, Power BI, and Microsoft Teams integration"
Standardized processes for setting up and delivery of projects
Migrated to Dynamics 365 Business Central
Used pre-built apps for additional functionality
Use of Power Platform
Creation of centralized solution called 'Launchpad'
Leveraging Dataverse for reusable components
"Integration with Microsoft 365, Workday, and ServiceNow"
Deployment of transformative models
Use of Azure cloud and Power BI
Implementation of FinOps Framework
Leveraging Microsoft Power Platform
Using AI Builder in Power Automate
Creating cloud flows in Power Automate
Building Power BI dashboards
"Exploring alternative, cost-effective, managed services solutions"
Utilizing Microsoft Azure Managed Application
Migrated data to Microsoft Azure
Used Azure Data Factory for data integration
Deployed Microsoft Dynamics 365 for ERP
Implemented Microsoft Power BI for dashboards and metrics
"Used Microsoft Purview, Sentinel, and Defender for cybersecurity"
Minerva Digital Platform
"3D strategy (diversification, digitalization, decarbonization)"
Real-time data accessibility
Remote Operations Center (ROC)
Migrating AVEVA PI System from on-premise to Azure
Using Azure ExpressRoute for closed network
Scheduling rehearsals for migration
Coordination to minimize data acquisition gap during system downtime
Partnering with Yodiwo
Building a tailored solution using Plano360 powered by Azure
Fully integrating with Power BI
Onboarding all stores in six months
Adopting Microsoft 365 and other Microsoft solutions
Enhancing security
Connecting disparate technology systems
Migrating to a unified system
"Expanding use of Microsoft technology for data storage, processing, and security"
retire current legacy systems
upgrade to new infrastructure
invest in Azure Synapse Analytics
create predictive models
use Linux server for data handling
use Azure Synapse for data transformations
Moving to Windows Server
Full cloud transformation on Microsoft Azure
Collaboration with Microsoft and BIPROGY
Aggregating data from multiple sources into a single unified platform
Simplifying data access for users
Establishing a data governance initiative
Expanding Data+ to include the analytic marketplace
Leveraging Microsoft Dynamics 365 for a hyperconnected CRM solution
Establishment of a CRM Center of Excellence (CoE)
Integration of AI features like Opportunity Scoring (iScore) and Digital Sales Assistant
White labeling CloudMonitor as part of CloudSource
Achieving an all-encompassing perspective of the Azure framework
Collaborating with CloudMonitor by Data-Driven
Use of Azure OpenAI Service and its built-in ChatGPT capabilities
Develop chatbots for employees
Use other Azure AI services to innovate business processes
Adoption of Microsoft Teams for video consultations
Integration of new platform with existing EPR system
Trial with the Speech and Language Therapy team
Leveraging Microsoft Power Platform and Azure services
Migrating to Microsoft Dynamics 365 Business Central
"Integrating with additional Microsoft products like Power BI, Office and Teams"
Using GoPro’s customizations and localizations
Adopted Microsoft products and services
Built a Zero Trust environment
"Integrated IT, Human Resources, and General Affairs departments"
Created an integrated cloud data and analytics solution with Microsoft Azure and Microsoft Power BI
Formed a Fusion Team
Adopted a market-leading lakehouse architecture underpinned by data mesh principles
adopting suitable technology
streamlining operations
implementing Dynamics 365 F&O
Adopting Microsoft Learn for Educators
"Incorporating Microsoft Dynamics 365, Power BI, and Azure into the curriculum"
Creating dedicated labs for hands-on training
Replaced existing loan management system with COREFIN
Used a full stack of Microsoft tools
Optimized processes through Power Automate and Power Pages
Integrated with SharePoint and Power BI
Consolidate knowledge in a central location
"Rely on a graph database, Microsoft Azure, and PRODYNA"
Use Azure DevOps to speed up the process
Collaborating closely with Microsoft
Building a state-of-the-art Microsoft Azure learning lab
Using Microsoft Azure for cloud migration
Creating a Data-Warehouse based on Azure
Using Power BI for visualizations
Collaboration with GDS Business Intelligence GmbH
Migrated to Microsoft Azure with Azure VMware Solution
Worked with Microsoft and implementation partner Bell Canada
Utilized FastTrack for Azure and Azure Migrate and Modernize teams
Leveraged Azure Hybrid Benefit and Microsoft Cost Management
Maximize the use of Microsoft Teams
Develop an e-learning platform
Automate workflows with Power Automate and Microsoft Forms
Use AI-powered conversational platform
Adopting a cloud-first strategy
Migrating workloads to Microsoft Azure
Using Azure Arc for hybrid infrastructure management
Utilizing Azure Monitor for monitoring and visibility
Implementing Microsoft Defender for Cloud for security
selected Joint Use 365 from Varasset Software
integrated existing data into Microsoft Dataverse
automated workflows with Microsoft Power Automate
enabled data-driven decisions using Microsoft Power BI dashboards
implemented Joint Use 365 on Microsoft Azure tenant
Adoption of Microsoft Power Platform
Integration of multiple apps into a single interface
Creation of tailored debt repayment plans
Partnered with Microsoft
Implemented Microsoft Intelligent Data Platform with Azure components
Streamlined master data management (MDM)
Adoption of Dell laptops running Windows 11 Pro
Utilization of Microsoft Power BI for reporting
Cloud-first approach
Agile way of working
Partnership with Microsoft and Ernst & Young
Developing a framework called 'Seed' with KPMG
Choosing Dynamics 365 for agile implementation capabilities
Training users and installing new POS systems
Leveraged Azure technology
Developed value-added services
Integrated digital and physical healthcare journeys
Implementing ESML AI Factory on Microsoft Azure
Collaboration with Microsoft Customer Success Team and Cloud Partner Program members
Data lake concept formulation
Selection of Azure Synapse Analytics and Power BI
Collaboration with JFE Systems and System EXE
Formation of data governance team
"Adopting a modern, cloud-based solution"
Partnering with Microsoft FastTrack and Ferranti Computer Systems
Rigorous three-week scrum schedule for updates
Transition to Microsoft Azure
Automated deployments
Secure data management
Employed Microsoft Power BI and Azure Synapse Analytics
Used Azure Data Explorer (ADX)
Linking ADX to Power BI
Optimized Power BI queries and reports
Moved healthcare analytics platform to Microsoft Azure
Created Platform for Analytics and Data (PANDA)
Collaborated closely with Microsoft for data security
Initiated an IT roadmap
Selected Microsoft Power Platform
Enlisted the expertise of VISEO
Migrating SAP landscape to Azure
Implementing a new cloud-native SAP billing platform
Using Azure API Management to tie in SAP environment with other applications and datasets
migrated to Dynamics 365 Business Central
implemented dashboards with Microsoft Power BI
deployed Power Apps for salespeople
Built an enhanced business continuity and disaster recovery strategy around Microsoft Azure
Used Commvault’s Metallic software as a service built on Azure
Architected and moved entire infrastructure to on-premises VMware clusters
Adopting Microsoft Power Platform
Launching a group-wide Microsoft Power Apps and Power BI training program
Creating an internal app store for Power Platform applications
Selection of Power BI and Microsoft Intelligent Data Platform
Integration of Microsoft Fabric
Building Alluvial as an end-to-end platform
Deployed Microsoft 365 Education
"Transitioned to Microsoft Outlook for email, OneDrive for data storage, and Teams for communications"
Organized a two-month change management campaign
Collaboration with Pepas Cloud
Development of Deep Quotes app using Power Apps
Digital transformation from paper-based to cloud-powered collaboration
Use of Azure Active Directory B2C and Microsoft Power BI Embedded
Selection of NSSOL for platform construction and system design
Agile development approach
Partnership with Microsoft Azure
Integration of Mediacoach with Azure platform
"Deployment of Mediacoach, an advanced match data visualization platform"
"Launching Beyond Stats, an advanced football statistics initiative"
Adoption of Microsoft Azure
Development of Shoppix mobile app
Use of Azure Databricks and Delta Lake for data processing
Containerized Azure microservices
Migration to the cloud
"Deployment of machine learning, AI, and IoT technology"
"Implementation of Microsoft 365 E5, Dynamics 365, and Azure stack"
Training and upskilling programs
Using Microsoft Graph Data Connect to access de-identified collaboration and communication data
Creating customized Power BI dashboards
Use Azure Synapse Analytics for financial reporting
Adopt Microsoft Power BI
Implement data literacy training
"Adopting Microsoft Dynamics 365 Marketing, Sales, and Customer Service"
Customizing features on a case-by-case basis
Implementing a global search function for quick data access
Migrating critical infrastructure to Microsoft Azure
Using Azure VMware Solution for minimal disruption
Implementing Azure Synapse Analytics for data analysis
Using Microsoft Power BI for data visualization
using Microsoft Power Apps
creating real-time dashboards
developing a suite of business applications
Migrating to Azure in 2014
Embracing the cloud to offer mobile and web clients
Utilizing Microsoft Unified Enterprise support
"combining right strategy, user-oriented technology, and optimum design"
full cloud utilization
utilization of Microsoft Azure
internal practice of data-driven management
Implemented Dynamics 365 Finance and Dynamics 365 Operations
Built a single digital core
Automated operational management processes
Streamlined budget planning process
Used Power BI for data analysis and reporting
Adopted Azure data centers for data storage
"Adopted Microsoft solutions such as Azure, SQL Server, and Power BI"
Implemented a hybrid cloud solution
"Developed in-house software, IST, using Microsoft tools"
Utilized machine learning algorithms for data encoding
Adopt Microsoft Dynamics 365
Fast initial rollout to 700 professionals
Dedicated change management team
Consolidating ERP systems with Dynamics 365 Finance and Dynamics 365 Project Operations
Implementing Power BI for analysis and visualization
Integrating the platform with Office 365
Partnered with Microsoft and Predica
Adopted Microsoft Azure and Power BI
"Designed architecture, set up cloud infrastructure"
Adoption of Microsoft Cloud for Nonprofit
Integration with existing Microsoft tools
Collaboration with Microsoft partner Kerv Digital
Use of pre-built components and custom features
introduced mixed reality technology from Microsoft
introduced augmented store
use of Microsoft Dynamics 365 CRM software
Partnering with Microsoft and PTC
Utilizing Microsoft Azure IoT and PTC ThingWorx
Creating IntelliDraught to turn equipment into smart devices
migration to Microsoft Azure
integration of SAP ERP system with Azure
use of IoT sensors and data insights platform
Reviewing solution requirements
Implementing Azure and Power BI
Setting up a dedicated Power BI Center of Excellence (CoE)
Partnered with Infront Consulting Group
Developed a cloud solution powered by Microsoft Azure
Implemented IoT devices to collect information
internal developers created a solution based on the Microsoft Intelligent Data Platform
Azure Synapse Analytics for real-time analytics
Microsoft Power BI for intuitive dashboards
Partnered with SRKK to implement Power BI
Built a Financial dashboard for real-time visibility into financial data
Created a Human Resources (HR) dashboard to manage workforce
Used existing Microsoft 365 licenses
Deployed Microsoft Teams for remote collaboration
Created intraoffice apps using Power Apps
Implemented digital health-screening protocols
"Chose Microsoft Azure, .NET, and Visual Studio"
Used fully managed Azure services
Leveraged open-source tools on Azure
benchmark studies
self-created Digital Academy C platform
training teams with evolving know-how
performed gap analysis
envisioned the target state
designed and built the IoT platform
added additional data sources
developed analytical models
unified customer data platform
targeted segments based on insights
personalized engagement journeys
Adopting Power BI and expanding its usage
Building a data warehouse and a unified data model
Creating the Samurai platform with Sensei and Ronin
Deploy Dynamics 365 Supply Chain Management
Move away from heavily customized system
Adopt cloud-based approach
Utilize Golden Template for standardization
using Dynamics 365 Supply Chain Management
using Power BI to analyze and visualize data
"implementing Microsoft Azure, Power BI, Dynamics 365, and Power Apps"
Migrating data warehouse to Azure Synapse Analytics
Using existing WhereScape automation software
Adopting Microsoft Power BI for data visualization
Built a SaaS tool powered by Microsoft Azure Machine Learning
Utilized responsible AI practices
Crawled the web and third-party data
Engaged subject matter experts
Deploying remote triage via telehealth
Using Microsoft Teams for secure video conferencing
Using Microsoft Power BI for data analysis and dashboard creation
Extensive production simulation test
Training and best practices
Integration with existing POS and WMS systems
Remote implementation using Microsoft Teams
Integrated Microsoft Dynamics 365 with external systems
Moved to cloud-based ERP solution
Automated processes like production orders
Migrating SAP and non-SAP environments to Microsoft Azure
Phased migration of entire estate
"Collaboration with Microsoft, SAP, and external partners like Accenture, LTIMindtree, and HCLTech"
Simplifying support models and application estate
Moving network to internet-based technologies
Adopting Microsoft Power Platform
Using low-code approach
Citizen development programme
Integration with Microsoft Dynamics 365
Automating order processing and fulfillment workflows
Using Power BI for enhanced reporting and analytics
Implemented a cloud solution on Microsoft Azure
Reviewed and simplified technical architecture with Microsoft team
Enhanced processes with technical support and functional guidance from Xencia
Used Azure's multi-cloud and hybrid cloud solution for consistent experience
Leveraged Azure credits and technical support
Digital transformation using Microsoft Intelligent Data Platform
Building a data platform for user insights and personalization
Reforming the Belgian customer loyalty program
Developing advanced analytics platforms with Microsoft
"Using Azure technologies (Azure Data Lake, Azure Data Factory, Azure Databricks, Azure Synapse Analytics)"
investment in Microsoft’s Cloud for Nonprofit
partnership with Microsoft and Wipfli
community engagement model evolved alongside technology architecture
digital skilling initiative
collaboration with Microsoft
flexible training schedules
Using aggregated data sources
Central dashboards visualized with Power BI
Excel spreadsheets as initial step
Microsoft Intelligent Data Platform
Using Azure Machine Learning
Responsible AI
Microsoft Data Scientist Program
Creating AI models to understand student attrition
Identifying key predictors of student attrition
Massive investment in new technologies
Collaboration with JEMS for development and deployment
Use of Microsoft environment and technologies
Adoption of OKR framework
Recommendation and implementation of Microsoft Viva Goals
Internal training and communication of OKR best practices
Migrating entire SAP estate to SAP on Azure
Using Azure M-series virtual machines and Premium SSD storage
Deploying a parallel N+1 landscape for development and testing
Utilizing Azure Site Recovery and a multiregional strategy for disaster recovery
Implemented Dynamics 365 Business Central
Integrated Power Apps
Transitioned from paper-based to digital system
Unified back- and front-end operations
Adopting Microsoft Azure Data Explorer (ADX)
Using Power BI for additional insights
Integrating Apache Kafka for ingesting data
Incorporating Airflow for workflow management
Using Azure Functions for faster coding
Employing Log Analytics for simpler analytics
Acquired Surface devices
Implemented Microsoft 365 at all levels
Used Microsoft Intune for device management
Employed Azure Active Directory for access control
Adoption of Microsoft Intelligent Data Platform
"Building analytics architecture with Azure Data Lake, Azure Synapse, Azure Machine Learning, and Power BI Premium"
Conducting Data Strategy Assessment with Obungi GmbH
Workshops for Power BI training
Hybrid migration approach
Engage client architect from SCC
Working with Hewlett Packard Enterprise
Move SAP workloads to Azure
Use of Microsoft Power Pages to overhaul web presence
Build six new portals
"Define solution requirements, pick needed components, drag and drop configuration"
Adopt Microsoft Dynamics 365 Finance
Centralize data and operations
Train staff on Management Reporter
Migration to Microsoft Dynamics 365
Implementation of cloud-based ERP platform
Utilization of Microsoft Azure for integration
Development of mobile applications for workforce
Expansive migration to Microsoft Azure
Collaboration with digital transformation partner InSpark
Use of Azure Migration and Modernization Program
Adoption of a hub-spoke network topology in Azure
Investment in Azure Backup and Azure Site Recovery
migrating SAP and peripheral systems to Azure
using Azure landing zone and SAP infrastructure as a service platform
active disaster recovery design and drills
"using Microsoft products like Microsoft 365, Microsoft Power Platform, Power BI, Power Automate, and Power Apps"
Adoption of agile project management
Reorganization of workforce into cross-functional Squads
Implementation of OKR goal management with Microsoft Viva Goals
Implemented X·CELERATE Insights analytics solution
Created ETL pipelines using Azure Synapse Analytics
Designed semantic models for Power BI reports and dashboards
Deploying Microsoft 365 including SharePoint and Microsoft Teams
Using Microsoft Dynamics 365 Customer Service and Microsoft Power Platform
Creating a solution based on Microsoft Dynamics 365 and Power Platform
Partnering with ITK Consulting
Communicating Microsoft 365 changes with the Microsoft 365 public roadmap
Using Power Automate workflows for quality and operational integrity
Aggregated data sources
Central dashboards visualized using Power BI
Use of Microsoft Intelligent Data Platform
Hourly data updates to SQL database
Harmonizing disparate data governance structures
Using Microsoft Intelligent Data Platform
Establishing Santeon Training Academy on Azure
Implementing Data Architecture and Interoperability
"Adopting Azure DevOps, Teams, and SharePoint"
partnered with Confiz
implemented Dynamics 365 Finance and Operations
invoice automation
courier portal integration
"Azure Cloud, Dynamics, and Power BI capabilities"
Migration to Microsoft Synapse cloud
Development of Warehouse Management System LogisCore
deploying Microsoft Dynamics 365
using Microsoft Azure and other Microsoft services
using Microsoft FastTrack for support in streamlining and accelerating cloud deployments
Using Microsoft Azure for cloud storage solutions
Automating video classification with AI
Pilot project with Azure Cognitive Services
Use of Azure Machine Learning for data models
Accessing support from Microsoft customer success team
Collaborated with Microsoft Services
Brainstorming session to articulate business goals
Developed a global digital sports platform
Partnered with Crayon and Pepas Cloud
Built tailored solutions with Microsoft Power Platform and Power Apps
Consolidating data into an on-premises environment
Migrating legacy systems and data to the cloud
"Building BEAM, the cloud data platform"
Using Microsoft Azure Synapse Analytics and Power BI
migrated on-premises data centers to the cloud
built an integrated data platform named Pegasus
implemented single sign-on and multifactor authentication
leveraged Azure Kubernetes
Digitization using Power Platform
Intuitive data entry with Power Apps
Configurable approvals process with Power Automate
Powerful data reporting with Power BI
Adoption of Microsoft Power BI for data visualizations
Use of SDMX standard to restructure data
Adopting a new MLOps platform
Using Microsoft technology
Building a data science team
Creating a repeatable framework for MLOps
implementation of a Data Lakehouse architecture
use of Power BI for data analysis
data integration through Azure Data Factory and Azure SQL
early migration to Microsoft Synapse
Partnering with Microsoft and Hexaware
Migrating to the cloud
Building a proof of concept using Microsoft Azure Synapse Analytics
Implementing an architectural framework using Microsoft Azure Databricks and Azure Data Lake Storage Gen2
Developing ETL pipelines using Azure Data Factory
Creating a self-service end user zone with Microsoft Power BI
Cloud-based CRM solution
Proof-of-concept trials
Feedback loop between development team and users
Reducing the scope and leveraging out-of-the-box features
Long-standing partnership with Microsoft
Migration to Office 365
Adoption of Power BI and other Power Platform services
Development of a new data strategy
Implementing DASA
Creating a Center of Excellence for Power Platform
Migrating to Business Central for cloud benefits
Integration with Mediaocean for media buying system
Creation of Sequential Liability process
Development of Power BI cash flow report
Partnership with Microsoft and Kabel
Creation of the DIY program
Use of Microsoft's low-code technology
Adopting Microsoft solutions
DocuSign integration
"Using Microsoft Azure, Power Platform, Power BI, and Microsoft 365"
Partnered with Microsoft and the Azure cloud platform
Developed a standardized and consistent Azure-based platform
Conducted a test case in APAC region to build the playbook
Engaged with Microsoft for cloud migration
"Moved office workloads and file storage to Microsoft Azure, SharePoint, and OneDrive"
Developed in-house solution with Microsoft Azure and Azure Cognitive Search
Used Azure Integration Services for live data feed
Developed the Perinatal Research Biorepository (PRB) Query Tool
Used Microsoft Power BI for data visualization
Used Microsoft Azure for cloud-based storage
Merged data from several systems into one platform
Unifying data layer implementation
API-based integration strategy
Use of Microsoft Cloud Solutions
Migration of all data to Microsoft Synapse cloud
Creation of Warehouse Management System LogisCore
Development of a scalable and developable data architecture
Use of Microsoft Power BI as the data information platform
Turning multiple client records into accessible information
Using demographic data to optimize appointment scheduling
Use of Microsoft Graph Data Connect
"Integration with Microsoft 365 data for productivity, identity, and security"
Building predictive and prescriptive analytics models
Use of Digital Manufacturing Platform (DMP)
Cloud strategy built on Microsoft Azure
Utilizing Azure IoT connectivity components
Adopted Microsoft Power Platform
Created Safety Log with mobile app and dashboard system
"Obtained 'per user' license for 1,800 colleagues"
"Used Microsoft Dynamics 365 Sales, Teams, and Power BI"
Integrated Microsoft Teams chat within Dynamics 365 Sales
Followed an approach that prioritized building trust and consensus among agents
Adopting Microsoft Azure cloud services
"Using Azure Database for PostgreSQL, Azure Cosmos DB, and Azure SQL"
Integrating data into an Intelligent Data Platform
Leveraging Power BI for reporting and data modeling
Built a dashboard using Power BI datamarts
Automated data processes and consolidated information
Utilized real-time analysis for crisis management
Integrated multiple data sources into a single location
Building the Global Data Platform (GDP)
Adopting Microsoft Azure
Using cloud-based services
Built a cloud-based platform on Azure Synapse Analytics
Launched a device-distribution program
Engaged with Microsoft to develop necessary applications quickly
Partnered with Microsoft technology implementation partner Fellowmind
Discovery phase to analyze current state and envision future solution
"Implementation of Microsoft Project, Power Platform, and Teams"
"No custom development, configured Microsoft technologies"
Selected Microsoft Project
Developed a cloud-based PPM platform
Integrated with Microsoft Teams and Microsoft 365
Built enterprise application using Microsoft Power Apps and Microsoft Power Automate
Integrated approach with Microsoft Dynamics 365
Use of Power BI for detailed reporting
Azure Maps for mapping visuals and location data
Automation with Power Automate Flows
Centralized project and financial resource planning
Adopt Microsoft Azure
Deploy Microsoft 365
Use Azure Virtual Desktop
Automate processes with Microsoft Teams PowerShell module and Azure Active Directory
Relied on Microsoft Services and Power Platform
Empowered trainees to develop solutions
Dynamic adjustment of project requirements
Leveraging Microsoft Intelligent Data Platform
Creating a modular data mesh architecture
Conducting exploratory sessions and envisioning meetings
Microsoft workshops and skilling programs
Early decision to implement Microsoft 365
"Selection of Dynamics 365 Business Central, Sales, and Marketing"
Building core systems with API interfaces
Partner with Microsoft
Leverage Microsoft Power Platform
Shift data to the cloud
Consolidate tooling and data
Build a modernized data platform
Follow a data mesh architecture
Engage a vendor for strategic vision
Standardize on Power BI
"Implement a new, reimagined architecture"
Full integration into Microsoft 365
Use of plug-and-play app iGlobe CRM
Subscription-based licensing model
consolidated email systems
adoption of Microsoft 365 suite
development of SharePoint champions
Using Microsoft Teams and Teams Phone for collaboration
Leveraging Microsoft 365 suite for seamless transition
Adoption of Azure-based SAP PE software as a service (SaaS) solution
Migration of existing SAP and non-SAP peripheral systems to Azure
Use of hybrid cloud capabilities to run applications in the most suitable location
Building a unified platform with Azure Synapse Analytics
Adopting Azure API Management
Automating data entry from Azure Data Lake
Integrating Synapse Analytics with Power BI
"Utilization of Azure Synapse Analytics, Microsoft Power BI, and Microsoft Purview"
Building a data analysis platform
Company-wide implementation of Microsoft 365
Implementation of digital transformation
"Using advanced platforms like Dynamics 365 Sales, Microsoft Azure, and the Microsoft Cloud"
Developing a five-year plan for managing diverse digital initiatives
Partnering with Microsoft for digital transformation
Implementing Microsoft 365 across all offices
Leveraging Microsoft Teams for remote work and virtual meetings
Using Microsoft SharePoint for real-time data access and document sharing
Hosting campaigns on Microsoft Azure
Refreshing legacy systems
Using Microsoft Power Platform including Power Apps and Power BI
Streamlining teamwork via Microsoft Teams
Migrated to Microsoft Azure
Utilized FastTrack for Azure and Enterprise Skills Initiative for upskilling staff
Defined primary and secondary datacenters
Used Azure Site Recovery and Azure Backup for business continuity
Deployed Microsoft 365 E5 for communication and file storage
Used Microsoft Surface devices to reduce CO2 emissions
Employing Microsoft solutions
Joint data strategy assessment with Oraylis
Phase-wise strategy development
Lighthouse project for initial implementation
Integration of Power Virtual Agents
Using CRM data for personalized conversations
Unified reporting with Microsoft Power BI
Collaboration with Microsoft
Development of ALYCE platform
Using AI and ML for data processing
Public invitation to tender for solutions
Use of Azure Intelligent Data Platform
Integration of Azure Synapse Analytics with existing Microsoft Power BI
"Migration to the cloud to improve efficiency, safety, and cost-effectiveness"
Building decentralized architecture and breaking down centralized BI teams
Migrated parts of European operations to the cloud
Implemented Microsoft Teams and Exchange Online
Used Multi-Geo to distribute data across multiple regions
Power Platform for data exchange with SAP
Published SAP reporting and GFOS in Power BI Premium
implemented Shopify connector app for Microsoft Dynamics 365 Business Central
used Microsoft Power BI for better insights on sales performance
"collaborated with Scapta, a Microsoft Cloud Partner Program member"
Integration with state’s Human Services Department (HSD)
Automated referrals based on eligibility for other programs
Use of Microsoft Dynamics 365 and data analytics through Microsoft Power BI
Switch to Microsoft Azure services
Migrate microservices to Azure
Setup data lake for historical data
Implementation of Azure File Sync
Use of Microsoft technologies
Adopt Microsoft Power Apps
Leverage low-code development platform
Utilize in-house development
Using Microsoft Power BI and Azure Synapse Analytics.
"Migrating data infrastructure into a fully managed, serverless platform with the help of Insight."
"Using a data lakehouse with Microsoft Azure Data Factory, Azure Synapse Analytics, and Azure Databricks"
Combining data foundation with Microsoft Power BI
Creating centralized data sources and improving the architecture with delta tables
Decentralized IT capabilities
"Implemented Power BI, Power Apps, and Power Automate"
Implementation of Microsoft Dynamics 365
"Development of specific systems like Z-Chamado, Commercial CRM, Workflow Underwriter, and Z-Atende"
Migration to Microsoft Azure ecosystem
Use of Fast Track Support from Microsoft
Collaboration with element61
Universal data platform on Microsoft Azure
Use of IoT sensors for various monitoring purposes
Data storage and availability via Microsoft Power BI
implemented an Enterprise Datawarehouse
moved to PaaS Services on Azure cloud
implemented Microsoft Intelligent Data Platform
deployed Power BI Premium for enterprise dashboarding and reporting
Unified disparate case management systems into a singular data warehouse
Digital transformation journey
Digitized supply chain using Microsoft Azure
Adoption of AI and machine learning models
Development of IoT solutions for shop floor connectivity
Use of Microsoft Power BI for richer insights
Adopting Microsoft Cloud and low-code/no-code technology
Utilizing Power Platform
Partnered with Microsoft for digital transformation
Supported by Jaffer Business Systems for end-to-end service
Implemented Azure Active Directory for security
Using Microsoft Power Platform
Creating in-house solutions
Aligning with business stakeholder and data strategy
Developed ECHT Dashboard based on Azure Synapse Analytics
"Partnered with Allgeier, a Microsoft Partner Network member"
Utilized Power BI and Azure DevOps for fast development
Setup Azure Data Factory pipelines for data gathering
Employed machine learning algorithms for data matching
cloud-based digital solution
partnering with Microsoft
weekly meetings with technical experts
Use of Azure Synapse Analytics
Integration with Microsoft products like Azure and Microsoft 365
Adoption of a scalable DWH solution
Adopted Microsoft Dynamics 365 Customer Insights
Integrated multiple data sources to create unified profiles
Used AI models for product recommendations
Leveraged Customer Insights for data unification
Deployed Windows 11 Enterprise across the entire company
Created global hardware standards
Used Intune for managing new hardware
Ring approach for deployment
Standardized hardware and drivers
Adoption of Microsoft 365 and Dynamics 365 Business Central
Consultation with BDO Lixar for ERP practices
Implementation of Viva Connections
Establishing a 'semantic model library'
Leveraging Microsoft Power BI
Automating reports
Using incremental refresh option
adopt Azure Cloud
"work with Microsoft and Microsoft Partner, New Business Dimensions"
roll out Microsoft 365
implement Power BI
Collaborated with Microsoft
Utilized Power BI and Azure Synapse Analytics
Developed 'Route to Market' Analytics Solution
adopt Microsoft solutions
partner with Softtek
"use Power Apps, Power Automate, Azure SQL, and Power BI"
Use of Microsoft Power BI and Azure Synapse Analytics
Create a single place for data access
Develop principles based on staff feedback
Replaced Citrix environment with Microsoft 365
Used Azure Virtual Desktop for remote access
Use of Microsoft Azure solutions
Integration with electronic medical records (EMRs)
Creation of a unified digital care pathway
"Deployment of Azure Functions, Logic Apps, Service Bus"
Adopting Microsoft Dynamics 365 Field Service and Business Central
Integration with Power BI and Microsoft Teams
Integrated ITM Portfolio and Project Management based on Project for the web and Power Platform
Seamless integration with Microsoft Teams
Customizing the solution with the support from Microsoft and partner LimeOn company
Developing a digital transformation strategy
Formulating training programs
Implementing cloud-based services
Contacted HSO for developing a suitable concept
Adopted Microsoft Dynamics 365
Utilized Binary Stream Subscription Billing Suite add-on
Used Microsoft Power Platform for customization
Use of no-code platform smapOne
Integration with Microsoft Power Apps
Utilization of Microsoft Azure
Partnership with Celebal Technologies
Use of Azure Synapse and Azure Data Factory
Development of a reference architecture using public cloud technologies
Proof of concept with Microsoft Cloud architects
Use of infrastructure as code
Use of Microsoft Purview Communication Compliance and eDiscovery
Role-based access controls
Pseudonymization of usernames
Audit logs
Used Azure IaaS and PaaS
Participated in Microsoft for Startups program
Validated designs at Microsoft Technology Center
Partnered with Microsoft and Accenture
Developed a highly scalable data ingestion and analytics system in the cloud
Embraced Industry 4.0 approach
Transition to a centralized CRM system
"Implement Microsoft Dynamics 365, Microsoft Power BI, and Microsoft Cloud for Nonprofit"
Use of Nonprofit Common Data Model
Adoption of Volunteer Management and Engagement tools
Integration of Fundraising and Engagement solution
Adopting Microsoft Teams for communication
Using Power BI for real-time visualization
"Integrating various Microsoft services (Forms, SharePoint, Power BI)"
Establishment of Integrated Digital Strategy Div.
Collaboration with Microsoft’s Data Hack program
Mandatory e-learning training for all employees
Used Microsoft Power Platform tools
Integrated app with Microsoft Teams
Collaboration with Microsoft partner B2 IT
Using data-led insights and workplace analytics
Collaborating with Microsoft Viva Insights
Trialing meeting-free Wednesdays
"Implemented a full stack solution from data engineering, analytics, and visualization"
Partnered with Microsoft to implement Power BI
Adoption of a data fabric approach
Loose coupling configuration for DEEP
Agile development system
Building a prototype environment quickly
Utilizing Azure services like Data Factory and Microsoft Purview
Adopt Microsoft Dynamics 365 Customer Insights
Conduct thorough evaluation of partner/service provider
Use built-in AI for predictive analytics
Implemented revenue analytics first to convince leadership of the project's value
"Divided data from 14 sources into three categories: structured, unstructured, and machine learning use cases"
Created 53 data pipelines and analyzed more than 60 business rules and over 100 measures
Implementing Microsoft Azure SQL-driven approach
Using Power BI for reporting
Combining MDX and Microsoft Azure SQL queries
Extracting data using an MDX query from OLAP cube
Adoption of Microsoft Dynamics 365 for enterprise fundraising and donor management
Use of Power BI for reporting
Implementation of AI and IoT solutions
Centralized data storage through Azure
Use of Microsoft Teams for communication
Adoption of Microsoft Dynamics 365 solutions delivered by DXC Technology
Use of standard solutions with continuous updates
Adopting Microsoft Power BI and Azure Synapse Analytics
Creating a proof of concept for membership dashboards
Using Azure Data Factory for ETL processes and orchestration
Integrating Azure Databricks for data processing
Migrating legacy data systems to the cloud
Use RISE with SAP on Azure
Engage with Kaar Tech for deployment
Centralized all stores under one platform
Eliminated manual work
Streamlined processes
Engaged in extensive training sessions with Business Experts Gulf
Using Microsoft Power Platform
Create more than 25 apps in support of the healthcare sector
Develop new solutions for contact tracing and vaccinations
Leveraging Microsoft cloud stack
Implementing Dynamics 365
Building PowerBI dashboards
Developing telemetry systems
Partnership with Microsoft
Building on Azure for US Government and Power BI
Developing a cloud-native solution with microservices
High resilient infrastructure
"selected Microsoft Azure for scale, security, and cost"
built multiple prototypes in the cloud
deployed Azure Active Directory for security and access control
Partnered with Microsoft
Used Dynamics 365 for ERP system replacement
Automated processes using Power Platform
Mapped requirements to Microsoft Dynamics 365 Business Central
Carefully mapped each requirement of the equipment maker
Adoption of Microsoft Navision for Finance and HR
Use of Microsoft 365 for collaboration
Migration to Azure Cloud
Adoption of PowerBI for reporting and productivity measurement
Adopt a complete Microsoft application and infrastructure environment
Implement and track SOPs in customer journeys
greenfield implementation of S/4HANA
ERP migration from on-premises to SAP on HANA
collaboration with Microsoft
use of Azure products and services
Adopting cloud computing via Microsoft Azure
Internal development of applications and automation flows
Using Microsoft Power Platform for data visualization and decision support
migrating workloads to Azure cloud
developing apps and automation flows using Microsoft Power Platform
utilizing Microsoft 365 collaboration tools
Adoption of Microsoft Dynamics 365 Project Operations
Phased implementation with an MVP in three months
Use of scaled agile framework
Teaming up with implementation partner Fellowmind
Building entirely on Microsoft Azure
Using Azure IoT Hub for device management
"Leveraging Azure serverless computing, Azure Logic Apps, and Azure API Apps"
Utilizing Azure Data Factory and Databricks for data processing
migrating to Microsoft Azure
leveraging Azure services for security
using additional Microsoft tools like Power Apps
building an Azure Data Lake
using reserved instances for cost savings
Use of Microsoft Power Platform to create a simulation game
Development of a chatbot using Power Virtual Agents
Tracking progress using a Power BI platform
Restructure the ICT system
Form cross-global teams by functional domains
Adopt Microsoft 365 as the global unified base
Deploy Microsoft Dynamics 365 Finance and Supply Chain Management
Deployment of Microsoft 365 tools
Migration of retail POS systems to Azure
Using Exchange Online for email
Utilizing multi-factor authentication and active threat protection
Invested in Power BI for reporting and analytics
Creation of ESPN Career Center using SharePoint spaces
Personalization through five personas
Integration of Bitmojis and gamification
"Teaming up with paiqo, a specialist in data and AI solutions"
Digitalization of production processes
Integrating digital tracing in-line without disrupting production
Adoption of Dynamics 365 Business Central
Integration with SCADA systems
Deployment of customer and vendor facing apps
Use of Power BI for real-time data access
Leveraging Microsoft Teams for collaboration
Exploring low-code apps
Using Microsoft PowerApps
Integrating Microsoft Teams with EHR EPIC
Collaboration with Microsoft and Stratiteq
Use of Microsoft Power BI supported by Azure
Implementation of DOC -Data in One Click- project
"Establishment of an internal, cross-cutting, interdepartmental operating structure"
establishing a foundation for applications using Microsoft Power Platform
forming a Power Platform Team
creating a centre of excellence
use of Microsoft Azure Data Factory for data integration
storing data in Azure Data Lake
using Power BI for report generation and dashboard creation
Adopting Microsoft Azure Machine Learning and other Azure AI services
Building the integrated forecasting solution in the cloud using machine learning capabilities
Collaborative planning with business partners
migrating from on-premise platform to Azure
upgrading legacy finance and accounting systems
utilizing AI-driven analytics of Power BI
automating processes via Power Apps
Implementing Pro-Sapien's HSEQ software on Microsoft 365
Increasing accessibility and engagement through familiar interfaces
Focused on fixing backend performance
Created a short-term automated solution with Azure Synapse for Power BI
Used Azure SQL sandbox methodology
Deploying RISE with SAP on Azure
Integration with Microsoft Cloud
Using SAP Business Technology Platform (BTP) with Microsoft Teams
Migrating to the cloud
Deploying SAP Fashion Management
Using RISE with SAP on Azure
Creating a cloud foundation
Using Microsoft Power Platform
Creating a custom API
Integrating with diary-management system
Developing a Power BI dashboard
Using Microsoft low-code and scalable infrastructure
Leveraging Azure services for serverless computing
Integration with Salesforce CRM using Power Automate
Building a mobile app with Power Apps
Using Power BI for dashboards and data-centric information
Adopt Dynamics 365 Human Resources
Integrate solution with Microsoft 365
Develop a process for automatic Outlook entries
Deploying Microsoft cloud services
"Implementing Azure Data Lake, Azure Databricks, and Power BI"
Creating omnichannel dashboards
Deploying Microsoft Cloud technologies and Microsoft Surface devices
Adopting Microsoft 365 and the Microsoft Cloud
Providing field workers with new Microsoft Surface tablets
Adoption of Microsoft Power Platform
Establishment of Power Platform Center for Enablement (C4E)
Use of Power Platform Center of Excellence (CoE) Starter Kit
Carried out detailed analyses and extensive comparisons of providers
Switched to Microsoft Azure for speech-to-text feature
Set up the new system in parallel without losing data
Utilizing Microsoft Azure
Implementing serverless technology
Setting up an end-to-end technology stack
Using a single data-science landscape for all projects
Utilizing Microsoft Azure services
Proof-of-concept testing with multiple partners
"Deployment of Azure services including Azure Data Factory, Azure Data Lake Storage, Databricks, Synapse, Enterprise Power BI"
Partnered with Microsoft
Rolled out training sessions
Adopted Microsoft’s Enterprise Skills Initiative
incorporating rapid image processing into a workflow
utilizing serverless Azure Functions and Azure Data Factory integrations
collaborating with Microsoft and Terawe
Introduction of chatbot Claimens
Utilizing Azure Cognitive Services
Collaboration with Parloa for telephony integration
automated approach by JSR
refined PMO process
smart planning templates
up-to-date dashboards
chose Azure Data Services for adaptability
developed a unified platform with Azure Synapse Analytics and Power BI
self-hosted integration runtime via Azure Data Factory and Express route
Introduced KBconnect platform based on Microsoft Azure IoT Central
Added IoT connectivity to QPM system
Moved cloud infrastructure and IoT platform to Azure
"Adoption of Microsoft Office 365 E1 and E3 + EMS E3, Office 365 ATP P1 and Power BI Premium"
Free COVID-19 E1 Trial by Microsoft leading to enterprise-wide implementation
Implementation of Azure Synapse Analytics and Azure Databricks
Conducting a hackathon to create a prototype
Creating a proof of concept in three days
Aggregation of data
Modern web portal implementation
Automated workflows
Digitally transform manual processes
Use of Microsoft Power Platform and Teams
Phased rollout to users
Adopting a fusion approach with enterprise engineering team
Establishing Microsoft Power Platform Center for Enablement
Setting up a small governance team
Creating a Power Platform citizen development governance model
Training engineers on Power Platform
scaling usage of Microsoft Dynamics 365
creating a CRM center of excellence
facilitating bi-directional communication with CRM champions
Developed relationships with strategic implementation partners
Leveraged change-management capabilities
Created technology-enhanced solutions
"Used Microsoft Power Platform, especially Power Automate"
Adopting a 'greenfield' cloud platform
Maximizing automation and orchestration
Migrating SAP environment to Microsoft Azure
Modernizing SAP S/4HANA features during migration
Using Microsoft Teams for communication and collaboration
Deploying Microsoft Viva Insights
Implementing an employee listening program
Launching OPENworking initiatives
Incentivizing flexible vacation policies
Collaboration with Microsoft partner Elitmind
Implementation of Azure Machine Learning forecasting algorithms
Use of Power BI for data visualization and analysis
Engagement with the National Enabling Programmes
Fast-tracking Microsoft Teams rollout
Building a ‘how to’ database
Creating real-time tasking system
shift towards data-driven business decision making
digitalization of the organization
evaluation of BI tools
partnership with Microsoft
Built an in-house solution called Cockpit
Migrated to Power BI and Azure Synapse Analytics
Conducted proof of concept (PoC)
Adopted fact-based PoC to ensure compatibility
Unified storage standards across all divisions
Moved CRM data to Azure cloud
Established central data warehouse
Focused on fundamental productivity KPIs
Implemented advanced analytics
"Adopted Microsoft 365, Power Platform, and Azure"
Established a Digital Workplace concept
Continuous exploration and trial of new features
Carefully planned adoption and change management activities
Deployment of Azure IoT solutions
Use of pilot projects to test different technologies
Integration with existing systems and legacy software
Chose Dynamics 365 Business Central for its flexibility and fast implementation
Collaborated with Microsoft Partner NAVERTICA
Use of Power Apps and Power Platform
Transition to Microsoft Dataverse
Enabled Power Platform for all employees
Center of Excellence Starter Kit
Environment strategy with controlled access
"Adoption of Microsoft Azure, Microsoft Office 365, Dynamics 365 CRM, and PowerBI"
Utilizing a service platform accessible from anywhere with smart systems and mobility
Integrating SaaS services to provide access from any location
Structured approach with IoP project
Development and support strategy
Tiered environment structure
Center of excellence for Power Platform
Move to the cloud
"Adopt Microsoft products including Microsoft 365, Teams, and Windows 10"
Adopt Microsoft Defender for security
Extensive evaluation of several solutions
Choosing a full-stack platform that integrates with Power BI
Leveraging familiar environments like SQL and SQL Server Analysis Services
Developed Driver App using Power Platform
Integrated app with Microsoft Teams and WFM system
Piloted the app at the Las Vegas location
Gained union approval through collaboration with union stewards
Evaluated multiple CRM options
Chose Dynamics 365 and Power BI for seamless integration
Selected ProStrategy for their domain expertise and proven track record
Ran a flexible and innovative project management approach
Formal RFI process to review leading CRM and digital marketing platforms
Choice of Microsoft Dynamics 365 based on business needs
Iterative and dynamic configuration process with user feedback
Pilot projects in select business units
Digitalization of the fitting process
Use of Microsoft Azure for data-based analysis
Go cloud native using open-source technologies and managed services on Azure
Use Azure Red Hat OpenShift for scalable web applications
Utilize a combination of managed Azure database services
partnered with Publicis Sapient Product & Data Engineering team
migrated to Azure
implemented Agile work processes
use of Azure DevOps for planning and execution
"engaged Tipsa, a Microsoft gold partner"
implemented Harvest Management app on Microsoft Dynamics 365 Business Central
supplemented with solutions built on Microsoft 365 and Microsoft Power BI
Partnered with element61
Used Microsoft Azure
Engaged in iterative development with feedback from end-users
Relocation of data processing activities to the Azure cloud
"Use of Azure IoT Hub, Azure Data Explorer, and Azure Databricks"
Partnering with Microsoft for cloud solutions
Engaged legal technologists
Used Microsoft services to build a legal OS
"Adopted Microsoft SharePoint, Lists, and Power BI to optimize and automate tasks"
Leveraging Azure Synapse Analytics
Utilizing Azure Data Factory and Power BI for visualizations
Implementing Spark notebooks for complex analytics
Automation with integrated CI/CD
Adoption of Microsoft Teams Phone
Upgrading from Office 365 E3 to E5
Deployment of custom-built apps using Microsoft Power Apps
Strategic PMO consultancy by Wellingtone
Custom-designed configuration of Microsoft Project Platform
Deployment of Project Online and Project for the web in a hybrid setup
Cross-divisional collaboration
Full-scale AI and data utilization
Using Microsoft's 'Data Hack' for technical support
Equipping geographic business leaders with flexible capabilities
Integration of Power Platform with Microsoft 365 and Teams
Establishment of Center of Excellence (CoE) and Centers of Design (CoDs)
fast-tracked adoption of Teams
"provisioned 30,000 users in a weekend"
created several room types for Teams Rooms
designed a support model for Teams Rooms
Migrating to Microsoft Azure
Building self-service systems
Automating operations
Switching to Azure Spring Apps
Using managed services instead of maintaining infrastructure
Migration to Microsoft Azure
RISE with SAP solution
Bare-metal BullSequana S servers
Collaboration with Microsoft
Migration to cloud infrastructure
Adoption of Microsoft 365 and Dynamics 365
proposed a cloud-based collaborative suite
use of Microsoft 365 E3
Teams for remote planning sessions
"pilot, refined planning, and tested migrations"
Building internal business applications using Microsoft Power Apps
Utilizing Power BI for detailed reporting
Integration with Microsoft Teams for collaboration
Automating workflows with Power Automate
Migration to Microsoft Azure
Competitive bidding process
Use of SQL Server Managed Instances in Azure
Leverage of Azure Synapse Analytics
Data platform kickstart consultation with Islet Group Oy
Utilized Microsoft Cloud Adoption Framework for Azure
Upgraded to Dynamics 365
Shifted to an Azure cloud server
Collaboration with technology partner Unify Dots
Collaboration with Accenture
Forming a Star Alliance with Microsoft
Cloud-first strategy with Microsoft Azure
Setting up Azure infrastructure
Weekly migration of systems
Utilized Microsoft Power Platform Center of Excellence (CoE) Starter Kit
Enterprise-level visibility with Power BI dashboards
Engaged with app makers to streamline environment
Use of Microsoft’s Power Platform
Adoption of low-code development
Collaboration with ministry employees for development
Custom-tailored support from Microsoft Dynamics 365 Sales
Centralize data and improve sales team collaboration
Auto-routing of inquiries and lead categorization
Regular qualitative surveys
Modernizing and consolidating IT environments with Microsoft 365
Reducing after-hours meetings
Adopting focus days without meetings
Adopted Microsoft Dynamics 365 Business Central and Microsoft Power Platform
Worked with partner EFOQUS for deployment
Partnered with Microsoft for implementation
Adopted DataOps with continuous integration and delivery
Delivery of solutions in slivers for immediate and incremental value
"Use of artificial intelligence, augmented reality, and drone technology"
Collaboration with Microsoft and Seikey
"Using Azure Data Factory, Data Lake, Synapse, and Cognitive Services"
Careful evaluation of available platforms
Integration of more than 20 application sources
Implementation of Power BI across all offices
Implemented Microsoft Dynamics 365
Adopted Power BI
Moved to Microsoft 365
Rapid deployment of low-code Power Apps
Utilizing Microsoft Power Platform
Building an app ecosystem for specific scenarios
Development of a Center of Excellence (CoE)
Implemented Azure IoT technology
Created GBConnected solution
Conducted pilot projects with various technology providers
Selected best-fit technology (Microsoft Azure) after testing prototypes
Focused on ease and speed of deployment
Digital transformation
Automation of traditional processes
Infusion of AI
Embracing Microsoft devices and technology
Providing broadband access to students
Using Microsoft Teams for communication
creation of a unified access point for grid assets
"use of Azure Cognitive Search, Azure Maps, and Power BI"
integration of IoT sensors and SCADA systems
Adopted Microsoft Dynamics 365 Supply Chain Management and Dynamics 365 Finance
Expert implementation support from partner Alithya and the FastTrack for Dynamics 365 team
Deployed Alithya EDGE for Operations
Using Microsoft Power Apps and Power Automate to build new applications
Leveraged Microsoft Dataverse for Teams
Developed a custom solution based on Microsoft Dynamics 365
migrating databases to Microsoft Azure
upgrading to the latest version of Windows Server
training IT workforce
partnering with AHEAD
integrating Nursys into national Provider Bridge platform
use of Microsoft Teams Live Events
creation of a studio inside headquarters
collaboration between the communications and IT departments
Rolled out Microsoft Power Platform across the business
Established a Centre of Excellence
Invested in Microsoft Dataverse
Utilized Microsoft Azure and Power BI
Ingested data from various sources into Microsoft Azure Data Lake
Developed optimal configuration for data management
Used Unsupervised Clustering Algorithm for segmentation
Adopted Foresight platform from Analytikus
Implemented early intervention program
Integrated with Azure Data Factory
Adoption of Azure Cosmos DB for database management
Use of Azure IoT Hub to interpret initial watch settings
Utilization of Microsoft Power BI for data manipulation and visualization
Migration of all systems to Microsoft Azure
Use of AIBeacon and Brand Loop Ads for data collection and advertisement delivery
Developing and operating retail media platforms
"Use of Azure Data Factory, Azure Machine Learning, and Power BI"
Advanced analytics to identify customer journeys
Gathering detailed requirements from cross-functional teams
Implementing proof of concepts on various tools
Stress testing tools with increased data volume
Rapid migration to Power BI
"Using Microsoft Azure, Power Platform, and Microsoft 365 technologies"
Creating a single source of truth for tax data
Optimizing the ETL process
Adopting Microsoft 365 and Azure
Implementing Dynamics 365 Customer Service
Gradual changes for smooth transition
Implemented Power BI
Adopted a cloud-first strategy with Azure
Using Power BI for reporting
Partnering with Satori Reporting
Creating an integrated ETL process
Using Azure Data Lake Storage and Azure Data Factory
Combining traditional therapy with a data-centric solution
Building a wellness center for in-person monitoring
Developing a digital platform for data collection and analysis
Partnering with Zühlke Austria
Replaced on-premises business systems with Azure cloud-based services.
Deployed VCC Max to replace entire business system.
Collaborated with QAT Global for agile development using Microsoft products and services.
Going all-in on the Microsoft platform
Deploying Surface devices and Microsoft 365 E5
"Using Microsoft Azure cloud services, AI, and ML"
Close working relationship with Microsoft
Leveraged existing Microsoft 365 environment
Implemented Azure and Power BI with the help of technology partner Systemfarmer
Partnered with Cooler Screens
Utilized Microsoft Azure IoT Edge runtime
Leveraged Power BI and Azure Synapse for analytics
embracing Microsoft Dynamics 365 Business Central and Dynamics 365 Sales
work in the cloud
streamlining and automating processes
deploying additional capabilities without increasing IT staff
Embrace Microsoft tools
Move EHR system to the cloud with Azure
Adopt Dynamics 365 for improved insights
Utilize Azure for disaster recovery and automation
Move daily analytic reports to Azure Synapse Analytics
Partner with Microsoft and Cloud Services for infrastructure assessment
Modernize existing data warehouse
Link data models to Power BI
Adoption of Azure Kubernetes Service
Use of microservices for resilience and scalability
Modular applications for customer-specific needs
Remote monitoring
Predictive maintenance using IoT and AI
Partnering with Toyota for practical tests
Adopting Azure Machine Learning and Power BI
Built on the Microsoft Azure ecosystem
Use of Microsoft technology to underpin connected workspace products
Migrated from on-premises solution to Azure cloud platform
Enlisted Microsoft Gold Partner abtis for implementation
Low-code development using Microsoft Power Platform
Creating a T-Mobile Center of Excellence
Building a community of low-code developers
Pay-as-you-go licensing
Training models with Microsoft Azure Machine Learning and automated machine learning capabilities
"Using Azure PaaS resources to simplify and streamline data storage, processing, and analytics"
"Using Azure responsible AI framework for safer, fairer, more transparent machine learning models"
Utilizing Microsoft products and ecosystem
Building applications in Power Apps
Using Power BI for data insights and reports
"Processing data with Azure Blob Storage, Azure SQL Database, and Azure Data Factory"
Integrating with Microsoft 365 suite and Power Platform
pilot site in Amsterdam
collaboration with Microsoft Cloud Supply Chain Engineering Organization
quick ramp-up and implementation
custom reverse supply chain module
Leveraged Microsoft Azure and Power BI
Integrated data for visual storytelling
Connected workflow and context within EHR systems
Used Azure Synapse Analytics for scalable data architecture
Embedded analytics in EPIC EHR and live data walls
Utilizing Microsoft Power Platform
Creating COBALT (Central Ops Buildings Administered Learning Tracker)
"Used Microsoft 365, Teams, and Microsoft Power Platform apps."
Embedded business consultants and technology professionals within clients' workplaces.
Automated approval processes with Power Automate for workplace safety.
Low-code-first strategy
Building on Microsoft Power Platform
Reuse existing templates where possible
Utilizing Microsoft Azure for scalability and flexibility
Collaboration with Microsoft partner VISEO
Adopting new technologies like Azure Machine Learning Studio and Azure Databricks
Migrating data to Microsoft Azure
Maximizing cloud presence
Using Azure Cloud Adoption Framework and Azure Enterprise Scale Landing Zone
Adopting DevOps practices and Infrastructure as Code for deployment
"Use of Microsoft Teams, Power Platform, and Project Online"
Collaboration with Innovative-e for implementation
Migration to Microsoft 365
Adopted Microsoft 365 and Microsoft Azure
Global rollout led by headquarters in Japan
Employed agile development technique called 'scrum'
"Migrated to VinoTEC, a cloud-hosted solution"
Utilized Microsoft Dynamics 365 Business Central
"Integrated with Microsoft 365, Microsoft Dynamics 365 Sales, and Microsoft Power Apps"
Deploying Microsoft Teams and Power BI
Distributing Surface Go and Honeywell devices
Using Shifts in Teams and Blue Yonder integration
Implement Azure Active Directory B2C
Write user policies for secure access
Utilize Microsoft technologies for identity authentication
Fusion development team collaboration
Use of Microsoft Power Virtual Agents
Custom analytics with Microsoft Power BI
Integration with existing IT infrastructure
In-house cloud migration and deployment
Use of Microsoft’s Enterprise Skills Initiative (ESI) for skill development
"Adoption of Microsoft Azure, Power BI, Data Factory, Microsoft 365, and Teams"
Move data analyses to Microsoft Azure Cloud
Create a data lake and redesign data architecture
Use cloud-native technologies
Implement zero trust security model
adopting the Open Education Analytics architecture
install an API in the collections process
move data to the cloud
create a deep and complex data lake on Microsoft’s Azure platform
Partnered with TRIVIUM BI
Use of Power BI and Microsoft platform
Engaged key colleagues across campus
Transformed 20 years of data into cloud-based solution
Provided training and support to stakeholders
Formed a new team
Switched to Power BI
Standardized on Power BI as data-visualization platform
"Built Power BI dataflows, datasets, reports, and apps"
"engaged TIPSA S.L., a Microsoft gold partner"
undertook a strategic transformation to implement VinoTEC
moved from on-premises systems to tools integrated with Microsoft 365
Collaboration between Data Warehouse and Business Intelligence Teams
Comprehensive and intensive requirement analysis
Adoption of Power BI
Seamless mobile adaptation
Swift onboarding with Microsoft consultants and sales teams support
Use of video footage and AI tools for species recognition
Collaboration with Microsoft Azure
Revamped the whole organization
Consulted with Microsoft for core banking on SQL
Invested in SQL Server Enterprise
Developed Cash in Motion app on Microsoft Power Platform
Integrated sales data with Azure Data Factory and ERP system
Simplify and rationalize Microsoft 365 budget
Centralize IT services
Unify technology standards
Digital transformation since 2007
Adopting Microsoft Azure and technology stack
Development of AI-powered chatbots
Multiple workshops for UX-centric proposal
Structuring multiple data sources
"Using Azure Synapse Analytics, Azure Data Lake, and Azure Databricks"
Leveraging existing Microsoft 365 E5 license
Hosting a Teams Excite Day event
Comprehensive communication plan
Creating dashboards for rollout schedule
Quarterly meet-ups for user feedback
Use of Microsoft Azure AI and Microsoft Power Platform
Conducted multivendor survey
Consultation with employees on AI benefits
Leveraged Microsoft Azure Synapse Analytics and Microsoft Power BI
Trial and pilot phase for two months
Phased implementation of Dynamics 365
Collaboration with Acxiom Consulting
Onboarding Power BI for decision making
Used Microsoft Dynamics 365
Adopted Microsoft Power Platform
Deployed Office 365 and Microsoft Teams
Worked with third-party implementation partners and Microsoft support teams
Adopted Microsoft Surface devices for all staff
Implemented Microsoft Dynamics 365 for various functions
Utilized Microsoft 365 apps and Microsoft Teams
engaged Tipsa S.L.
cloud-based solution built on Microsoft Dynamics 365 Business Central
"free, online OliTEC – Edible Oil: 2-Hour Assessment"
Utilized Microsoft Premier Support
Partnered with Dolphin Consulting
Developed a data warehouse using Azure Data Factory and Azure SQL Database
Cloud-first approach
Use of Azure Databricks and Azure Synapse Analytics
"Development of Information, Data, and Insights (IDI) platform"
Global initiative around Microsoft Azure Defender for IoT
Involving all key stakeholders in the rollout
Creating a Center of Excellence for Global Industrial Security
Adopting Azure Platform as a Service (PaaS)
Leveraging cloud technology to avoid legacy hardware and software
Utilizing Zammo.ai's no-code virtual assistant development platform
Leveraging Microsoft Azure Bot Service and other Azure AI tools
Cloud Adoption Framework workshop
Infrastructure and security assessments
Migrating to Microsoft Azure
Setting up analytics on Azure Synapse and Power BI
Rebuild entire IT infrastructure on Azure
Use Azure Synapse Analytics
Work with Microsoft partner oh22
Invest in training and education
Use of Microsoft Power Platform
Rapid prototyping within six weeks
Developer training on Microsoft Power Platform
Using Microsoft Dynamics 365 Guides and Dynamics 365 Remote Assist on Microsoft HoloLens 2
Creating the TILT Lab for rapid testing and innovation
In-house development to simplify user experience
Use of Microsoft PowerApps and Microsoft 365 licencing
Developing business cases with contributions from Microsoft
Collaborated with Microsoft partner Stridon
Rolled out Microsoft 365 tools
Adapted plans quickly due to COVID-19
Replaced on-premises ERP with Microsoft Dynamics 365 Business Central
Worked with VOX ISM for deployment
Had a training lead for each team
"Deployed core ERP functionality first, then customer engagement capabilities"
Extended Dynamics 365 using the Microsoft Power Platform
Partnered with Microsoft and used Microsoft Azure for compute power
Implemented Microsoft Power BI for data insights
Adopted the serverless compute tier of Microsoft Azure SQL Database
Utilized Azure Data Factory to pull raw data from ERP systems
Employed Azure Functions to trigger data refresh cycles
Used Microsoft Power BI for reporting and analytics
Adoption of Microsoft Teams for Education
"Use of Microsoft 365 tools like Power Automate, Flow, SharePoint, OneDrive"
"Engaged with Athon, a Microsoft Partner Network member"
Adopted Microsoft Dynamics 365 Business Central
Implemented Power BI for reporting
migration to Microsoft Azure
use of Microsoft 365
deployment of Microsoft Power BI
Developing custom in-house solutions using Microsoft Power Platform
Using an agile development methodology
Mapping out care management processes
Adopt Azure Well-Architected Framework
Identify and optimize unused resources
Implement cost monitoring and management tools
Deployment of Microsoft Teams
"Training of over 3,000 judicial servants"
Use of Microsoft Office 365 suite for cloud technology efficiency and internal communication
Utilization of Microsoft Azure for quick solution deployment
Leveraging Power BI for data usage
Deploying Microsoft Dynamics 365 Finance and Supply Chain Management for back-office processes
Using Dynamics 365 Customer Insights for CRM and a 360-degree view of patient interactions
Implementing Dynamics 365 Customer Voice and Dynamics 365 Marketing for enhanced patient experience and marketing campaigns
"Utilized Microsoft Dataverse, Microsoft Power Apps, and Microsoft Power Automate"
Partnered with digital partner Signetic to develop a digital solution
Collaboration with Fidelity Factory
"Utilizing Microsoft Power Platform, Power Apps, Power BI"
Rapid development and deployment
Set up a Center of Excellence (CoE)
Use of Microsoft Power Platform Center of Excellence Starter Kit
Tiered structure of environments to manage access
Collaboration with Microsoft to build a data warehouse with Azure Synapse Analytics
Using public cloud for scalability and agility
Regular weekly cadence meetings with Microsoft
developed a multicloud-driven data strategy
fostered a data-driven culture
"leveraged AI, machine learning, and advanced analytics"
Partnered with Microsoft
Built serverless data solution centered on Microsoft Azure Synapse Analytics
Used Azure Event Hubs and Azure Logic Apps
Partnership with Trivium BI
Use of Power BI to democratize data across campus
Building numerous dashboards and embedding them into a JU app
Standardized marketing tactics and processes
Leveraged Microsoft Power Platform for automation
Used APIs wherever possible
Utilized Power Automate Desktop for tasks not covered by APIs
Created a framework for deciding the level of automation
Complete migration to Microsoft Dynamics 365 Business Central
Integration with Power Platform
Creation of an Alsimet IT Department
Using Microsoft 365 and Dynamics 365 integrated solution
Switching to a Microsoft Azure public cloud-based microservice consumption project
"Implementing Dynamics 365 Business Central, Dynamics 365 Customer Engagement, and Power BI"
Collaboration with Microsoft and ICONICS
Creation of a dedicated internal team (Data Control Room Immobiliare)
Implementation of IoT and AI solutions
Use of Azure IoT platform and ICONICS
Early move to the cloud
"Utilizing Microsoft services like Power Platform, Dynamics 365, and Azure"
Involving citizen developers in app development
"Engaging a digital solutions partner, Insight"
Using Microsoft Power Platform
Collaboration with the Store Leader Council
Input gathering from the Store Leader Council
Utilized Microsoft Power Virtual Agents to build a chatbot
Implemented an omnichannel approach for customer communication
Enabled multi-authoring capabilities for subject matter experts to update the bot
Connected Power Virtual Agents to Microsoft Dynamics 365 Customer Service
Utilized Microsoft Power BI and Microsoft Power Apps
Automated and standardized processes
Implemented a low-code solution for app development
Mapped processes end-to-end for better visibility
Implementing Microsoft Dynamics 365 for Finance and Supply Chain
Implementing Microsoft Dynamics 365 for Customer Service
Using Microsoft Azure
Utilizing Microsoft Power BI for reporting
Use of InfoTiles SaaS solution built on Microsoft Azure
Integration of IoT sensors for real-time data collection
Attendance at Nordic Edge smart city expo to discover InfoTiles
Implement Microsoft Dynamics 365 Finance and Supply Chain Management
"Collaborate with local Microsoft Partner Network member, XPLUS"
Deploying Microsoft Teams
Promoting Microsoft Power Platform
Creating apps with Power Apps and Dataverse for Teams
Adoption of Microsoft 365 and Microsoft Teams
Use of Power BI for real-time reporting
Integration of SMS with Teams via Power Automate
Partnering with technology provider Microsoft
Collaboration with infectious disease and epidemiology experts
Enlisting health insurance providers and biotech startups
"Using Microsoft Power Platform tools like Power Apps, Power Virtual Agents"
Empowering team members to undertake in-house development
Adoption of Microsoft Dynamics 365 Customer Insights
Implementation of Azure Synapse Analytics
Use of Power BI for data visualization
Building a comprehensive customer data platform
Implemented Passport 360
Integration with Microsoft Azure Cognitive Services
Standardized approach across business units
Using Microsoft Power Platform and Microsoft Dynamics 365
Rolling out Microsoft Dynamics 365
Creating InstaQuote solution
Building a smart customer data repository
Creation of a common data platform in Microsoft Azure
Deployment of a data and analytics platform using Azure Data Lakes and Azure Data Factory
"Adoption of Microsoft Dynamics 365 business applications—Commerce, Supply Chain Management, and Finance"
Integration with existing SAP systems
Collaboration with Microsoft FastTrack for Dynamics 365 experts
Adopted Microsoft Dynamics 365 Sales
Adopted Power BI for analytics
Moved CRM to the cloud
Developed sophisticated dashboards in Power BI
Tied sales data to finance team
Investment in Microsoft Azure IoT solutions
Using IoT Edge devices for operational efficiency
"Unified view of productivity, safety, and quality data"
Monitoring security and compliance in real time
Move from on-premise servers to Azure cloud
Use of Power BI to reduce report preparation time
"Utilizing Microsoft Azure, Dynamics 365, Office 365, and Power BI"
Digital transformation of operations
Deployment of Microsoft Dynamics365
Use of LinkedIn Sales Navigator
Use of Microsoft PowerBI
Adopted Microsoft Azure SQL Database serverless compute tier
"Implemented Azure IoT Edge, Azure IoT Hub, Azure Stream Analytics, and Azure Machine Learning"
Connected all hydropower plants to a central database in the cloud
Providing access to Microsoft Power Platform
Creating a Center of Excellence to train and support employees
Maintaining rigorous governance requirements for app development
Setting up automated rules to prevent unauthorized app sharing
Monitoring app usage and decommissioning outdated apps
Using Microsoft Azure as the foundational platform
Engaging with Microsoft FastTrack for Azure
Automating virtual machine scaling and load balancing
Building a digital care pathway using Azure solutions
creating online classrooms
using Power BI dashboard visualizations
analyzing student performance to fill lesson gaps
Developed a new Wi-Fi-enabled product using Microsoft Azure IoT technology
Built a new ecosystem with Azure IoT Hub
Launched DUOX WiFi Monitor product
Deployed a new grant management solution built on GrantCare and Microsoft Dynamics 365
"Used Power BI and Power Apps to customize dashboards, templatize views, and automate processes"
Partnered with KPMG and GrantCare to leverage Microsoft technology and risk advisory services
Adoption of cloud technology and teleworking
"Use of Microsoft Teams, Azure Bot Service, and QnA Maker"
Building a chatbot (Litera) for internal IT service desk
Use Microsoft Azure Purview in private preview
"Combine Azure Purview with Azure Synapse Analytics, Power BI, Azure Data Lake Storage, and SQL Server"
Automate data scanning and classification
Develop a roadmap in collaboration with Microsoft
Implementation of Exasol as a high-performance analytics database
Deployment on Microsoft Azure
Use of Microsoft Power BI for visualization
Partnered with MyProteus US
"Used built-in, out-of-the-box functions"
Standardized platform
Migrated supply chain management to the cloud
"Implemented Dynamics 365 Finance and Operations, Dynamics 365 Sales, and Dynamics 365 Marketing"
Introduced a SharePoint Online portal
Implemented Microsoft Teams for communication
Automated workflows using Power Apps and Power Automate
Used Power BI dashboards for data analysis
"Implementing the CadDo solution to codify ESG knowledge, priorities, and preferences"
"Using an end-to-end Microsoft stack including Azure, SQL Server 2019, and Power BI"
Building a proof of concept within seven days
Built volunteer-management system on Microsoft Dynamics 365
Used system to restrict movement from spread zones
Identified volunteers with special skills for hot zones
Predictive analysis combining infection rate data with other information sources
Developing own digital systems
Partnering with consultants and Nexer
Implementing Microsoft technologies
Deployed Dynamics 365 Supply Chain Management in Americas first
Rolled out Dynamics 365 to other regions progressively
Worked with Microsoft FastTrack and product engineering team
Engaged Microsoft Industry Solutions
Committed to a complex cloud modernization
"Used Dynamics for CRM, SharePoint, and Power BI"
Orchestrated nearly 500 ecosystem partners
Created a virtual operations team
Developed a comprehensive loan forgiveness solution using Microsoft Power Platform
Provided a live link for prospective customers to experience the prototype
Consolidation under a single platform
"Use of Microsoft Azure, Microsoft Teams, and Workplace Analytics"
Migration from Skype for Business to Teams
Use of Azure DevOps
Implemented on-premises Microsoft Dynamics solution
Upgraded to cloud-based Dynamics 365
Rolled out multiple Dynamics 365 applications
Implement a cloud-based historian on Microsoft Azure
Use Uptake Fusion powered by ShookIOT
Install Uptake Ignition Connector on drilling rigs
Adopting Microsoft Power Apps and Power Platform
Combining Power Apps with Microsoft Teams
Using Power Automate and Power BI for comprehensive functionality
Collaborating with Microsoft
Using AI and algorithms to predict patient needs
Deploying Microsoft Azure Machine Learning
Implementing pilot tests in select markets
Adopted Microsoft Azure Data Factory for data replication
Implemented Azure SQL Managed Instance for reporting and analytics
Conducted a proof of concept with Microsoft
Choosing a cloud-based solution
Implementing Microsoft Dynamics 365
Moving to the cloud with Microsoft Azure
"Using Azure Event Hubs, Azure SQL Database, and Azure Databricks"
Power BI for data visualization
Series of scoping and design workshops
Using an end-to-end Microsoft stack
Rebuilding MyFuture platform on Microsoft Azure
"Proactively offering daily office hours, hands-on training, live webinars, and one-on-one help"
Facilitating a nationwide Facebook group for Club staff
serverless architecture
componentised app services
multi-region deployment
Moved training and migration guidance to a virtual environment
Implemented best practices for adoption and change management
Created Power BI dashboards to keep executives informed
Set up alternate email addresses as a workaround
Implemented virtual hubs via Microsoft Teams
Standardizing data sets and reporting tools
Migrating data to Microsoft Azure Data Lake Storage
Using Power Query to optimize data load time
Adopted a cloud-first strategy
Chosen Microsoft Azure as a trusted platform
Created an innovation board
Development of a customized telehealth solution
Creation of a separate Office 365 tenant for each practice
Hosted educational webinars and created user guides
Adopting EDLIGO analytics and AI platform
"Using Microsoft Azure, Power BI, and Office 365 for analytics, authentication, and storage"
Building a BI platform on Azure with Power BI
Automating KPI visualization
Engaging with teams for data validation and automated reporting
Real-time data availability for self-service analysis
"Developed a CRM solution using Microsoft Dynamics 365, Power BI, and Azure"
Use of Common Data Model for Nonprofits
Collaboration with Microsoft and Wipfli
Using Microsoft Azure cloud platform
Implementing Azure services for workflows and automation
Migration from on-premises datacenters to Azure
Adoption of Microsoft Dynamics 365 Sales and Microsoft Power Platform
Collaboration with PowerObjects for implementation
Training workshops for leadership and sales staff
Unified fractured data landscape into a common customer data platform
Deployment of Microsoft Dynamics 365 Customer Insights and Dynamics 365 Marketing
Establishment of API connectors to numerous data sources
Thorough data exploration and cleanup
Launch of global strategy alliance with Microsoft
Achieve four gold-level expertise certifications for Microsoft's Global Partner Ecosystem
Import Microsoft Azure Hybrid Cloud
Apply artificial intelligence and big data analytics technologies
Develop IoT applications and cloud solutions
"Built a new data platform around Microsoft Azure Data Lake Storage, Azure Databricks, and Azure Synapse Analytics"
Evaluated cloud vendors and chose Microsoft Azure for seamless integration
Adoption of Microsoft 365
Deployment with the help of ENCAMINA
Training sessions (face-to-face and online)
Involvement of all levels and departments
Data Insights offering powered by Microsoft Azure
Discovery Workshop to understand requirements and define a roadmap
Iterative approach focusing on building solutions one business area at a time
Building a data pipeline
Completing data validation
Establishing user training and documentation on data models
"Adding more data sources such as IoT devices, cloud systems, and plant automations"
Rapid adoption of Microsoft 365
Setting up virtual command centers
Using Power BI and Power Apps to track and report issues
Remote patient surveillance
Virtual collaboration via Teams
Consolidating data and building foundational architecture and reports
Introducing standardized survey responses
Utilizing historical information and success metrics
Adoption of Microsoft 365 tools
Implementation of Microsoft Dynamics 365 Sales
Support from Microsoft and systems integrator Logicom Solutions Ltd
rolling out Microsoft Dynamics 365 Field Service
assigning service jobs among staff and subcontractors dynamically
aligning with pivot to condition- and predictive-based maintenance
Implementation of Fault Detection and Diagnostics (FDD)
Utilization of customized ICONICS platform with Microsoft Power BI
Adoption of preemptive maintenance and sustainability plans
"Use of Microsoft technologies like Azure IoT, Dynamics 365, and Power BI"
Deployed Microsoft Dynamics 365 Sales and Customer Service
Used Microsoft Power Platform
Implemented Microsoft Power BI for data visualization and analytics
Utilized Microsoft Power Automate to connect systems
"Collaborated with C Centric, Microsoft Premier Support, FastTrack for Dynamics 365, and Advanced Cloud Engineering"
Moving IT resources to the cloud: Azure
Deploying Microsoft 365 E5
Using Microsoft Teams for collaboration
moving IT resources to the cloud
migrating the company’s 100-terabyte SAP landscape to Azure
implementing innovative technologies like chatbots
Adopting Microsoft Power Platform
Using low-code solutions
In-house app development
adopt Microsoft Azure cloud platform
build a digital internal collaboration platform
use Microsoft Azure Data Warehouse
integrate Azure platform services
develop innovative services with Azure IoT and Azure Machine Learning
Modernize customer portal on Microsoft Azure
Optimize security software and communication systems with Microsoft 365 E5
Train-the-trainer method for Microsoft Teams
Introduce a new CRM system
Collaborate with Microsoft and partner HSO
Incorporate Dynamics 365 into existing IT landscape
Using Microsoft 365 and Teams for collaboration
Creating Global Business Services (GBS)
Developing Technician Plus with multiple apps
Adopt Microsoft Power Apps
Develop custom apps tailored to company needs
Leverage Power Automate for workflows
Investing in in-house capabilities
Choosing Azure and Power BI for scalability and affordability
Creating a data warehouse and using Azure Data Factory
Modernized Darwin software with Microsoft Azure Time Series Insights and Azure IoT Edge
Used services in Azure IoT Hub to manage IoT tags
Utilized Time Series Insights for real-time IoT data dashboarding and advanced analytics
Internal branding as Arla Analytics Powerhouse
Centralized data foundation on Azure
Combining SAP BW on HANA for financial data and Azure for other data
Avoiding duplication and latency by leveraging existing systems
migrating workloads to Azure Data Explorer
collaborating with Microsoft team
Adoption of Microsoft Dynamics 365 and Microsoft Power Platform
Standardizing on a single technology platform
Implementing cloud-based solutions
Preplanning and utilizing Microsoft-provided tools
"Adopted Microsoft Dynamics 365 Sales, Power Apps, and Power BI"
Implemented a cloud-based CRM platform
Used Microsoft Power Pages with Dynamics 365 Sales to automate manual processes
Performed a successful rollout and change management process
Partnering with Qnovate for cloud migration
Migrating SAP landscape to Microsoft Azure
Syncing Dynamics 365 to existing SAP infrastructure
migrating data-analytics services to Microsoft Azure
using Azure Kubernetes Service (AKS) for scalable infrastructure
"using Azure Data Lake, Azure Data Factory, and Microsoft SQL Server for data analysis"
transforming information into insights using Microsoft Power BI
Migrated IT resources to Microsoft Azure
Invested in factory upgrades
Implemented Microsoft Power BI
Adopted Microsoft Azure SQL Database hyperscale service tier
Integrated data from dozens of sources into an operational data store and enterprise data warehouse
Used Microsoft Power BI for data analysis
Adoption of Microsoft Dynamics 365
Integration with existing Microsoft Office 365
Assistance from Microsoft's Customer Success team
"centralized, fully interoperable member-engagement platform"
"move data to a unified, cloud-based platform"
use AI and machine learning
adopt Microsoft Dynamics 365
"Deploying Azure Data Lake Storage, Azure Databricks, and Power BI"
Partnering with Kagool to move data from SAP systems to Azure Data Lake Storage using Velocity
Testing Microsoft Dynamics 365 Fraud Protection on live site
Deploying a new e-commerce environment with a dedicated fraud protection layer
Using AI algorithms for real-time fraud detection
Scaling fraud protection capabilities with cloud-based resources
"Building an online citizen platform based on Azure PaaS, Dynamics 365 CE, and Office 365"
"Creating The Digital Platform—an e-government web portal, mobile app, and chatbot service"
Implementing bots for 24/7 access to government services
Using Microsoft Power BI for robust reporting capabilities
Migrated infrastructure to Azure virtual machines
Rebuilt app for microservice-based cloud architecture
Used Azure API Management to publish microservices
"Integration of Microsoft Dynamics 365, Power BI, and Azure solutions"
Building a digital hub that integrates government and business services
Migrated to Microsoft Azure Synapse Analytics
Developed an Azure Synapse pilot
Developing and deploying digital solutions and platforms
Using cutting-edge digital solutions like Asset Insight
Deployment of a hybrid ecosystem with cloud data analysis and on-premises management
Use of ERP solutions VinoTEC and OliTEC built on Power BI and Dynamics 365 Business Central
Pilot deployment in 76 stores
Rollout across European stores
Eventual global deployment
Adoption of Dynamics 365 Customer Insights
Integration with Dynamics 365 Marketing
Selected Microsoft Dynamics 365 Customer Insights
Bringing together siloed data from all sources
"Use of Microsoft 365 including Office 365, Windows 10, and Enterprise Mobility + Security"
Deployment of Microsoft Teams for caregiver collaboration
Implementation of AI for patient risk identification
organization-wide transformation initiative called Project Connect
consolidate systems with QuantiQ
build apps and workflows using Microsoft Dynamics 365 and Microsoft Power Platform
digitizing paper-based staff rostering system
Implemented Microsoft Dynamics 365 and Microsoft Azure
Used Power BI for data visualization and analysis
Integrated Adobe Marketing Cloud for digital engagement
Implementing Microsoft Dynamics 365 Supply Chain Management
Using IoT Intelligence to transform data from IoT-enabled devices into actionable insights
Breaking down data siloes to create a single source of real-time truth
Migration to a cloud-based solution
Centralizing data sources
Using Microsoft Azure and Power BI
Transforming process into a fully paperless and online system
Experimenting with AI for improved predictions
Collaboration with local Microsoft partner Genisoft
Migrated KOM-MICS to Microsoft Azure
Collected and analyzed data from machine tools and robots
Used IoT applications running on Azure virtual machines
Using Microsoft Azure and Azure Database for PostgreSQL
"Fully managed services to avoid managing uptime, backup, and recovery scenarios"
Incorporation of Azure Databricks and Microsoft Power BI for advanced capabilities
Adopted Microsoft Project Online
Integrated Microsoft Power BI with Office 365
Committed to using Microsoft Planner and Microsoft Teams
Choosing Dynamics 365 for its built-in capabilities and low-risk deployment
Integrating various business systems into one ERP suite
Early adoption and close collaboration with Microsoft
Use of Internet of Things and cloud computing
Integration with Azure functionality
Teaming with consulting firm Aptude
Utilizing Microsoft Office 365 and Project Online
Focusing on training and process standardization
"Integrating Visio, SharePoint, Teams, and Power BI"
migration to Azure centered on essential SAP applications
using a standard SAP migration process
replatforming legacy systems
creating a data lake
selected a full line of Microsoft software including Dynamics 365 and Power BI
built an end-to-end cloud-based IT infrastructure
used Microsoft FastTrack consulting services
Adoption of Microsoft Power Apps and Dynamics 365
Equipping remote workers with tablets
Encouragement of app development within the company
Moving mainframe applications to Microsoft Azure SQL Database Managed Instance
Using TimeXtender Discovery Hub for data management
Utilizing Microsoft Power BI for data visualization
Embracing digital transformation using cloud technologies
Migrated to Microsoft Dynamics 365 for Finance and Operations
Deployed Microsoft Power BI
Rolled out a cloud-based e-commerce system (BigCommerce)
Licensed Microsoft Office 365
Connected Windows 10 devices with Microsoft Azure Active Directory
Adoption of cloud-first strategy
Migration of IT processes and infrastructure to the cloud
Use of Microsoft Azure and Azure IoT technologies
Development of Job Site Insights (JSI) application
"Embrace of SaaS, PaaS, and IaaS"
Integration of IoT sensors and advanced analytics
Developing RoomFinder app using Microsoft Office 365 services
Collaborating with Microsoft Partner Macaw
"Using Power Apps, Flow, and Power BI"
Use Azure to pull data from back-end systems
Push data to Power BI for real-time insights
Notify staff one to two hours beforehand about influxes
Total refresh of customer-facing website
Rebuild of staff intranet
Aggressive use of modern data analytics and Business Intelligence
Migrating existing hosted SAP platform to Microsoft Azure
Rewriting job descriptions and aligning IT staff roles
Implementing Azure Data Lake Storage and Microsoft Power BI
Using Microsoft Azure Site Recovery for migration
Signed a strategic partnership with Microsoft Azure
Extensive use of Azure platform as a service (PaaS) and infrastructure as a service (IaaS)
Leveraging deep customer intelligence with Adobe Experience Cloud
Adopt Microsoft Dynamics 365
Implement with the help of RSM consulting partner
Used Microsoft Power BI Embedded
"Connected to data sources, created reporting layers, embedded .pbix files"
Partnering with Microsoft to leverage Azure and Dynamics 365
"Aligning Adobe Experience Cloud, Creative Cloud, and Document Cloud"
Developing an integration roadmap with Microsoft technologies
Built a virtual agent using the Microsoft Dynamics 365 AI solution for customer service
Included the virtual agent in the HP Support Assistant tool installed on every HP computer
Adopt Microsoft Azure platform
Utilize hybrid cloud approach
Migrate resource-intensive workloads to Azure first
Adopting Microsoft Azure platform including Azure IoT solution accelerators
"Using Microsoft Dynamics 365, Microsoft Power BI, and Microsoft Office 365"
Collaborating closely with Microsoft to deliver water management solutions
Change data capture replication process for SAP ERP systems
Utilization of a cloud data platform
Implementation of Qlik Replicate and Qlik Compose
Adopted Qlik Cloud Analytics
Hybrid SaaS and on-premises Qlik Sense infrastructure
Create mappings between multiple systems and sources
Centralized data hub
Use of Talend Data Fabric
Initial three-day proof of concept
Migration of first reports with occasional support from Bison
Use of Qlik's in-memory technology and graphical visualization functions
Precise selection and transfer of data
Individualized data sets generation
Adopting self-service analytics
Selecting Qlik as data visualization partner
Interactive data dashboards
Collaborating with Qlik and India-based Qlik Partner Diagonal Consulting
Implementing customized dashboards and advanced capabilities
Using Qlik NPrinting and Qlik Data Gateway
Relaying data from core mainframe to digital application systems
Deployment of streaming solutions
Use of Qlik Data Integration and Qlik Replicate
Opted for Qlik Sense® SaaS
Connected data sources and made key adjustments
Developed a range of sales and accounting applications
Issued a tender outlining basic criteria
Partnering with a strong BI partner (GINQO)
Adopting and embracing Qlik Sense
Switching to Qlik Cloud
Market analysis to select Qlik Sense®
"Enlisted support of Informatec to implement the software, establish the data model, and set up initial applications"
utilized Talend Cloud Data Fabric
collaboration with local consultancy btProvider
"Partnered with local Qlik Partner, Manar Technologies"
Adopted QlikView and later Qlik Sense
Migrated from SAP ECC to SAP S/4HANA
Used Databricks Data Intelligence Platform with Qlik Data Integration
Migrated to Qlik Sense
Facilitated organization-wide training
Replacement of old and inaccurate electricity meters
Collaboration with authorized Qlik Partner Cast Solutions
Implemented SAP connectors to bring data into QlikView
Developed a proof of concept using Qlik Replicate and Qlik Compose
Rebuilt the data analytics platform with Qlik products
Using Qlik Data Integration
Implementation of Qlik Cloud® Analytics
Move data processing to a lakehouse
Integrate data with Qlik
Use Databricks to deliver a global view of performance
replacing spreadsheet-based reporting with Qlik Sense
creation of Openreach Data and Insight (ODI) team
Developed dashboards and report-building applications using Qlik Sense
Consolidated data from siloed sources into a common language and format
In-sourced marketing mix modeling
API integrations
Single page application on React framework
Selected Talend® Data Fabric
Built a comprehensive data tracker
Combined with third party visualization and analytics solutions
Using API to alter qualification and acquisition process
Being more selective on which leads to buy
Exposing lead scoring model to an API
Connecting API to Loan Management System (LMS)
Migrated to Snowflake cloud
Used Qlik Data Integration and Qlik Data Analytics
