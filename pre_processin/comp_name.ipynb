{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31220/2800771450.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ashkan/Code/testimonial_llm_analysis/indus_clean.json') as f:\n",
    "    indust = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_csv(indust, filename):\n",
    "    # Define the fieldnames for the CSV file\n",
    "    fieldnames = ['story_id', 'company_name', 'industry_1', 'industry_2', 'industry_3']\n",
    "\n",
    "    # Open the CSV file in write mode\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write the header row\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Write each row of data\n",
    "        for item in indust:\n",
    "            row = {\n",
    "                'story_id': item['story_id'],\n",
    "                'company_name': item.get('company_name', ''),\n",
    "                'industry_1': item['industry'][0] if len(item['industry']) >= 1 else '',\n",
    "                'industry_2': item['industry'][1] if len(item['industry']) >= 2 else '',\n",
    "                'industry_3': item['industry'][2] if len(item['industry']) >= 3 else ''\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Call the function to convert the 'indust' list to a CSV file\n",
    "convert_to_csv(indust, 'indust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a dataframe\n",
    "df = pd.read_csv('/home/ashkan/Code/testimonial_llm_analysis/indust.csv')\n",
    "\n",
    "# Lowercase every column\n",
    "df = df.apply(lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['industry_1'] = df['industry_1'].apply(lambda x: 'other' if x == 'o' else x)\n",
    "df.loc[df['industry_1'] == 'other', ['industry_2', 'industry_3']] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_17 = pd.read_csv('/mnt/c/Users/alotf/OneDrive - University of Surrey/[1] Raw Data/slr_paper/companies-2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the combined_data.json file\n",
    "with open('/home/ashkan/Code/customer_stories/results/combined_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the id and company keys' values\n",
    "id_company_data = [(item['id'], item['company']) for item in data]\n",
    "\n",
    "# Define the fieldnames for the CSV file\n",
    "fieldnames = ['id', 'company']\n",
    "\n",
    "# Save the data as a CSV file\n",
    "with open('id_company.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(fieldnames)\n",
    "    writer.writerows(id_company_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org = pd.read_csv(csvfile.name)\n",
    "df_org = df_org.apply(lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df.merge(df_org[['id', 'company']], left_on='story_id', right_on='id', how='left').drop(columns='id')\n",
    "df_merge.rename(columns={'company': 'company_org'}, inplace=True)\n",
    "df_merge = df_merge[['story_id', 'company_name', 'company_org', 'industry_1', 'industry_2', 'industry_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_merge[df_merge['company_name'].isna()]\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.to_csv('df_merge.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete empty stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries before deletion: 2865\n",
      "Number of entries after deletion: 2723\n"
     ]
    }
   ],
   "source": [
    "# Open the combined_data.json file\n",
    "with open('/home/ashkan/Code/customer_stories/results/combined_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Get the number of entries before deletion\n",
    "num_entries_before = len(data)\n",
    "\n",
    "# Check if the 'content' key is empty and delete the entry if it is\n",
    "data = [entry for entry in data if entry.get('content')]\n",
    "\n",
    "# Get the number of entries after deletion\n",
    "num_entries_after = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries before deletion: 2865\n",
      "Number of entries after deletion: 2723\n"
     ]
    }
   ],
   "source": [
    "# Print the number of entries before and after deletion\n",
    "print(f\"Number of entries before deletion: {num_entries_before}\")\n",
    "print(f\"Number of entries after deletion: {num_entries_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path for the new JSON file\n",
    "new_file_path = '/home/ashkan/Code/customer_stories/results/data_wo_empty.json'\n",
    "\n",
    "# Save the data list as a JSON file\n",
    "with open(new_file_path, 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"   For Mayborn Group, the company behind the Tommee Tippee brand of baby care products, brand loyalty and market presence are important priorities. Tracking critical indicators in these areas and reaching a single source of truth is essential, and it\\u2019s an area where high-quality data is key. Acquiring this data quickly and accurately from such a broad range of sources and locations is, however, a major task. With Qlik, Mayborn can now access reliable data in consistently useable formats to create high value analytics. Using Qlik Data Integration, Mayborn can access data from multiple locations to feed into its data warehouse. And Qlik Cloud\\u00ae Analytics generates dashboards that tap into every part of Mayborn\\u2019s operation to deliver key insights. \\n\\n \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries before deletion: 2865\n",
      "Number of entries after deletion: 2583\n"
     ]
    }
   ],
   "source": [
    "# Filter the data list to remove entries with content length less than 1000 characters\n",
    "data = [entry for entry in data if len(entry.get('content', '')) >= 1000]\n",
    "\n",
    "# Update the number of entries after deletion\n",
    "num_entries_after = len(data)\n",
    "\n",
    "# Print the number of entries before and after deletion\n",
    "print(f\"Number of entries before deletion: {num_entries_before}\")\n",
    "print(f\"Number of entries after deletion: {num_entries_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated data list as a JSON file\n",
    "with open('/home/ashkan/Code/customer_stories/results/data_final.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc_miniproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
